{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjUbSeEwdx_I"
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers datasets sentencepiece\n",
        "#!pip install -q pytorch-lightning wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THqwzDQjebo3"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvKvx2DAd68a"
      },
      "source": [
        "# Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlNI3FeMeDRs",
        "outputId": "d73b8f91-21c9-43bd-8316-977fc9cefcf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N46NoBykeFLH",
        "outputId": "59268fc9-034c-41fe-d229-752f7c736814"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['validation', 'test', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data_path = '/content/drive/MyDrive/Donut/synthtiger_files/outputs/SynthDoG_he'\n",
        "#data_path = '/content/drive/MyDrive/Donut/SynthDoG_he'\n",
        "os.listdir(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwfRg6hje2eP"
      },
      "source": [
        "# Load model and processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkyp4qVFeQv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05e1981-4a62-4e40-9df1-d2568125a39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import VisionEncoderDecoderConfig\n",
        "\n",
        "image_size = [1280, 960]\n",
        "max_length = 768\n",
        "\n",
        "# update image_size of the encoder\n",
        "# during pre-training, a larger image size was used\n",
        "config = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\n",
        "config.encoder.image_size = image_size # (height, width)\n",
        "# update max_length of the decoder (for generation)\n",
        "config.decoder.max_length = max_length\n",
        "# TODO we should actually update max_position_embeddings and interpolate the pre-trained ones:\n",
        "# https://github.com/clovaai/donut/blob/0acc65a85d140852b8d9928565f0f6b2d98dc088/donut/model.py#L602"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jznZw27e9dC",
        "outputId": "06e5ead9-0a42-4f0c-abfb-f04ae294f01a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65840\n"
          ]
        }
      ],
      "source": [
        "from transformers import DonutProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
        "\n",
        "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\", config=config)\n",
        "\n",
        "print(len(processor.tokenizer))\n",
        "\n",
        "#change to hebrew tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-he')\n",
        "\n",
        "tokenizer.add_special_tokens({'bos_token' : '<s>'})\n",
        "\n",
        "processor.tokenizer = tokenizer\n",
        "model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
        "print(len(processor.tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfxLT9Yaw6Pi",
        "outputId": "6b4c8549-d21c-48d6-8738-01683276442b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MBartDecoder(\n",
              "  (embed_tokens): Embedding(65840, 1024)\n",
              "  (embed_positions): MBartLearnedPositionalEmbedding(1538, 1024)\n",
              "  (layers): ModuleList(\n",
              "    (0-3): 4 x MBartDecoderLayer(\n",
              "      (self_attn): MBartAttention(\n",
              "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      )\n",
              "      (activation_fn): GELUActivation()\n",
              "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (encoder_attn): MBartAttention(\n",
              "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      )\n",
              "      (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model.decoder.model.decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMIj8kX5fi0S"
      },
      "source": [
        "# Create PyTorch dataset\n",
        "Here we create a regular PyTorch dataset.<br>\n",
        "\n",
        "The model doesn't directly take the (image, JSON) pairs as input and labels. Rather, we create pixel_values and labels. Both are PyTorch tensors. The pixel_values are the input images (resized, padded and normalized), and the labels are the input_ids of the target sequence (which is a flattened version of the JSON), with padding tokens replaced by -100 (to make sure these are ignored by the loss function). Both are created using DonutProcessor (which internally combines an image processor, for the image modality, and a tokenizer, for the text modality).\n",
        "\n",
        "Note that we're also adding tokens to the vocabulary of the decoder (and corresponding tokenizer) for all keys of the dictionaries in our dataset, like \"<s_menu>\". This makes sure the model learns an embedding vector for them. Without doing this, some keys might get split up into multiple subword tokens, in which case the model just learns an embedding for the subword tokens, rather than a direct embedding for these keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym4iXZKvfzV-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from typing import Any, List, Tuple\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "added_tokens = []\n",
        "\n",
        "class DonutDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Donut. This class takes a HuggingFace Dataset as input.\n",
        "\n",
        "    Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt),\n",
        "    and it will be converted into pixel_values (vectorized image) and labels (input_ids of the tokenized string).\n",
        "\n",
        "    Args:\n",
        "        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
        "        max_length: the max number of tokens for the target sequences\n",
        "        split: whether to load \"train\", \"validation\" or \"test\" split\n",
        "        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
        "        task_start_token: the special token to be fed to the decoder to conduct the target task\n",
        "        prompt_end_token: the special token at the end of the sequences\n",
        "        sort_json_key: whether or not to sort the JSON keys\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_name_or_path: str,\n",
        "        max_length: int,\n",
        "        split: str = \"train\",\n",
        "        ignore_id: int = -100,\n",
        "        task_start_token: str = \"\",\n",
        "        prompt_end_token: str = None,\n",
        "        sort_json_key: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.split = split\n",
        "        self.ignore_id = ignore_id\n",
        "        self.task_start_token = task_start_token\n",
        "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
        "        self.sort_json_key = sort_json_key\n",
        "\n",
        "        self.dataset = self.load_dataset(dataset_name_or_path)\n",
        "        self.dataset_length = len(self.dataset)\n",
        "\n",
        "    def load_dataset(self,dataset_name_or_path):\n",
        "\n",
        "      dataset = []\n",
        "      with open(os.path.join(dataset_name_or_path,'metadata.jsonl'), 'r') as file:\n",
        "        for line in file:\n",
        "\n",
        "          data_point = json.loads(line)\n",
        "          img_path = os.path.join(dataset_name_or_path,data_point['file_name'])\n",
        "          text_sequence = json.loads(data_point['ground_truth'])['gt_parse']['text_sequence']\n",
        "          dataset.append({'img_path':img_path,'text_sequence':text_sequence})\n",
        "      return dataset\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.dataset_length\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
        "        Convert gt data into input_ids (tokenized string)\n",
        "        Returns:\n",
        "            input_tensor : preprocessed image\n",
        "            input_ids : tokenized gt_data\n",
        "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
        "        \"\"\"\n",
        "        sample = self.dataset[idx]\n",
        "        img = Image.open(sample[\"img_path\"])\n",
        "\n",
        "        # inputs\n",
        "        pixel_values = processor(img, random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
        "        pixel_values = pixel_values.squeeze()\n",
        "\n",
        "        # targets\n",
        "        target_sequence = sample['text_sequence']\n",
        "        input_ids = processor.tokenizer( #add bos token to sequence\n",
        "            processor.tokenizer.bos_token + \" \" + target_sequence,\n",
        "            add_special_tokens=False,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token\n",
        "        return pixel_values, labels, target_sequence #returns pixels labels and the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6MIPOzFj1DU"
      },
      "source": [
        "instantiate the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tiroPI3gCqt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
        "# source: https://github.com/clovaai/donut/blob/master/config/train_cord.yaml\n",
        "processor.image_processor.size = image_size[::-1] # should be (width, height)\n",
        "processor.image_processor.do_align_long_axis = False\n",
        "\n",
        "train_dataset = DonutDataset(os.path.join(data_path,'train'), max_length=max_length,\n",
        "                             split=\"train\", task_start_token=\"\", prompt_end_token=\"\",\n",
        "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
        "                             )\n",
        "\n",
        "val_dataset = DonutDataset(os.path.join(data_path,'validation'), max_length=max_length,\n",
        "                             split=\"validation\", task_start_token=\"\", prompt_end_token=\"\",\n",
        "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
        "                             )\n",
        "\n",
        "\n",
        "test_dataset = DonutDataset(os.path.join(data_path,'test'), max_length=max_length,\n",
        "                             split=\"test\", task_start_token=\"\", prompt_end_token=\"\",\n",
        "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EJZAoArvLcC",
        "outputId": "cea648c1-03b4-4420-bf0f-989ab94dc4d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65840"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(processor.tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LDXVijLhv41w",
        "outputId": "d3c447e1-d34f-40fd-a696-185761b58ff8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "','"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "processor.tokenizer.decode([3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePjU3ETBtoGD"
      },
      "outputs": [],
      "source": [
        "pixel_values, labels, target_sequence = train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGNFY7whvELO",
        "outputId": "9fe4ab03-be61-4bce-cd9b-c0507e5810e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> ימניה חום ויוב ש, בלי רוח כמעט.ב ישראל, מגדירים יום שרבי כיום שבו הלחו ת היחסית המ מו<unk> פחותה\n",
            "ימניה חום ויוב ש, בלי רוח כמעט.ב ישראל, מגדירים יום שרבי כיום שבו הלחו ת היחסית המ מוצעת פחותה מ־05\n"
          ]
        }
      ],
      "source": [
        "decoded_seq = processor.tokenizer.decode(labels.tolist())#, skip_special_tokens=True\n",
        "print(decoded_seq[0:100])\n",
        "print(target_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-98JPQn7j3mj"
      },
      "source": [
        "test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMBu-W5xknCb"
      },
      "source": [
        "Another important thing is that we need to set 2 additional attributes in the configuration of the model. This is not required, but will allow us to train the model by only providing the decoder targets, without having to provide any decoder inputs.\n",
        "\n",
        "The model will automatically create the decoder_input_ids (the decoder inputs) based on the labels, by shifting them one position to the right and prepending the decoder_start_token_id. I recommend checking this video if you want to understand how models like Donut automatically create decoder_input_ids - and more broadly how Donut works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c30fCl6kQe-"
      },
      "outputs": [],
      "source": [
        "processor.tokenizer.bos_token # added this token to represent start of seq\n",
        "\n",
        "\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "model.config.decoder_start_token_id = processor.tokenizer.bos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SHs_PmH2j7e",
        "outputId": "4e80670c-3651-4f32-fe3b-5c8e3d30e4d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-he', vocab_size=65839, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t65838: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t65839: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "processor.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CxCivgnkvFe",
        "outputId": "7b649e3a-8dd6-4e87-d2b2-613c00b7f9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad token ID: <pad>\n",
            "Decoder start token ID: <s>\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "print(\"Pad token ID:\", processor.decode([model.config.pad_token_id]))\n",
        "print(\"Decoder start token ID:\", processor.decode([model.config.decoder_start_token_id])) # start_token!!!!!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtitqhXWlUS9"
      },
      "source": [
        "# Create PyTorch DataLoaders\n",
        "\n",
        "Next, we create corresponding PyTorch DataLoaders, which allow us to loop over the dataset in batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2kYeFWtk1By"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# feel free to increase the batch size if you have a lot of memory\n",
        "# I'm fine-tuning on Colab and given the large image size, batch size > 1 is not feasible\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4) #dataloader gets train dataset to do overfiting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpzXy_RVlctf",
        "outputId": "d25e7fb1-7a03-4ded-891a-f49ffb7e426d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 1280, 960])\n"
          ]
        }
      ],
      "source": [
        "#Let's verify a batch:\n",
        "batch = next(iter(train_dataloader))\n",
        "pixel_values, labels, target_sequences = batch\n",
        "print(pixel_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdXDI0KN1E01",
        "outputId": "34b70ad6-f9df-4b9c-8d8b-d036caa21b72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ניהם היו מוע מדים בעיתיים ינת הגוש הגדו ל של מצבי עים רפוב ליקנים מהימין ה שמרני- דתי: ג' וליאני, ש עמד  ש אחת ה ערים הליבר ליות ביות ר ות- הברית, גי לה עמדות ליברליות מדי ל בנושאים הפלות מל ותיות, הגבלות על נשק, ויחס למהגרים, וגם חייו האי שיים הוו בשערוריות והיו רחוקים ממודל איש ה משפחה המסור.מקיין נחשב למי שגילה עצמ אות יתר מהקו המפלגתי, ונזקפו נ<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
          ]
        }
      ],
      "source": [
        "print(processor.tokenizer.decode(labels[0].tolist(), skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZa1LKlLlpeV"
      },
      "outputs": [],
      "source": [
        "# for id in labels.squeeze().tolist()[:30]:\n",
        "#   if id != -100:\n",
        "#     print(processor.tokenizer.decode([id]))\n",
        "#   else:\n",
        "#     print(id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgDxJp1a0jr9",
        "outputId": "9f83ec01-c860-448f-802d-2c785ba1effe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"ניהם היו מוע מדים בעיתיים מבחינת הגוש הגדו ל של מצבי עים רפוב ליקנים מהימין ה שמרני- דתי: ג' וליאני, ש עמד ברא ש אחת ה ערים הליבר ליות ביות ר בארצות- הברית, גי לה עמדות ליברליות מדי לטעמם בנושאים הפלות מלאכ ותיות, הגבלות על נשק, ויחס למהגרים, וגם חייו האי שיים הוכתמו בשערוריות והיו רחוקים ממודל איש ה משפחה המסור.מקיין נחשב למי שגילה עצמ אות יתר מהקו המפלגתי, ונזקפו נ\",\n",
              " 'מותו של הסב והאבוב מחקה את הברווז.באופן דומ ה מתאר החליל את הציפור, ה קלרינט משויך לחתו ל ותופי הדוד מבשרים את בוא הציידים.הדמויו ת מזוהות עם הכלים המוזיקליים, מה שיוצר בקרב ה צופים ציפייה להופעת הדמות עם השמעת הנעימה המתאימה לה.ד\"ר אח מד נזי ף ) בע רבית: أحمد نظيف\\u200e; נולד ב-8 ביולי 2591( הוא ראש המ משלה של מצרים בעבר, שכיהן בתפקידו מאז ה-41 ביולי 2 400 ו עד 92 בינוא',\n",
              " 'יברסיטא ות מעמיד ות לרשותם א ת מ יטב המ אמנים וה מורי ם.אס\" א מ רח יב מש נה ל שנה את היק ף התח רויות ה בינל אומיות, בהן',\n",
              " \"'C' -זכה ה ykS eht ni ralleC 800, בתחרות cnalB letsaC ud , יחד עם ח ברת אל על , כיין ה לבן הטוב ביותר המוגש ב מחלקה ראשונה. התחרות נערכ ה בין 62 ח ברות תעו פה, ביניהן אייר פרנס, בריטי ש איירוו ייז וחבר ות מובילות אחרו ת.בד צמבר 7002 ו שוב ביונ י 002 9, זכו שלוש ת היינו ת לצי ונים גבוהים אצל מבקר היינו\")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "target_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkMPi7ASlto5",
        "outputId": "52b5b0ae-04ae-4038-9a94-33f4b8715369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32668\n",
            "3953\n"
          ]
        }
      ],
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF-K1D8tlw3S",
        "outputId": "c326f900-4a79-4db5-d3b3-72151e2762f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 1280, 960])\n",
            "רביי י שראל ה ם עקור ים או צאצ אי עקו רים כ תוצאה ממלחמ ת העצ מאות, כ לומר אנש ים שע זבו את מקו ם יישו בם ול א הורש ו לחזור אל יו.לעתים האנשים עקרו ליישוב אחר בתחילת המלח מה וכ שמקו ם מו שבם השני נכבש הם נשארו תח ת ש\n"
          ]
        }
      ],
      "source": [
        "# let's check the first validation batch\n",
        "batch = next(iter(val_dataloader))\n",
        "pixel_values, labels, target_sequences = batch\n",
        "print(pixel_values.shape)\n",
        "\n",
        "print(target_sequences[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SySz8X_Sn0ff"
      },
      "source": [
        "# Define LightningModule\n",
        "Next, we define a LightningModule, which is the standard way to train a model in PyTorch Lightning. A LightningModule is an nn.Module with some additional functionality.\n",
        "\n",
        "Basically, PyTorch Lightning will take care of all device placements (.to(device)) for us, as well as the backward pass, putting the model in training mode, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUMIrQpQl4nH"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "from nltk import edit_distance\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "\n",
        "\n",
        "class DonutModelPLModule(pl.LightningModule):\n",
        "    def __init__(self, config, processor, model):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.processor = processor\n",
        "        self.model = model\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        pixel_values, labels, _ = batch\n",
        "\n",
        "        outputs = self.model(pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        self.log(\"train_loss\", loss)\n",
        "        print(f\"train loss: {loss}\")\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
        "        pixel_values, labels, answers = batch\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        # we feed the prompt to the model\n",
        "        decoder_input_ids = torch.full((batch_size, 1), self.model.config.decoder_start_token_id, device=self.device)\n",
        "\n",
        "        outputs = self.model.generate(pixel_values,\n",
        "                                   decoder_input_ids=decoder_input_ids,\n",
        "                                   max_length=max_length,\n",
        "                                   early_stopping=True,\n",
        "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
        "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
        "                                   use_cache=True,\n",
        "                                   num_beams=1,\n",
        "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
        "                                   return_dict_in_generate=True,)\n",
        "\n",
        "        predictions = []\n",
        "        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n",
        "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
        "            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
        "            predictions.append(seq)\n",
        "\n",
        "        scores = []\n",
        "        for pred, answer in zip(predictions, answers):\n",
        "            #pred = re.sub(r\"(?:(?<=>) | (?=))\", \"\", answer, count=1)\n",
        "            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n",
        "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
        "\n",
        "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
        "                print(f\"Prediction: {predictions}\")\n",
        "                print(f\"    Answer: {answer}\")\n",
        "                print(f\" Normed ED: {scores[0]}\")\n",
        "\n",
        "        self.log(\"val_edit_distance\", np.mean(scores))\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # you could also add a learning rate scheduler if you want\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK_A5Pnjn6vQ"
      },
      "outputs": [],
      "source": [
        "config = {\"max_epochs\":100,\n",
        "          \"val_check_interval\":1, # how many times we want to validate during an epoch\n",
        "          \"check_val_every_n_epoch\":10,\n",
        "          \"gradient_clip_val\":1.0,\n",
        "          \"num_training_samples_per_epoch\": 44,\n",
        "          \"lr\":3e-5,\n",
        "          \"train_batch_sizes\": [8],\n",
        "          \"val_batch_sizes\": [1],\n",
        "          # \"seed\":2022,\n",
        "          \"num_nodes\": 1,\n",
        "          \"warmup_steps\": 300, # 800/8*30/10, 10%\n",
        "          \"result_path\": \"./result\",\n",
        "          \"verbose\": True,\n",
        "          }\n",
        "\n",
        "model_module = DonutModelPLModule(config, processor, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "82d1272a2cd5481ca5704290fcb099ef",
            "67cc97da815b46ac9fc09d12f1f812e0",
            "640828bbc02c49d5b97497a1856fe7f0",
            "c9df95ccb5b5438ea35fb0893a9975ee",
            "13eb1132dc6341efbb6e3981889f8f7f",
            "7bcd2c1ad3bd4485809fda79477ebedc",
            "810b73528d484e3baaab713f7c3c3715",
            "73316b9f8d48462b8bee82addd8721bd",
            "0c03e7e21a1640d68a70e37ee358c2c7",
            "67901ddb81914ca6ab19969277ecdabf",
            "3fbf5f101476406a8b7c621627f8ce8f"
          ]
        },
        "id": "EtQ5q8qvoBLt",
        "outputId": "6103fbe6-037e-48c0-ee96-9aaaae5cfee3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type                      | Params | Mode\n",
            "-----------------------------------------------------------\n",
            "0 | model | VisionEncoderDecoderModel | 210 M  | eval\n",
            "-----------------------------------------------------------\n",
            "210 M     Trainable params\n",
            "0         Non-trainable params\n",
            "210 M     Total params\n",
            "841.466   Total estimated model params size (MB)\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82d1272a2cd5481ca5704290fcb099ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 21.131343841552734\n",
            "train loss: 20.54745864868164\n",
            "train loss: 20.863561630249023\n",
            "train loss: 20.867813110351562\n",
            "train loss: 21.50075912475586\n",
            "train loss: 20.30603790283203\n",
            "train loss: 14.674810409545898\n",
            "train loss: 13.685200691223145\n",
            "train loss: 12.17957592010498\n",
            "train loss: 11.411356925964355\n",
            "train loss: 11.215615272521973\n",
            "train loss: 9.365970611572266\n",
            "train loss: 10.317680358886719\n",
            "train loss: 10.664036750793457\n",
            "train loss: 9.447834014892578\n",
            "train loss: 8.53312873840332\n",
            "train loss: 9.072164535522461\n",
            "train loss: 8.35280704498291\n",
            "train loss: 8.414700508117676\n",
            "train loss: 8.42129135131836\n",
            "train loss: 7.576664924621582\n",
            "train loss: 7.337076187133789\n",
            "train loss: 7.344854831695557\n",
            "train loss: 7.3509111404418945\n",
            "train loss: 7.231627941131592\n",
            "train loss: 7.046614170074463\n",
            "train loss: 6.748353958129883\n",
            "train loss: 6.662676811218262\n",
            "train loss: 6.0243449211120605\n",
            "train loss: 6.2848591804504395\n",
            "train loss: 5.834998607635498\n",
            "train loss: 5.856175422668457\n",
            "train loss: 5.849306583404541\n",
            "train loss: 5.760014533996582\n",
            "train loss: 5.5794596672058105\n",
            "train loss: 5.404055118560791\n",
            "train loss: 5.439591407775879\n",
            "train loss: 5.059661388397217\n",
            "train loss: 5.3821702003479\n",
            "train loss: 5.437144756317139\n",
            "train loss: 4.916604518890381\n",
            "train loss: 4.925980091094971\n",
            "train loss: 4.790709018707275\n",
            "train loss: 4.814319133758545\n",
            "train loss: 4.669803142547607\n",
            "train loss: 4.727206707000732\n",
            "train loss: 4.399510860443115\n",
            "train loss: 4.4364399909973145\n",
            "train loss: 4.609750747680664\n",
            "train loss: 4.558109760284424\n",
            "train loss: 4.285938262939453\n",
            "train loss: 4.440366268157959\n",
            "train loss: 4.468109607696533\n",
            "train loss: 4.394528865814209\n",
            "train loss: 4.795083045959473\n",
            "train loss: 4.369666576385498\n",
            "train loss: 4.26497745513916\n",
            "train loss: 4.222238540649414\n",
            "train loss: 4.800137042999268\n",
            "train loss: 5.368415355682373\n",
            "train loss: 4.34702205657959\n",
            "train loss: 4.319826602935791\n",
            "train loss: 3.9255683422088623\n",
            "train loss: 4.076183319091797\n",
            "train loss: 4.087005138397217\n",
            "train loss: 4.1870598793029785\n",
            "train loss: 3.8046817779541016\n",
            "train loss: 4.1934709548950195\n",
            "train loss: 4.06036376953125\n",
            "train loss: 4.707652568817139\n",
            "train loss: 3.9779298305511475\n",
            "train loss: 4.109914302825928\n",
            "train loss: 3.8824546337127686\n",
            "train loss: 3.8813939094543457\n",
            "train loss: 4.0879225730896\n",
            "train loss: 3.8577721118927\n",
            "train loss: 3.6554207801818848\n",
            "train loss: 3.6952545642852783\n",
            "train loss: 3.608180284500122\n",
            "train loss: 3.6545372009277344\n",
            "train loss: 3.9303910732269287\n",
            "train loss: 3.678467035293579\n",
            "train loss: 4.7497735023498535\n",
            "train loss: 3.775054693222046\n",
            "train loss: 3.801506757736206\n",
            "train loss: 3.5606117248535156\n",
            "train loss: 3.692737102508545\n",
            "train loss: 4.030919551849365\n",
            "train loss: 3.750300645828247\n",
            "train loss: 3.9894628524780273\n",
            "train loss: 3.868014335632324\n",
            "train loss: 3.605293035507202\n",
            "train loss: 3.835968017578125\n",
            "train loss: 3.6967475414276123\n",
            "train loss: 3.8640859127044678\n",
            "train loss: 3.7432727813720703\n",
            "train loss: 3.7014384269714355\n",
            "train loss: 3.538691282272339\n",
            "train loss: 4.012235641479492\n",
            "train loss: 3.706590175628662\n",
            "train loss: 3.8421096801757812\n",
            "train loss: 3.6927878856658936\n",
            "train loss: 3.7541377544403076\n",
            "train loss: 3.937190055847168\n",
            "train loss: 4.309323310852051\n",
            "train loss: 3.865882635116577\n",
            "train loss: 3.474526882171631\n",
            "train loss: 3.7869045734405518\n",
            "train loss: 3.8593814373016357\n",
            "train loss: 3.5731899738311768\n",
            "train loss: 3.576012134552002\n",
            "train loss: 3.723266839981079\n",
            "train loss: 3.4733686447143555\n",
            "train loss: 3.763352870941162\n",
            "train loss: 3.494539499282837\n",
            "train loss: 3.5272319316864014\n",
            "train loss: 3.6802356243133545\n",
            "train loss: 3.4576008319854736\n",
            "train loss: 3.4945108890533447\n",
            "train loss: 3.6027488708496094\n",
            "train loss: 3.6077992916107178\n",
            "train loss: 3.570770263671875\n",
            "train loss: 4.250838756561279\n",
            "train loss: 3.621983051300049\n",
            "train loss: 3.4359359741210938\n",
            "train loss: 3.3593881130218506\n",
            "train loss: 3.4120495319366455\n",
            "train loss: 3.674835205078125\n",
            "train loss: 3.6237761974334717\n",
            "train loss: 3.5595860481262207\n",
            "train loss: 3.3980965614318848\n",
            "train loss: 3.635289192199707\n",
            "train loss: 3.5251128673553467\n",
            "train loss: 3.4432973861694336\n",
            "train loss: 3.3707523345947266\n",
            "train loss: 3.256618022918701\n",
            "train loss: 3.4805715084075928\n",
            "train loss: 3.5795884132385254\n",
            "train loss: 3.5332589149475098\n",
            "train loss: 3.2840323448181152\n",
            "train loss: 3.622303009033203\n",
            "train loss: 3.4553067684173584\n",
            "train loss: 3.554462432861328\n",
            "train loss: 3.3105309009552\n",
            "train loss: 3.464961528778076\n",
            "train loss: 3.656559467315674\n",
            "train loss: 3.408487558364868\n",
            "train loss: 3.5586342811584473\n",
            "train loss: 3.384843349456787\n",
            "train loss: 3.5120229721069336\n",
            "train loss: 3.2973291873931885\n",
            "train loss: 3.346115827560425\n",
            "train loss: 3.306741952896118\n",
            "train loss: 3.2421109676361084\n",
            "train loss: 3.4060492515563965\n",
            "train loss: 3.341139316558838\n",
            "train loss: 3.329836368560791\n",
            "train loss: 3.441166877746582\n",
            "train loss: 3.458329677581787\n",
            "train loss: 3.316425085067749\n",
            "train loss: 3.523632526397705\n",
            "train loss: 3.6915805339813232\n",
            "train loss: 3.2501816749572754\n",
            "train loss: 3.5995538234710693\n",
            "train loss: 3.1918063163757324\n",
            "train loss: 3.428793430328369\n",
            "train loss: 3.3168060779571533\n",
            "train loss: 3.668830633163452\n",
            "train loss: 3.242185592651367\n",
            "train loss: 3.2948830127716064\n",
            "train loss: 3.2243943214416504\n",
            "train loss: 3.344146251678467\n",
            "train loss: 3.0455591678619385\n",
            "train loss: 3.5918710231781006\n",
            "train loss: 3.385185480117798\n",
            "train loss: 3.358222484588623\n",
            "train loss: 3.4465909004211426\n",
            "train loss: 3.3748865127563477\n",
            "train loss: 3.520230293273926\n",
            "train loss: 3.199205160140991\n",
            "train loss: 3.3333399295806885\n",
            "train loss: 3.3629724979400635\n",
            "train loss: 3.2749664783477783\n",
            "train loss: 3.3327088356018066\n",
            "train loss: 3.4518918991088867\n",
            "train loss: 3.8037824630737305\n",
            "train loss: 3.785663366317749\n",
            "train loss: 3.5100042819976807\n",
            "train loss: 3.441378116607666\n",
            "train loss: 3.7327170372009277\n",
            "train loss: 3.379423141479492\n",
            "train loss: 3.165682077407837\n",
            "train loss: 3.2992024421691895\n",
            "train loss: 3.4210681915283203\n",
            "train loss: 3.41235613822937\n",
            "train loss: 3.542379140853882\n",
            "train loss: 3.6868839263916016\n",
            "train loss: 3.386016845703125\n",
            "train loss: 3.644451141357422\n",
            "train loss: 3.885777473449707\n",
            "train loss: 3.3534250259399414\n",
            "train loss: 3.424558401107788\n",
            "train loss: 3.212102174758911\n",
            "train loss: 3.3569860458374023\n",
            "train loss: 3.4522171020507812\n",
            "train loss: 3.2654294967651367\n",
            "train loss: 3.441977024078369\n",
            "train loss: 3.3216495513916016\n",
            "train loss: 3.173027992248535\n",
            "train loss: 3.2241604328155518\n",
            "train loss: 3.303062915802002\n",
            "train loss: 3.33663272857666\n",
            "train loss: 3.43287992477417\n",
            "train loss: 3.2851827144622803\n",
            "train loss: 3.331991195678711\n",
            "train loss: 3.293705940246582\n",
            "train loss: 3.377227306365967\n",
            "train loss: 3.3933284282684326\n",
            "train loss: 3.4615161418914795\n",
            "train loss: 3.340226173400879\n",
            "train loss: 3.2818973064422607\n",
            "train loss: 3.291628837585449\n",
            "train loss: 3.295224666595459\n",
            "train loss: 3.1735265254974365\n",
            "train loss: 3.198056936264038\n",
            "train loss: 3.394287586212158\n",
            "train loss: 3.205564022064209\n",
            "train loss: 3.257422685623169\n",
            "train loss: 3.4433932304382324\n",
            "train loss: 3.2632064819335938\n",
            "train loss: 3.3070366382598877\n",
            "train loss: 3.241309642791748\n",
            "train loss: 3.366384744644165\n",
            "train loss: 3.4627459049224854\n",
            "train loss: 3.22930645942688\n",
            "train loss: 3.2547638416290283\n",
            "train loss: 3.1896932125091553\n",
            "train loss: 3.3727967739105225\n",
            "train loss: 3.1489453315734863\n",
            "train loss: 3.3462274074554443\n",
            "train loss: 3.4301917552948\n",
            "train loss: 3.1214890480041504\n",
            "train loss: 3.083468198776245\n",
            "train loss: 3.063155174255371\n",
            "train loss: 3.141101598739624\n",
            "train loss: 3.366488456726074\n",
            "train loss: 3.5330116748809814\n",
            "train loss: 3.39509654045105\n",
            "train loss: 3.547487735748291\n",
            "train loss: 3.426417827606201\n",
            "train loss: 3.443484306335449\n",
            "train loss: 3.3877663612365723\n",
            "train loss: 3.251951217651367\n",
            "train loss: 3.406694173812866\n",
            "train loss: 3.6626272201538086\n",
            "train loss: 3.3805458545684814\n",
            "train loss: 3.2670531272888184\n",
            "train loss: 3.2711079120635986\n",
            "train loss: 3.1618852615356445\n",
            "train loss: 3.2426507472991943\n",
            "train loss: 3.5535223484039307\n",
            "train loss: 3.224642753601074\n",
            "train loss: 3.135517120361328\n",
            "train loss: 3.2785167694091797\n",
            "train loss: 3.4524693489074707\n",
            "train loss: 3.380122184753418\n",
            "train loss: 3.1080968379974365\n",
            "train loss: 3.139423131942749\n",
            "train loss: 3.1873395442962646\n",
            "train loss: 3.722792863845825\n",
            "train loss: 3.3503127098083496\n",
            "train loss: 3.283360242843628\n",
            "train loss: 3.0429117679595947\n",
            "train loss: 3.5434730052948\n",
            "train loss: 3.291609525680542\n",
            "train loss: 3.3648810386657715\n",
            "train loss: 3.1170077323913574\n",
            "train loss: 3.5139482021331787\n",
            "train loss: 3.2890424728393555\n",
            "train loss: 3.0296578407287598\n",
            "train loss: 3.2726573944091797\n",
            "train loss: 3.090733528137207\n",
            "train loss: 3.7277278900146484\n",
            "train loss: 3.292959451675415\n",
            "train loss: 3.1467931270599365\n",
            "train loss: 3.365654945373535\n",
            "train loss: 3.1466622352600098\n",
            "train loss: 3.2983458042144775\n",
            "train loss: 3.6157262325286865\n",
            "train loss: 3.564636707305908\n",
            "train loss: 3.112626552581787\n",
            "train loss: 3.2430408000946045\n",
            "train loss: 3.330120325088501\n",
            "train loss: 3.0891451835632324\n",
            "train loss: 3.2810726165771484\n",
            "train loss: 3.298586130142212\n",
            "train loss: 3.162161350250244\n",
            "train loss: 3.166398763656616\n",
            "train loss: 3.006958246231079\n",
            "train loss: 3.6545426845550537\n",
            "train loss: 3.396073579788208\n",
            "train loss: 3.0948095321655273\n",
            "train loss: 3.1519598960876465\n",
            "train loss: 3.155783176422119\n",
            "train loss: 3.191635847091675\n",
            "train loss: 3.1253228187561035\n",
            "train loss: 3.1800150871276855\n",
            "train loss: 3.263611078262329\n",
            "train loss: 3.1298654079437256\n",
            "train loss: 3.2131030559539795\n",
            "train loss: 3.660015821456909\n",
            "train loss: 3.3498756885528564\n",
            "train loss: 3.1058833599090576\n",
            "train loss: 3.7555007934570312\n",
            "train loss: 3.0795819759368896\n",
            "train loss: 3.162383556365967\n",
            "train loss: 3.224480628967285\n",
            "train loss: 3.604828357696533\n",
            "train loss: 3.144650936126709\n",
            "train loss: 3.093748092651367\n",
            "train loss: 3.3562679290771484\n",
            "train loss: 3.125453472137451\n",
            "train loss: 3.0330193042755127\n",
            "train loss: 2.9681448936462402\n",
            "train loss: 3.19830060005188\n",
            "train loss: 3.492307186126709\n",
            "train loss: 3.29433536529541\n",
            "train loss: 3.0667977333068848\n",
            "train loss: 3.2356345653533936\n",
            "train loss: 3.055781602859497\n",
            "train loss: 3.1961724758148193\n",
            "train loss: 3.2771217823028564\n",
            "train loss: 3.1897807121276855\n",
            "train loss: 3.1294305324554443\n",
            "train loss: 3.1002256870269775\n",
            "train loss: 3.4382872581481934\n",
            "train loss: 3.143630027770996\n",
            "train loss: 2.8964474201202393\n",
            "train loss: 3.432020902633667\n",
            "train loss: 3.254340887069702\n",
            "train loss: 3.121290922164917\n",
            "train loss: 3.347841262817383\n",
            "train loss: 3.638444662094116\n",
            "train loss: 3.255176067352295\n",
            "train loss: 3.3731698989868164\n",
            "train loss: 2.9897356033325195\n",
            "train loss: 3.2084901332855225\n",
            "train loss: 3.263711929321289\n",
            "train loss: 3.658658504486084\n",
            "train loss: 2.9934608936309814\n",
            "train loss: 3.219315528869629\n",
            "train loss: 3.163553476333618\n",
            "train loss: 3.0564520359039307\n",
            "train loss: 3.240679979324341\n",
            "train loss: 3.6488001346588135\n",
            "train loss: 3.1480307579040527\n",
            "train loss: 2.998551845550537\n",
            "train loss: 3.095522403717041\n",
            "train loss: 3.195941686630249\n",
            "train loss: 3.1392316818237305\n",
            "train loss: 3.3993947505950928\n",
            "train loss: 3.0492372512817383\n",
            "train loss: 3.22217059135437\n",
            "train loss: 3.1941685676574707\n",
            "train loss: 2.94736909866333\n",
            "train loss: 3.125981330871582\n",
            "train loss: 3.329599618911743\n",
            "train loss: 3.269498109817505\n",
            "train loss: 2.974055051803589\n",
            "train loss: 3.6105546951293945\n",
            "train loss: 3.1342740058898926\n",
            "train loss: 3.148869276046753\n",
            "train loss: 2.9693763256073\n",
            "train loss: 3.10425066947937\n",
            "train loss: 3.0756218433380127\n",
            "train loss: 3.307248592376709\n",
            "train loss: 2.9744064807891846\n",
            "train loss: 3.1833133697509766\n",
            "train loss: 3.714724540710449\n",
            "train loss: 3.3113725185394287\n",
            "train loss: 3.090510368347168\n",
            "train loss: 3.1356186866760254\n",
            "train loss: 3.1408395767211914\n",
            "train loss: 3.2532100677490234\n",
            "train loss: 3.034440517425537\n",
            "train loss: 3.1546647548675537\n",
            "train loss: 3.24871563911438\n",
            "train loss: 3.088090658187866\n",
            "train loss: 3.163116931915283\n",
            "train loss: 4.120400428771973\n",
            "train loss: 3.535907030105591\n",
            "train loss: 2.941690683364868\n",
            "train loss: 3.3073923587799072\n",
            "train loss: 3.146456241607666\n",
            "train loss: 3.244352340698242\n",
            "train loss: 3.1514644622802734\n",
            "train loss: 3.0243139266967773\n",
            "train loss: 2.9955990314483643\n",
            "train loss: 3.081608772277832\n",
            "train loss: 3.105942964553833\n",
            "train loss: 2.987593412399292\n",
            "train loss: 3.026038885116577\n",
            "train loss: 3.1659653186798096\n",
            "train loss: 2.973508358001709\n",
            "train loss: 3.1820244789123535\n",
            "train loss: 2.960857391357422\n",
            "train loss: 3.0816121101379395\n",
            "train loss: 3.1109166145324707\n",
            "train loss: 3.0237019062042236\n",
            "train loss: 2.969674587249756\n",
            "train loss: 3.3915328979492188\n",
            "train loss: 3.1399338245391846\n",
            "train loss: 3.171041488647461\n",
            "train loss: 2.9835002422332764\n",
            "train loss: 3.0126514434814453\n",
            "train loss: 3.2009589672088623\n",
            "train loss: 3.331129312515259\n",
            "train loss: 3.0104918479919434\n",
            "train loss: 3.115342378616333\n",
            "train loss: 3.085137367248535\n",
            "train loss: 3.208662986755371\n",
            "train loss: 3.0823590755462646\n",
            "train loss: 3.099306583404541\n",
            "train loss: 3.1127560138702393\n",
            "train loss: 3.073300361633301\n",
            "train loss: 3.092451333999634\n",
            "train loss: 3.0248055458068848\n",
            "train loss: 3.474867582321167\n",
            "train loss: 2.998945713043213\n",
            "train loss: 3.2215576171875\n",
            "train loss: 3.094090461730957\n",
            "train loss: 3.120911121368408\n",
            "train loss: 3.2494940757751465\n",
            "train loss: 2.9983038902282715\n",
            "train loss: 3.112826108932495\n",
            "train loss: 2.9594266414642334\n",
            "train loss: 3.4560670852661133\n",
            "train loss: 3.007401466369629\n",
            "train loss: 3.2169296741485596\n",
            "train loss: 3.544837236404419\n",
            "train loss: 3.2019736766815186\n",
            "train loss: 3.116124153137207\n",
            "train loss: 2.8410067558288574\n",
            "train loss: 3.154085397720337\n",
            "train loss: 3.216749906539917\n",
            "train loss: 3.1428580284118652\n",
            "train loss: 3.032176971435547\n",
            "train loss: 3.083247661590576\n",
            "train loss: 3.0705814361572266\n",
            "train loss: 2.9909374713897705\n",
            "train loss: 3.0046157836914062\n",
            "train loss: 3.675814628601074\n",
            "train loss: 3.084529161453247\n",
            "train loss: 3.065403938293457\n",
            "train loss: 3.0293822288513184\n",
            "train loss: 3.107264280319214\n",
            "train loss: 3.195347309112549\n",
            "train loss: 3.259613275527954\n",
            "train loss: 2.8214030265808105\n",
            "train loss: 3.0660839080810547\n",
            "train loss: 2.9949448108673096\n",
            "train loss: 3.1073098182678223\n",
            "train loss: 3.1626241207122803\n",
            "train loss: 3.06844425201416\n",
            "train loss: 3.22426700592041\n",
            "train loss: 3.0742874145507812\n",
            "train loss: 3.282317876815796\n",
            "train loss: 3.066330671310425\n",
            "train loss: 2.918956756591797\n",
            "train loss: 2.996687650680542\n",
            "train loss: 3.333920478820801\n",
            "train loss: 3.1384596824645996\n",
            "train loss: 3.0934391021728516\n",
            "train loss: 3.018584728240967\n",
            "train loss: 3.392042636871338\n",
            "train loss: 3.7305431365966797\n",
            "train loss: 2.9134325981140137\n",
            "train loss: 3.0630064010620117\n",
            "train loss: 3.3240933418273926\n",
            "train loss: 3.0836620330810547\n",
            "train loss: 3.3713810443878174\n",
            "train loss: 3.3603062629699707\n",
            "train loss: 3.39711856842041\n",
            "train loss: 3.044708728790283\n",
            "train loss: 3.0045511722564697\n",
            "train loss: 3.04299259185791\n",
            "train loss: 3.0851292610168457\n",
            "train loss: 3.271674156188965\n",
            "train loss: 3.2387003898620605\n",
            "train loss: 3.6032981872558594\n",
            "train loss: 3.1594862937927246\n",
            "train loss: 3.2298970222473145\n",
            "train loss: 3.0110135078430176\n",
            "train loss: 3.1290738582611084\n",
            "train loss: 3.1467838287353516\n",
            "train loss: 3.1770308017730713\n",
            "train loss: 2.9315521717071533\n",
            "train loss: 2.922661542892456\n",
            "train loss: 2.9554741382598877\n",
            "train loss: 3.2068979740142822\n",
            "train loss: 2.9552571773529053\n",
            "train loss: 3.041602611541748\n",
            "train loss: 2.9796788692474365\n",
            "train loss: 3.0513625144958496\n",
            "train loss: 2.9988667964935303\n",
            "train loss: 3.021219253540039\n",
            "train loss: 3.233727216720581\n",
            "train loss: 3.012942314147949\n",
            "train loss: 2.960752010345459\n",
            "train loss: 3.3288402557373047\n",
            "train loss: 3.007956027984619\n",
            "train loss: 3.000763177871704\n",
            "train loss: 3.0048022270202637\n",
            "train loss: 3.0173909664154053\n",
            "train loss: 2.995579242706299\n",
            "train loss: 3.2111446857452393\n",
            "train loss: 3.0943405628204346\n",
            "train loss: 3.1795902252197266\n",
            "train loss: 3.2305448055267334\n",
            "train loss: 3.068990707397461\n",
            "train loss: 3.0274124145507812\n",
            "train loss: 3.0738651752471924\n",
            "train loss: 3.750012159347534\n",
            "train loss: 3.002692937850952\n",
            "train loss: 2.987955093383789\n",
            "train loss: 3.236448287963867\n",
            "train loss: 2.982882499694824\n",
            "train loss: 2.8269975185394287\n",
            "train loss: 3.026268482208252\n",
            "train loss: 3.069756507873535\n",
            "train loss: 2.954648733139038\n",
            "train loss: 3.4934322834014893\n",
            "train loss: 3.454955816268921\n",
            "train loss: 2.9401345252990723\n",
            "train loss: 2.9260776042938232\n",
            "train loss: 3.0347325801849365\n",
            "train loss: 3.0129966735839844\n",
            "train loss: 2.9663429260253906\n",
            "train loss: 3.0288681983947754\n",
            "train loss: 2.866316795349121\n",
            "train loss: 3.2501378059387207\n",
            "train loss: 3.063817262649536\n",
            "train loss: 3.0274107456207275\n",
            "train loss: 3.058202028274536\n",
            "train loss: 3.0854196548461914\n",
            "train loss: 2.8975746631622314\n",
            "train loss: 3.0841681957244873\n",
            "train loss: 2.9623005390167236\n",
            "train loss: 2.9514293670654297\n",
            "train loss: 2.9647955894470215\n",
            "train loss: 3.003459930419922\n",
            "train loss: 2.917278528213501\n",
            "train loss: 3.0148539543151855\n",
            "train loss: 3.0384349822998047\n",
            "train loss: 3.0515103340148926\n",
            "train loss: 3.025763750076294\n",
            "train loss: 2.8419370651245117\n",
            "train loss: 3.6767044067382812\n",
            "train loss: 3.0321390628814697\n",
            "train loss: 3.1596546173095703\n",
            "train loss: 3.05863094329834\n",
            "train loss: 2.9065299034118652\n",
            "train loss: 3.1260364055633545\n",
            "train loss: 2.8985249996185303\n",
            "train loss: 3.3527510166168213\n",
            "train loss: 3.0199925899505615\n",
            "train loss: 3.3095078468322754\n",
            "train loss: 3.2673799991607666\n",
            "train loss: 3.1762893199920654\n",
            "train loss: 3.083167552947998\n",
            "train loss: 3.147376298904419\n",
            "train loss: 3.0983641147613525\n",
            "train loss: 3.1287782192230225\n",
            "train loss: 3.1871039867401123\n",
            "train loss: 2.9005024433135986\n",
            "train loss: 2.861039876937866\n",
            "train loss: 2.948636770248413\n",
            "train loss: 3.023175001144409\n",
            "train loss: 3.039281129837036\n",
            "train loss: 2.9471957683563232\n",
            "train loss: 2.8445892333984375\n",
            "train loss: 3.066108226776123\n",
            "train loss: 2.995901346206665\n",
            "train loss: 2.963942050933838\n",
            "train loss: 2.982865810394287\n",
            "train loss: 3.2123687267303467\n",
            "train loss: 2.9338459968566895\n",
            "train loss: 2.9804270267486572\n",
            "train loss: 3.1170265674591064\n",
            "train loss: 3.046029567718506\n",
            "train loss: 3.0659637451171875\n",
            "train loss: 2.936330556869507\n",
            "train loss: 3.028384208679199\n",
            "train loss: 3.0088562965393066\n",
            "train loss: 2.85537052154541\n",
            "train loss: 3.106482744216919\n",
            "train loss: 3.1135871410369873\n",
            "train loss: 2.9495818614959717\n",
            "train loss: 3.1753885746002197\n",
            "train loss: 3.1050093173980713\n",
            "train loss: 2.909836769104004\n",
            "train loss: 2.9648208618164062\n",
            "train loss: 3.044113874435425\n",
            "train loss: 2.8705058097839355\n",
            "train loss: 3.111514091491699\n",
            "train loss: 3.0326695442199707\n",
            "train loss: 3.007312536239624\n",
            "train loss: 2.9068350791931152\n",
            "train loss: 2.9885611534118652\n",
            "train loss: 3.127743721008301\n",
            "train loss: 3.461683988571167\n",
            "train loss: 3.0699517726898193\n",
            "train loss: 3.034132719039917\n",
            "train loss: 2.9810895919799805\n",
            "train loss: 3.159313678741455\n",
            "train loss: 3.006523370742798\n",
            "train loss: 3.0799238681793213\n",
            "train loss: 2.87613582611084\n",
            "train loss: 3.0952324867248535\n",
            "train loss: 2.9345149993896484\n",
            "train loss: 3.060774564743042\n",
            "train loss: 2.8350656032562256\n",
            "train loss: 3.0140581130981445\n",
            "train loss: 3.230875015258789\n",
            "train loss: 3.042724847793579\n",
            "train loss: 3.009486198425293\n",
            "train loss: 2.8168675899505615\n",
            "train loss: 3.0692062377929688\n",
            "train loss: 2.9648678302764893\n",
            "train loss: 2.9715018272399902\n",
            "train loss: 2.9540655612945557\n",
            "train loss: 2.8749277591705322\n",
            "train loss: 2.9265854358673096\n",
            "train loss: 2.8361544609069824\n",
            "train loss: 3.164517879486084\n",
            "train loss: 2.9186975955963135\n",
            "train loss: 2.9866786003112793\n",
            "train loss: 3.0165719985961914\n",
            "train loss: 2.9657392501831055\n",
            "train loss: 2.926772356033325\n",
            "train loss: 2.9851202964782715\n",
            "train loss: 2.9979069232940674\n",
            "train loss: 3.0420007705688477\n",
            "train loss: 3.0105044841766357\n",
            "train loss: 2.8946547508239746\n",
            "train loss: 3.064578056335449\n",
            "train loss: 3.2969934940338135\n",
            "train loss: 3.007871389389038\n",
            "train loss: 2.9162251949310303\n",
            "train loss: 2.943225622177124\n",
            "train loss: 3.048342704772949\n",
            "train loss: 3.044529676437378\n",
            "train loss: 3.122279644012451\n",
            "train loss: 2.91859769821167\n",
            "train loss: 3.1103971004486084\n",
            "train loss: 2.8834590911865234\n",
            "train loss: 3.0931906700134277\n",
            "train loss: 4.0017499923706055\n",
            "train loss: 3.000427722930908\n",
            "train loss: 3.138411283493042\n",
            "train loss: 2.887993574142456\n",
            "train loss: 2.833484649658203\n",
            "train loss: 2.893125057220459\n",
            "train loss: 2.862769365310669\n",
            "train loss: 2.9949591159820557\n",
            "train loss: 3.156520128250122\n",
            "train loss: 3.200220823287964\n",
            "train loss: 2.871495008468628\n",
            "train loss: 2.9882571697235107\n",
            "train loss: 2.9280428886413574\n",
            "train loss: 3.1072604656219482\n",
            "train loss: 2.839071750640869\n",
            "train loss: 3.1382956504821777\n",
            "train loss: 3.0024254322052\n",
            "train loss: 3.0953688621520996\n",
            "train loss: 2.7889840602874756\n",
            "train loss: 3.055830955505371\n",
            "train loss: 2.8494017124176025\n",
            "train loss: 3.121246814727783\n",
            "train loss: 3.337554454803467\n",
            "train loss: 2.932554244995117\n",
            "train loss: 3.1264145374298096\n",
            "train loss: 2.8405730724334717\n",
            "train loss: 2.8665149211883545\n",
            "train loss: 3.0969386100769043\n",
            "train loss: 2.9476687908172607\n",
            "train loss: 3.1006040573120117\n",
            "train loss: 2.8784239292144775\n",
            "train loss: 2.7879750728607178\n",
            "train loss: 2.9809768199920654\n",
            "train loss: 2.9870715141296387\n",
            "train loss: 2.823310613632202\n",
            "train loss: 3.492295742034912\n",
            "train loss: 2.848538398742676\n",
            "train loss: 3.041325807571411\n",
            "train loss: 3.026996612548828\n",
            "train loss: 3.2047250270843506\n",
            "train loss: 2.8481407165527344\n",
            "train loss: 3.0762031078338623\n",
            "train loss: 3.076768159866333\n",
            "train loss: 3.19208025932312\n",
            "train loss: 3.2622570991516113\n",
            "train loss: 2.8958914279937744\n",
            "train loss: 3.015800714492798\n",
            "train loss: 3.0400502681732178\n",
            "train loss: 2.8980515003204346\n",
            "train loss: 2.9072964191436768\n",
            "train loss: 2.9604177474975586\n",
            "train loss: 3.2196555137634277\n",
            "train loss: 2.910658597946167\n",
            "train loss: 2.952993631362915\n",
            "train loss: 2.9605860710144043\n",
            "train loss: 3.1448304653167725\n",
            "train loss: 2.8563883304595947\n",
            "train loss: 2.8608546257019043\n",
            "train loss: 2.951289653778076\n",
            "train loss: 2.846468210220337\n",
            "train loss: 3.0164289474487305\n",
            "train loss: 2.8609108924865723\n",
            "train loss: 2.915940523147583\n",
            "train loss: 2.9056179523468018\n",
            "train loss: 3.0588812828063965\n",
            "train loss: 3.0265953540802\n",
            "train loss: 2.8381776809692383\n",
            "train loss: 2.956116199493408\n",
            "train loss: 3.0469436645507812\n",
            "train loss: 3.044097661972046\n",
            "train loss: 2.958667755126953\n",
            "train loss: 2.9414541721343994\n",
            "train loss: 3.0116524696350098\n",
            "train loss: 3.003131628036499\n",
            "train loss: 3.1509318351745605\n",
            "train loss: 2.9275319576263428\n",
            "train loss: 2.9125614166259766\n",
            "train loss: 2.9286749362945557\n",
            "train loss: 2.9804203510284424\n",
            "train loss: 2.9001271724700928\n",
            "train loss: 3.0936317443847656\n",
            "train loss: 2.986555337905884\n",
            "train loss: 3.099536418914795\n",
            "train loss: 2.918466567993164\n",
            "train loss: 3.6219558715820312\n",
            "train loss: 3.0553090572357178\n",
            "train loss: 3.2767953872680664\n",
            "train loss: 2.9800658226013184\n",
            "train loss: 2.9669575691223145\n",
            "train loss: 2.974353313446045\n",
            "train loss: 3.1422524452209473\n",
            "train loss: 3.043295383453369\n",
            "train loss: 3.059814453125\n",
            "train loss: 2.9749984741210938\n",
            "train loss: 2.991283416748047\n",
            "train loss: 3.0679187774658203\n",
            "train loss: 3.422804832458496\n",
            "train loss: 3.1136651039123535\n",
            "train loss: 2.8951776027679443\n",
            "train loss: 3.1283609867095947\n",
            "train loss: 3.049072027206421\n",
            "train loss: 2.8963959217071533\n",
            "train loss: 3.152031898498535\n",
            "train loss: 3.244602918624878\n",
            "train loss: 2.9498822689056396\n",
            "train loss: 3.2648963928222656\n",
            "train loss: 2.896291732788086\n",
            "train loss: 2.917311668395996\n",
            "train loss: 2.921849250793457\n",
            "train loss: 2.9252166748046875\n",
            "train loss: 3.087655544281006\n",
            "train loss: 3.0245344638824463\n",
            "train loss: 2.9799516201019287\n",
            "train loss: 3.1573667526245117\n",
            "train loss: 2.914825677871704\n",
            "train loss: 2.920659303665161\n",
            "train loss: 2.9892029762268066\n",
            "train loss: 2.9982552528381348\n",
            "train loss: 3.015543222427368\n",
            "train loss: 2.887103319168091\n",
            "train loss: 2.9444868564605713\n",
            "train loss: 3.0309998989105225\n",
            "train loss: 2.956852674484253\n",
            "train loss: 2.8860740661621094\n",
            "train loss: 3.070199966430664\n",
            "train loss: 2.9956002235412598\n",
            "train loss: 2.856226921081543\n",
            "train loss: 2.8914451599121094\n",
            "train loss: 2.894447088241577\n",
            "train loss: 2.9810996055603027\n",
            "train loss: 2.844398021697998\n",
            "train loss: 2.8725597858428955\n",
            "train loss: 2.8813705444335938\n",
            "train loss: 2.8540661334991455\n",
            "train loss: 2.973301887512207\n",
            "train loss: 2.876209259033203\n",
            "train loss: 3.0152394771575928\n",
            "train loss: 3.083317756652832\n",
            "train loss: 2.8809635639190674\n",
            "train loss: 3.0309951305389404\n",
            "train loss: 3.0147809982299805\n",
            "train loss: 3.061790943145752\n",
            "train loss: 2.9132840633392334\n",
            "train loss: 3.0728797912597656\n",
            "train loss: 3.072389602661133\n",
            "train loss: 2.927687644958496\n",
            "train loss: 2.904655933380127\n",
            "train loss: 2.978013753890991\n",
            "train loss: 2.9532864093780518\n",
            "train loss: 2.93691086769104\n",
            "train loss: 2.854887008666992\n",
            "train loss: 2.889793634414673\n",
            "train loss: 3.287341356277466\n",
            "train loss: 2.941758871078491\n",
            "train loss: 2.9881906509399414\n",
            "train loss: 2.939249277114868\n",
            "train loss: 3.132650136947632\n",
            "train loss: 2.915691375732422\n",
            "train loss: 2.853358507156372\n",
            "train loss: 2.945772171020508\n",
            "train loss: 2.89046311378479\n",
            "train loss: 2.977153778076172\n",
            "train loss: 2.952113628387451\n",
            "train loss: 2.95888352394104\n",
            "train loss: 2.798868179321289\n",
            "train loss: 2.9322099685668945\n",
            "train loss: 2.8006067276000977\n",
            "train loss: 2.952895164489746\n",
            "train loss: 3.0410025119781494\n",
            "train loss: 3.022862195968628\n",
            "train loss: 2.75592303276062\n",
            "train loss: 3.0109505653381348\n",
            "train loss: 2.854849100112915\n",
            "train loss: 2.9989945888519287\n",
            "train loss: 2.8312292098999023\n",
            "train loss: 2.8443715572357178\n",
            "train loss: 2.8965625762939453\n",
            "train loss: 2.986931562423706\n",
            "train loss: 2.868851900100708\n",
            "train loss: 2.922917604446411\n",
            "train loss: 3.0548596382141113\n",
            "train loss: 2.915736198425293\n",
            "train loss: 3.0026373863220215\n",
            "train loss: 2.812899351119995\n",
            "train loss: 2.8182990550994873\n",
            "train loss: 2.8992416858673096\n",
            "train loss: 2.857048749923706\n",
            "train loss: 2.8461570739746094\n",
            "train loss: 2.901061534881592\n",
            "train loss: 2.8170292377471924\n",
            "train loss: 2.98036789894104\n",
            "train loss: 2.971705436706543\n",
            "train loss: 3.181420087814331\n",
            "train loss: 3.1277639865875244\n",
            "train loss: 3.005499839782715\n",
            "train loss: 3.001833438873291\n",
            "train loss: 2.8745675086975098\n",
            "train loss: 2.944643259048462\n",
            "train loss: 3.0380547046661377\n",
            "train loss: 2.945604085922241\n",
            "train loss: 2.8754239082336426\n",
            "train loss: 3.002352237701416\n",
            "train loss: 3.0517280101776123\n",
            "train loss: 2.876671314239502\n",
            "train loss: 2.8576221466064453\n",
            "train loss: 2.845331907272339\n",
            "train loss: 2.812290906906128\n",
            "train loss: 2.8722054958343506\n",
            "train loss: 3.047091484069824\n",
            "train loss: 2.9215075969696045\n",
            "train loss: 3.0352394580841064\n",
            "train loss: 2.7093474864959717\n",
            "train loss: 2.9561541080474854\n",
            "train loss: 2.991604804992676\n",
            "train loss: 2.9643502235412598\n",
            "train loss: 2.8322322368621826\n",
            "train loss: 2.965404510498047\n",
            "train loss: 2.8670315742492676\n",
            "train loss: 2.8393473625183105\n",
            "train loss: 2.8481204509735107\n",
            "train loss: 3.0194199085235596\n",
            "train loss: 2.863485336303711\n",
            "train loss: 2.8505420684814453\n",
            "train loss: 2.89821720123291\n",
            "train loss: 2.937605381011963\n",
            "train loss: 2.925774574279785\n",
            "train loss: 3.139512300491333\n",
            "train loss: 3.1494836807250977\n",
            "train loss: 2.9471802711486816\n",
            "train loss: 2.8320045471191406\n",
            "train loss: 2.894240617752075\n",
            "train loss: 2.6626222133636475\n",
            "train loss: 2.907917022705078\n",
            "train loss: 2.933678388595581\n",
            "train loss: 3.644022226333618\n",
            "train loss: 2.8255996704101562\n",
            "train loss: 2.7844808101654053\n",
            "train loss: 3.2497220039367676\n",
            "train loss: 2.920839786529541\n",
            "train loss: 2.9418582916259766\n",
            "train loss: 2.932377338409424\n",
            "train loss: 3.2934625148773193\n",
            "train loss: 2.8911025524139404\n",
            "train loss: 2.984462022781372\n",
            "train loss: 2.948596954345703\n",
            "train loss: 2.9678080081939697\n",
            "train loss: 2.844186305999756\n",
            "train loss: 2.955223560333252\n",
            "train loss: 2.978868007659912\n",
            "train loss: 2.9848926067352295\n",
            "train loss: 3.021169900894165\n",
            "train loss: 2.7070913314819336\n",
            "train loss: 2.8908133506774902\n",
            "train loss: 2.93942928314209\n",
            "train loss: 2.876041889190674\n",
            "train loss: 2.9662749767303467\n",
            "train loss: 2.978685140609741\n",
            "train loss: 3.1493637561798096\n",
            "train loss: 3.037060022354126\n",
            "train loss: 3.0163066387176514\n",
            "train loss: 2.7542505264282227\n",
            "train loss: 2.864971399307251\n",
            "train loss: 2.918922185897827\n",
            "train loss: 3.2554402351379395\n",
            "train loss: 2.9961419105529785\n",
            "train loss: 3.206758975982666\n",
            "train loss: 2.8749547004699707\n",
            "train loss: 2.9549291133880615\n",
            "train loss: 2.9091954231262207\n",
            "train loss: 2.744814872741699\n",
            "train loss: 2.925241470336914\n",
            "train loss: 2.984036684036255\n",
            "train loss: 2.815748691558838\n",
            "train loss: 2.898334264755249\n",
            "train loss: 2.910984992980957\n",
            "train loss: 2.9870967864990234\n",
            "train loss: 2.944371461868286\n",
            "train loss: 2.9924514293670654\n",
            "train loss: 2.9049110412597656\n",
            "train loss: 2.842036008834839\n",
            "train loss: 3.051076889038086\n",
            "train loss: 2.831883192062378\n",
            "train loss: 3.014430284500122\n",
            "train loss: 3.235644578933716\n",
            "train loss: 3.193723440170288\n",
            "train loss: 2.928227186203003\n",
            "train loss: 2.8643553256988525\n",
            "train loss: 2.829416513442993\n",
            "train loss: 2.910815715789795\n",
            "train loss: 3.2867512702941895\n",
            "train loss: 2.8075621128082275\n",
            "train loss: 2.973311185836792\n",
            "train loss: 3.0919418334960938\n",
            "train loss: 2.9117519855499268\n",
            "train loss: 3.13687801361084\n",
            "train loss: 2.7398459911346436\n",
            "train loss: 2.8896729946136475\n",
            "train loss: 2.854780435562134\n",
            "train loss: 2.870455265045166\n",
            "train loss: 2.7378554344177246\n",
            "train loss: 2.853799343109131\n",
            "train loss: 2.832731246948242\n",
            "train loss: 2.874499559402466\n",
            "train loss: 2.761650800704956\n",
            "train loss: 2.8936657905578613\n",
            "train loss: 2.970623254776001\n",
            "train loss: 2.856428861618042\n",
            "train loss: 2.6778151988983154\n",
            "train loss: 2.9996464252471924\n",
            "train loss: 2.8557627201080322\n",
            "train loss: 2.809870958328247\n",
            "train loss: 2.9688823223114014\n",
            "train loss: 3.2026939392089844\n",
            "train loss: 2.882622480392456\n",
            "train loss: 2.959132194519043\n",
            "train loss: 2.913313150405884\n",
            "train loss: 3.1146459579467773\n",
            "train loss: 2.863197088241577\n",
            "train loss: 2.780073404312134\n",
            "train loss: 2.9562134742736816\n",
            "train loss: 2.8593180179595947\n",
            "train loss: 3.0294785499572754\n",
            "train loss: 2.915818214416504\n",
            "train loss: 2.9374186992645264\n",
            "train loss: 2.807407855987549\n",
            "train loss: 2.8681018352508545\n",
            "train loss: 3.144538402557373\n",
            "train loss: 2.835843086242676\n",
            "train loss: 2.8318400382995605\n",
            "train loss: 3.052096128463745\n",
            "train loss: 3.0423357486724854\n",
            "train loss: 2.895991563796997\n",
            "train loss: 2.9284489154815674\n",
            "train loss: 2.9153482913970947\n",
            "train loss: 2.8874545097351074\n",
            "train loss: 2.948972225189209\n",
            "train loss: 2.722322463989258\n",
            "train loss: 3.0579259395599365\n",
            "train loss: 2.7625999450683594\n",
            "train loss: 3.070497751235962\n",
            "train loss: 2.7424585819244385\n",
            "train loss: 2.8867175579071045\n",
            "train loss: 2.9801480770111084\n",
            "train loss: 2.9987869262695312\n",
            "train loss: 2.9162189960479736\n",
            "train loss: 3.2195498943328857\n",
            "train loss: 2.970752716064453\n",
            "train loss: 3.1023759841918945\n",
            "train loss: 3.0117452144622803\n",
            "train loss: 2.841179847717285\n",
            "train loss: 2.919015645980835\n",
            "train loss: 3.0479371547698975\n",
            "train loss: 3.1099114418029785\n",
            "train loss: 3.036444664001465\n",
            "train loss: 3.0684421062469482\n",
            "train loss: 2.915266990661621\n",
            "train loss: 3.0313615798950195\n",
            "train loss: 2.9700937271118164\n",
            "train loss: 3.0315330028533936\n",
            "train loss: 2.8098838329315186\n",
            "train loss: 2.9463272094726562\n",
            "train loss: 2.780503511428833\n",
            "train loss: 2.868565797805786\n",
            "train loss: 3.093264102935791\n",
            "train loss: 3.00028657913208\n",
            "train loss: 2.826608896255493\n",
            "train loss: 3.5872385501861572\n",
            "train loss: 2.7675118446350098\n",
            "train loss: 2.827765464782715\n",
            "train loss: 2.824443817138672\n",
            "train loss: 2.8143508434295654\n",
            "train loss: 2.9562065601348877\n",
            "train loss: 2.9837052822113037\n",
            "train loss: 3.0748605728149414\n",
            "train loss: 2.7431161403656006\n",
            "train loss: 2.8137028217315674\n",
            "train loss: 2.8818814754486084\n",
            "train loss: 2.9141201972961426\n",
            "train loss: 2.8234498500823975\n",
            "train loss: 3.1816534996032715\n",
            "train loss: 2.9672505855560303\n",
            "train loss: 2.9626739025115967\n",
            "train loss: 2.847754955291748\n",
            "train loss: 3.0768816471099854\n",
            "train loss: 2.844848155975342\n",
            "train loss: 2.982638120651245\n",
            "train loss: 2.852825164794922\n",
            "train loss: 2.833040952682495\n",
            "train loss: 2.8170485496520996\n",
            "train loss: 3.3361408710479736\n",
            "train loss: 2.9769911766052246\n",
            "train loss: 2.9873929023742676\n",
            "train loss: 3.059478521347046\n",
            "train loss: 2.9517476558685303\n",
            "train loss: 2.9050188064575195\n",
            "train loss: 2.9284422397613525\n",
            "train loss: 2.8260722160339355\n",
            "train loss: 2.9196648597717285\n",
            "train loss: 2.8209877014160156\n",
            "train loss: 2.976015567779541\n",
            "train loss: 2.756688356399536\n",
            "train loss: 2.9862048625946045\n",
            "train loss: 2.813694953918457\n",
            "train loss: 2.7320053577423096\n",
            "train loss: 2.8062644004821777\n",
            "train loss: 2.8501007556915283\n",
            "train loss: 3.0016860961914062\n",
            "train loss: 2.8460988998413086\n",
            "train loss: 2.9745357036590576\n",
            "train loss: 2.951338291168213\n",
            "train loss: 2.994823455810547\n",
            "train loss: 2.936352252960205\n",
            "train loss: 3.0896801948547363\n",
            "train loss: 2.984596014022827\n",
            "train loss: 2.9880919456481934\n",
            "train loss: 2.993786334991455\n",
            "train loss: 2.8943772315979004\n",
            "train loss: 2.7437548637390137\n",
            "train loss: 2.8593218326568604\n",
            "train loss: 2.787132978439331\n",
            "train loss: 2.9115328788757324\n",
            "train loss: 2.745481491088867\n",
            "train loss: 3.4930531978607178\n",
            "train loss: 2.838866710662842\n",
            "train loss: 3.048328161239624\n",
            "train loss: 2.982635974884033\n",
            "train loss: 3.0083625316619873\n",
            "train loss: 2.9846816062927246\n",
            "train loss: 2.7394349575042725\n",
            "train loss: 2.9763834476470947\n",
            "train loss: 2.8355491161346436\n",
            "train loss: 2.6872243881225586\n",
            "train loss: 2.970734119415283\n",
            "train loss: 2.8039872646331787\n",
            "train loss: 2.841715097427368\n",
            "train loss: 3.2142741680145264\n",
            "train loss: 2.855804920196533\n",
            "train loss: 2.8226940631866455\n",
            "train loss: 2.913342237472534\n",
            "train loss: 2.7859973907470703\n",
            "train loss: 2.9128835201263428\n",
            "train loss: 2.9390716552734375\n",
            "train loss: 3.0580015182495117\n",
            "train loss: 2.8906683921813965\n",
            "train loss: 3.001255750656128\n",
            "train loss: 2.773270606994629\n",
            "train loss: 2.9247312545776367\n",
            "train loss: 3.0892014503479004\n",
            "train loss: 2.773552417755127\n",
            "train loss: 2.8430464267730713\n",
            "train loss: 2.976487636566162\n",
            "train loss: 2.814009428024292\n",
            "train loss: 2.9050674438476562\n",
            "train loss: 2.8137497901916504\n",
            "train loss: 2.8641092777252197\n",
            "train loss: 2.8777403831481934\n",
            "train loss: 2.993345260620117\n",
            "train loss: 2.9315524101257324\n",
            "train loss: 2.8947012424468994\n",
            "train loss: 2.879634380340576\n",
            "train loss: 2.8311767578125\n",
            "train loss: 2.79292893409729\n",
            "train loss: 2.840843677520752\n",
            "train loss: 2.8612029552459717\n",
            "train loss: 2.870194435119629\n",
            "train loss: 2.7984423637390137\n",
            "train loss: 2.9060215950012207\n",
            "train loss: 2.825732946395874\n",
            "train loss: 2.710026741027832\n",
            "train loss: 2.9531452655792236\n",
            "train loss: 2.9124197959899902\n",
            "train loss: 2.7420237064361572\n",
            "train loss: 3.0182578563690186\n",
            "train loss: 2.71576189994812\n",
            "train loss: 2.77404522895813\n",
            "train loss: 2.7954564094543457\n",
            "train loss: 2.7727513313293457\n",
            "train loss: 2.8338241577148438\n",
            "train loss: 2.8572585582733154\n",
            "train loss: 2.834012031555176\n",
            "train loss: 2.858564853668213\n",
            "train loss: 2.9008657932281494\n",
            "train loss: 3.2580325603485107\n",
            "train loss: 3.2396695613861084\n",
            "train loss: 2.975961208343506\n",
            "train loss: 2.829641103744507\n",
            "train loss: 2.956225872039795\n",
            "train loss: 2.95221209526062\n",
            "train loss: 2.9108035564422607\n",
            "train loss: 2.841688871383667\n",
            "train loss: 2.7104392051696777\n",
            "train loss: 2.9907784461975098\n",
            "train loss: 2.8929057121276855\n",
            "train loss: 2.7724204063415527\n",
            "train loss: 2.7640862464904785\n",
            "train loss: 2.918077230453491\n",
            "train loss: 2.771470069885254\n",
            "train loss: 2.7212109565734863\n",
            "train loss: 2.943920373916626\n",
            "train loss: 2.7800538539886475\n",
            "train loss: 2.9400649070739746\n",
            "train loss: 3.057288885116577\n",
            "train loss: 3.0292916297912598\n",
            "train loss: 2.8264379501342773\n",
            "train loss: 2.8514411449432373\n",
            "train loss: 3.0100929737091064\n",
            "train loss: 2.7479231357574463\n",
            "train loss: 2.7902615070343018\n",
            "train loss: 2.826216697692871\n",
            "train loss: 2.6616451740264893\n",
            "train loss: 2.629683256149292\n",
            "train loss: 2.702112913131714\n",
            "train loss: 2.9355480670928955\n",
            "train loss: 2.8045995235443115\n",
            "train loss: 2.8965587615966797\n",
            "train loss: 2.6561195850372314\n",
            "train loss: 2.8699965476989746\n",
            "train loss: 2.799318790435791\n",
            "train loss: 2.9058306217193604\n",
            "train loss: 2.922178268432617\n",
            "train loss: 2.911391019821167\n",
            "train loss: 2.891029119491577\n",
            "train loss: 2.751060962677002\n",
            "train loss: 2.750744104385376\n",
            "train loss: 2.735231637954712\n",
            "train loss: 2.8077428340911865\n",
            "train loss: 2.9327216148376465\n",
            "train loss: 2.776323080062866\n",
            "train loss: 2.891977548599243\n",
            "train loss: 2.9283061027526855\n",
            "train loss: 2.782231569290161\n",
            "train loss: 2.7690999507904053\n",
            "train loss: 2.7887320518493652\n",
            "train loss: 2.8581151962280273\n",
            "train loss: 2.8123786449432373\n",
            "train loss: 2.983372926712036\n",
            "train loss: 2.6827192306518555\n",
            "train loss: 2.750086545944214\n",
            "train loss: 2.6935372352600098\n",
            "train loss: 3.0497987270355225\n",
            "train loss: 2.79280948638916\n",
            "train loss: 2.7344021797180176\n",
            "train loss: 2.8619329929351807\n",
            "train loss: 2.8489770889282227\n",
            "train loss: 2.702270030975342\n",
            "train loss: 3.006308078765869\n",
            "train loss: 2.7365875244140625\n",
            "train loss: 2.7857956886291504\n",
            "train loss: 2.8215174674987793\n",
            "train loss: 2.7950732707977295\n",
            "train loss: 2.8924622535705566\n",
            "train loss: 2.8696701526641846\n",
            "train loss: 2.683478355407715\n",
            "train loss: 3.0442779064178467\n",
            "train loss: 2.8264379501342773\n",
            "train loss: 2.663365364074707\n",
            "train loss: 2.6773507595062256\n",
            "train loss: 2.782221794128418\n",
            "train loss: 2.7876243591308594\n",
            "train loss: 2.850416421890259\n",
            "train loss: 2.6870272159576416\n",
            "train loss: 2.7509207725524902\n",
            "train loss: 2.7360024452209473\n",
            "train loss: 2.9427103996276855\n",
            "train loss: 2.8726694583892822\n",
            "train loss: 2.6890270709991455\n",
            "train loss: 2.783240795135498\n",
            "train loss: 2.824528217315674\n",
            "train loss: 2.724661350250244\n",
            "train loss: 2.6267001628875732\n",
            "train loss: 2.769117832183838\n",
            "train loss: 2.91706919670105\n",
            "train loss: 2.7918899059295654\n",
            "train loss: 2.9051835536956787\n",
            "train loss: 2.8359436988830566\n",
            "train loss: 2.820854663848877\n",
            "train loss: 2.833843231201172\n",
            "train loss: 2.773259401321411\n",
            "train loss: 2.785100221633911\n",
            "train loss: 2.715778350830078\n",
            "train loss: 2.910170555114746\n",
            "train loss: 2.692857503890991\n",
            "train loss: 3.1415789127349854\n",
            "train loss: 2.863560914993286\n",
            "train loss: 2.793278932571411\n",
            "train loss: 2.782233238220215\n",
            "train loss: 3.007159471511841\n",
            "train loss: 2.818948984146118\n",
            "train loss: 2.710132598876953\n",
            "train loss: 2.6204380989074707\n",
            "train loss: 2.6989552974700928\n",
            "train loss: 2.6828715801239014\n",
            "train loss: 2.8036234378814697\n",
            "train loss: 2.6160521507263184\n",
            "train loss: 2.658660888671875\n",
            "train loss: 2.8986730575561523\n",
            "train loss: 2.6895158290863037\n",
            "train loss: 2.7214696407318115\n",
            "train loss: 2.543595314025879\n",
            "train loss: 2.8295652866363525\n",
            "train loss: 2.6540310382843018\n",
            "train loss: 2.8659703731536865\n",
            "train loss: 2.8571078777313232\n",
            "train loss: 2.7863361835479736\n",
            "train loss: 2.928544282913208\n",
            "train loss: 2.777409076690674\n",
            "train loss: 2.7716498374938965\n",
            "train loss: 2.9031529426574707\n",
            "train loss: 2.689972162246704\n",
            "train loss: 2.635483741760254\n",
            "train loss: 2.6664535999298096\n",
            "train loss: 2.8413798809051514\n",
            "train loss: 2.9318175315856934\n",
            "train loss: 2.6178362369537354\n",
            "train loss: 2.6216859817504883\n",
            "train loss: 2.9589715003967285\n",
            "train loss: 2.680743932723999\n",
            "train loss: 2.6662075519561768\n",
            "train loss: 3.2132458686828613\n",
            "train loss: 2.6814417839050293\n",
            "train loss: 2.642998695373535\n",
            "train loss: 2.825019598007202\n",
            "train loss: 2.7218291759490967\n",
            "train loss: 2.92029070854187\n",
            "train loss: 2.7972207069396973\n",
            "train loss: 2.808602809906006\n",
            "train loss: 2.6973400115966797\n",
            "train loss: 2.8632829189300537\n",
            "train loss: 2.6822283267974854\n",
            "train loss: 2.8425753116607666\n",
            "train loss: 2.938051700592041\n",
            "train loss: 2.6871821880340576\n",
            "train loss: 2.7486791610717773\n",
            "train loss: 2.663928508758545\n",
            "train loss: 2.706212043762207\n",
            "train loss: 2.743727922439575\n",
            "train loss: 2.6385257244110107\n",
            "train loss: 2.7433717250823975\n",
            "train loss: 2.5930402278900146\n",
            "train loss: 2.6140217781066895\n",
            "train loss: 2.740839719772339\n",
            "train loss: 2.6085448265075684\n",
            "train loss: 2.9015791416168213\n",
            "train loss: 2.6484689712524414\n",
            "train loss: 2.5717289447784424\n",
            "train loss: 2.8008337020874023\n",
            "train loss: 2.693074941635132\n",
            "train loss: 2.67850399017334\n",
            "train loss: 2.7423219680786133\n",
            "train loss: 2.676604986190796\n",
            "train loss: 2.7103559970855713\n",
            "train loss: 2.7914130687713623\n",
            "train loss: 2.7514922618865967\n",
            "train loss: 2.682095527648926\n",
            "train loss: 2.669721841812134\n",
            "train loss: 2.6835529804229736\n",
            "train loss: 2.6008927822113037\n",
            "train loss: 2.5812041759490967\n",
            "train loss: 2.6954565048217773\n",
            "train loss: 2.7502212524414062\n",
            "train loss: 2.6920320987701416\n",
            "train loss: 2.8137569427490234\n",
            "train loss: 2.5816879272460938\n",
            "train loss: 2.6165900230407715\n",
            "train loss: 2.8637380599975586\n",
            "train loss: 2.602766513824463\n",
            "train loss: 2.6067006587982178\n",
            "train loss: 2.7423155307769775\n",
            "train loss: 2.710402727127075\n",
            "train loss: 2.655768871307373\n",
            "train loss: 2.7429966926574707\n",
            "train loss: 2.8391945362091064\n",
            "train loss: 2.7817015647888184\n",
            "train loss: 2.731734275817871\n",
            "train loss: 2.5975446701049805\n",
            "train loss: 2.8503737449645996\n",
            "train loss: 2.7243454456329346\n",
            "train loss: 2.502904176712036\n",
            "train loss: 2.7696657180786133\n",
            "train loss: 2.7175581455230713\n",
            "train loss: 2.6536686420440674\n",
            "train loss: 2.857759714126587\n",
            "train loss: 2.792144536972046\n",
            "train loss: 2.842811107635498\n",
            "train loss: 3.0433850288391113\n",
            "train loss: 2.7805304527282715\n",
            "train loss: 2.743053436279297\n",
            "train loss: 2.6052300930023193\n",
            "train loss: 2.5077433586120605\n",
            "train loss: 2.4547200202941895\n",
            "train loss: 2.5610930919647217\n",
            "train loss: 2.7122883796691895\n",
            "train loss: 2.5140180587768555\n",
            "train loss: 2.560788631439209\n",
            "train loss: 2.690950870513916\n",
            "train loss: 2.6128828525543213\n",
            "train loss: 2.8644628524780273\n",
            "train loss: 3.1860032081604004\n",
            "train loss: 2.3828837871551514\n",
            "train loss: 2.733330726623535\n",
            "train loss: 2.760134696960449\n",
            "train loss: 2.7072975635528564\n",
            "train loss: 2.547668218612671\n",
            "train loss: 2.4672610759735107\n",
            "train loss: 2.6497254371643066\n",
            "train loss: 3.023970603942871\n",
            "train loss: 2.902813673019409\n",
            "train loss: 2.2771146297454834\n",
            "train loss: 2.5235695838928223\n",
            "train loss: 2.3226125240325928\n",
            "train loss: 2.5317347049713135\n",
            "train loss: 2.6395883560180664\n",
            "train loss: 2.916638135910034\n",
            "train loss: 2.417189359664917\n",
            "train loss: 2.642077684402466\n",
            "train loss: 2.733541250228882\n",
            "train loss: 2.360717296600342\n",
            "train loss: 2.6110596656799316\n",
            "train loss: 2.5138943195343018\n",
            "train loss: 2.4491474628448486\n",
            "train loss: 2.7168989181518555\n",
            "train loss: 2.5199830532073975\n",
            "train loss: 2.4638800621032715\n",
            "train loss: 2.5326943397521973\n",
            "train loss: 2.4573211669921875\n",
            "train loss: 2.5274174213409424\n",
            "train loss: 2.7562644481658936\n",
            "train loss: 2.0910022258758545\n",
            "train loss: 2.472313642501831\n",
            "train loss: 2.3781654834747314\n",
            "train loss: 1.9136615991592407\n",
            "train loss: 2.7837467193603516\n",
            "train loss: 2.697935104370117\n",
            "train loss: 2.6493916511535645\n",
            "train loss: 2.48396372795105\n",
            "train loss: 2.4537110328674316\n",
            "train loss: 2.535731315612793\n",
            "train loss: 2.4497733116149902\n",
            "train loss: 2.461854934692383\n",
            "train loss: 2.215141773223877\n",
            "train loss: 2.621988296508789\n",
            "train loss: 2.3438940048217773\n",
            "train loss: 2.641033411026001\n",
            "train loss: 2.2946178913116455\n",
            "train loss: 2.661252975463867\n",
            "train loss: 2.039531707763672\n",
            "train loss: 2.395637273788452\n",
            "train loss: 2.814011812210083\n",
            "train loss: 2.6591386795043945\n",
            "train loss: 2.5817174911499023\n",
            "train loss: 2.2803726196289062\n",
            "train loss: 2.465118408203125\n",
            "train loss: 2.2344186305999756\n",
            "train loss: 2.118523597717285\n",
            "train loss: 2.6487910747528076\n",
            "train loss: 2.111652135848999\n",
            "train loss: 2.2387168407440186\n",
            "train loss: 2.624971389770508\n",
            "train loss: 2.628782272338867\n",
            "train loss: 2.250162124633789\n",
            "train loss: 2.5839035511016846\n",
            "train loss: 2.072922468185425\n",
            "train loss: 2.6440865993499756\n",
            "train loss: 2.4942338466644287\n",
            "train loss: 2.928410530090332\n",
            "train loss: 2.134706974029541\n",
            "train loss: 2.2640302181243896\n",
            "train loss: 2.618377685546875\n",
            "train loss: 2.8471689224243164\n",
            "train loss: 2.260439395904541\n",
            "train loss: 2.4481890201568604\n",
            "train loss: 2.3515145778656006\n",
            "train loss: 2.3130908012390137\n",
            "train loss: 2.253192186355591\n",
            "train loss: 2.378225803375244\n",
            "train loss: 2.243732213973999\n",
            "train loss: 2.7678921222686768\n",
            "train loss: 2.570159673690796\n",
            "train loss: 2.4154133796691895\n",
            "train loss: 2.832549810409546\n",
            "train loss: 2.691952705383301\n",
            "train loss: 2.5826892852783203\n",
            "train loss: 2.0851452350616455\n",
            "train loss: 2.3570821285247803\n",
            "train loss: 2.4327616691589355\n",
            "train loss: 2.349069356918335\n",
            "train loss: 1.9808653593063354\n",
            "train loss: 2.4393362998962402\n",
            "train loss: 2.5032854080200195\n",
            "train loss: 2.359105110168457\n",
            "train loss: 2.4189045429229736\n",
            "train loss: 2.284942150115967\n",
            "train loss: 1.3447437286376953\n",
            "train loss: 2.1815218925476074\n",
            "train loss: 2.3708484172821045\n",
            "train loss: 2.4200713634490967\n",
            "train loss: 2.258840322494507\n",
            "train loss: 2.5278217792510986\n",
            "train loss: 2.1671483516693115\n",
            "train loss: 2.4486215114593506\n",
            "train loss: 2.90092134475708\n",
            "train loss: 2.1636366844177246\n",
            "train loss: 2.3052353858947754\n",
            "train loss: 2.65112566947937\n",
            "train loss: 2.1411259174346924\n",
            "train loss: 2.5208444595336914\n",
            "train loss: 1.947532296180725\n",
            "train loss: 1.588865876197815\n",
            "train loss: 2.3742284774780273\n",
            "train loss: 2.112689733505249\n",
            "train loss: 1.7116546630859375\n",
            "train loss: 2.198127031326294\n",
            "train loss: 1.9871464967727661\n",
            "train loss: 1.9754966497421265\n",
            "train loss: 2.6322691440582275\n",
            "train loss: 2.2487008571624756\n",
            "train loss: 2.1025853157043457\n",
            "train loss: 2.322955846786499\n",
            "train loss: 2.514523506164551\n",
            "train loss: 2.404949426651001\n",
            "train loss: 2.5264077186584473\n",
            "train loss: 2.143002510070801\n",
            "train loss: 2.1772913932800293\n",
            "train loss: 2.2707436084747314\n",
            "train loss: 2.5345253944396973\n",
            "train loss: 2.0001583099365234\n",
            "train loss: 1.8731131553649902\n",
            "train loss: 2.137516736984253\n",
            "train loss: 1.8345260620117188\n",
            "train loss: 2.0844314098358154\n",
            "train loss: 2.4000234603881836\n",
            "train loss: 2.421041488647461\n",
            "train loss: 1.3972384929656982\n",
            "train loss: 2.3752408027648926\n",
            "train loss: 3.0745620727539062\n",
            "train loss: 2.782533645629883\n",
            "train loss: 2.1455862522125244\n",
            "train loss: 1.7696385383605957\n",
            "train loss: 2.151040554046631\n",
            "train loss: 2.0446228981018066\n",
            "train loss: 2.0891270637512207\n",
            "train loss: 1.9870518445968628\n",
            "train loss: 1.846328854560852\n",
            "train loss: 2.1858420372009277\n",
            "train loss: 1.5629148483276367\n",
            "train loss: 1.6788387298583984\n",
            "train loss: 2.2979776859283447\n",
            "train loss: 2.0546720027923584\n",
            "train loss: 2.465579032897949\n",
            "train loss: 2.2830545902252197\n",
            "train loss: 1.746077299118042\n",
            "train loss: 2.3067750930786133\n",
            "train loss: 1.5020580291748047\n",
            "train loss: 2.123420000076294\n",
            "train loss: 1.7408932447433472\n",
            "train loss: 2.2186009883880615\n",
            "train loss: 1.9565131664276123\n",
            "train loss: 2.2776596546173096\n",
            "train loss: 2.219053268432617\n",
            "train loss: 2.014523506164551\n",
            "train loss: 1.2556816339492798\n",
            "train loss: 1.7946559190750122\n",
            "train loss: 1.9120750427246094\n",
            "train loss: 2.4510557651519775\n",
            "train loss: 2.6061887741088867\n",
            "train loss: 1.3993661403656006\n",
            "train loss: 1.8075848817825317\n",
            "train loss: 2.3592512607574463\n",
            "train loss: 2.0333986282348633\n",
            "train loss: 1.9779828786849976\n",
            "train loss: 2.3499977588653564\n",
            "train loss: 1.735782265663147\n",
            "train loss: 2.3454582691192627\n",
            "train loss: 2.5497097969055176\n",
            "train loss: 1.9084820747375488\n",
            "train loss: 1.6310733556747437\n",
            "train loss: 1.475518822669983\n",
            "train loss: 1.64258873462677\n",
            "train loss: 1.4647307395935059\n",
            "train loss: 2.1748108863830566\n",
            "train loss: 1.8589473962783813\n",
            "train loss: 1.854448676109314\n",
            "train loss: 2.3609800338745117\n",
            "train loss: 1.8137733936309814\n",
            "train loss: 2.1035430431365967\n",
            "train loss: 2.387578248977661\n",
            "train loss: 1.6996821165084839\n",
            "train loss: 1.7201710939407349\n",
            "train loss: 2.2223706245422363\n",
            "train loss: 2.1610963344573975\n",
            "train loss: 2.318572521209717\n",
            "train loss: 1.8557544946670532\n",
            "train loss: 1.9328649044036865\n",
            "train loss: 1.5422992706298828\n",
            "train loss: 0.9344996213912964\n",
            "train loss: 1.6215988397598267\n",
            "train loss: 1.958617091178894\n",
            "train loss: 1.5309566259384155\n",
            "train loss: 1.5699074268341064\n",
            "train loss: 1.65790593624115\n",
            "train loss: 1.7916271686553955\n",
            "train loss: 1.4739805459976196\n",
            "train loss: 1.6520230770111084\n",
            "train loss: 1.448639988899231\n",
            "train loss: 2.2325549125671387\n",
            "train loss: 2.1517560482025146\n",
            "train loss: 2.0546677112579346\n",
            "train loss: 1.346463680267334\n",
            "train loss: 1.422970175743103\n",
            "train loss: 1.8707880973815918\n",
            "train loss: 1.9875764846801758\n",
            "train loss: 2.4466404914855957\n",
            "train loss: 1.9690635204315186\n",
            "train loss: 1.8973194360733032\n",
            "train loss: 2.204472780227661\n",
            "train loss: 1.5860105752944946\n",
            "train loss: 2.24643611907959\n",
            "train loss: 1.9126908779144287\n",
            "train loss: 1.0698045492172241\n",
            "train loss: 2.013606548309326\n",
            "train loss: 2.439573287963867\n",
            "train loss: 2.333298683166504\n",
            "train loss: 1.1046807765960693\n",
            "train loss: 1.1417444944381714\n",
            "train loss: 1.5674022436141968\n",
            "train loss: 2.6125693321228027\n",
            "train loss: 1.4207143783569336\n",
            "train loss: 1.427234172821045\n",
            "train loss: 1.8923618793487549\n",
            "train loss: 1.7755259275436401\n",
            "train loss: 1.560799241065979\n",
            "train loss: 0.9699230194091797\n",
            "train loss: 1.635391116142273\n",
            "train loss: 2.316169023513794\n",
            "train loss: 1.9739279747009277\n",
            "train loss: 1.880863904953003\n",
            "train loss: 1.6192491054534912\n",
            "train loss: 2.2385928630828857\n",
            "train loss: 1.8205606937408447\n",
            "train loss: 2.105076551437378\n",
            "train loss: 1.4696298837661743\n",
            "train loss: 1.5055410861968994\n",
            "train loss: 2.065167188644409\n",
            "train loss: 1.9960143566131592\n",
            "train loss: 2.013455867767334\n",
            "train loss: 1.6690855026245117\n",
            "train loss: 1.919615387916565\n",
            "train loss: 1.8501313924789429\n",
            "train loss: 1.377199411392212\n",
            "train loss: 1.4196492433547974\n",
            "train loss: 1.3690040111541748\n",
            "train loss: 1.8671025037765503\n",
            "train loss: 1.471797227859497\n",
            "train loss: 1.0925663709640503\n",
            "train loss: 1.4103397130966187\n",
            "train loss: 1.3635214567184448\n",
            "train loss: 1.8214213848114014\n",
            "train loss: 1.6696414947509766\n",
            "train loss: 0.9911751747131348\n",
            "train loss: 1.527708649635315\n",
            "train loss: 1.307955026626587\n",
            "train loss: 1.2272297143936157\n",
            "train loss: 1.876407504081726\n",
            "train loss: 2.4876654148101807\n",
            "train loss: 1.9311057329177856\n",
            "train loss: 1.3313157558441162\n",
            "train loss: 1.3115664720535278\n",
            "train loss: 1.6430281400680542\n",
            "train loss: 1.1384116411209106\n",
            "train loss: 1.1449016332626343\n",
            "train loss: 1.5636303424835205\n",
            "train loss: 1.5976847410202026\n",
            "train loss: 1.0368822813034058\n",
            "train loss: 1.524324893951416\n",
            "train loss: 1.9252243041992188\n",
            "train loss: 1.2919703722000122\n",
            "train loss: 1.4501187801361084\n",
            "train loss: 1.3752433061599731\n",
            "train loss: 1.6052367687225342\n",
            "train loss: 1.7456222772598267\n",
            "train loss: 1.7641416788101196\n",
            "train loss: 1.6017205715179443\n",
            "train loss: 0.8068515062332153\n",
            "train loss: 1.2000577449798584\n",
            "train loss: 1.1667159795761108\n",
            "train loss: 2.0185248851776123\n",
            "train loss: 1.731792688369751\n",
            "train loss: 1.576646327972412\n",
            "train loss: 2.239271640777588\n",
            "train loss: 1.1402727365493774\n",
            "train loss: 1.4208813905715942\n",
            "train loss: 1.4891314506530762\n",
            "train loss: 2.340630054473877\n",
            "train loss: 0.9297248721122742\n",
            "train loss: 1.357843279838562\n",
            "train loss: 1.3050161600112915\n",
            "train loss: 2.044503927230835\n",
            "train loss: 0.9805837869644165\n",
            "train loss: 1.0217560529708862\n",
            "train loss: 1.7569069862365723\n",
            "train loss: 0.9506505727767944\n",
            "train loss: 1.2988898754119873\n",
            "train loss: 1.9514509439468384\n",
            "train loss: 1.030339241027832\n",
            "train loss: 1.4701945781707764\n",
            "train loss: 1.8441548347473145\n",
            "train loss: 1.3968095779418945\n",
            "train loss: 0.910831868648529\n",
            "train loss: 1.1081275939941406\n",
            "train loss: 1.043771743774414\n",
            "train loss: 1.2374345064163208\n",
            "train loss: 0.9858859181404114\n",
            "train loss: 1.0124483108520508\n",
            "train loss: 1.6336191892623901\n",
            "train loss: 1.9007818698883057\n",
            "train loss: 1.6090185642242432\n",
            "train loss: 1.0847541093826294\n",
            "train loss: 1.6413962841033936\n",
            "train loss: 1.9486382007598877\n",
            "train loss: 1.1905800104141235\n",
            "train loss: 1.9764176607131958\n",
            "train loss: 1.7757275104522705\n",
            "train loss: 0.9579381346702576\n",
            "train loss: 1.688056468963623\n",
            "train loss: 0.8009616136550903\n",
            "train loss: 1.007964849472046\n",
            "train loss: 1.0047392845153809\n",
            "train loss: 1.7200316190719604\n",
            "train loss: 1.8590919971466064\n",
            "train loss: 1.8667691946029663\n",
            "train loss: 1.8609237670898438\n",
            "train loss: 1.8156594038009644\n",
            "train loss: 0.9166821241378784\n",
            "train loss: 2.2106990814208984\n",
            "train loss: 1.8076720237731934\n",
            "train loss: 1.0362428426742554\n",
            "train loss: 1.2975512742996216\n",
            "train loss: 0.796683132648468\n",
            "train loss: 1.2139182090759277\n",
            "train loss: 1.0844088792800903\n",
            "train loss: 1.9675263166427612\n",
            "train loss: 1.3630017042160034\n",
            "train loss: 1.7400814294815063\n",
            "train loss: 0.6693776249885559\n",
            "train loss: 1.6465020179748535\n",
            "train loss: 1.6506894826889038\n",
            "train loss: 1.1614171266555786\n",
            "train loss: 0.7946380376815796\n",
            "train loss: 0.9398755431175232\n",
            "train loss: 1.6092262268066406\n",
            "train loss: 1.353450894355774\n",
            "train loss: 0.621590793132782\n",
            "train loss: 1.2605234384536743\n",
            "train loss: 1.3031413555145264\n",
            "train loss: 1.4310431480407715\n",
            "train loss: 1.3203779458999634\n",
            "train loss: 0.9790468811988831\n",
            "train loss: 0.8327751159667969\n",
            "train loss: 0.9936484098434448\n",
            "train loss: 1.9627196788787842\n",
            "train loss: 1.433241844177246\n",
            "train loss: 1.5034244060516357\n",
            "train loss: 1.0221046209335327\n",
            "train loss: 0.7468743324279785\n",
            "train loss: 1.2902963161468506\n",
            "train loss: 0.5903227925300598\n",
            "train loss: 1.2208844423294067\n",
            "train loss: 2.0163538455963135\n",
            "train loss: 2.068434238433838\n",
            "train loss: 1.5182381868362427\n",
            "train loss: 0.9752562046051025\n",
            "train loss: 1.0002663135528564\n",
            "train loss: 0.9593679308891296\n",
            "train loss: 1.8392009735107422\n",
            "train loss: 1.2910337448120117\n",
            "train loss: 0.9702193140983582\n",
            "train loss: 1.9963359832763672\n",
            "train loss: 1.488232135772705\n",
            "train loss: 2.2495663166046143\n",
            "train loss: 0.9756079316139221\n",
            "train loss: 2.003045082092285\n",
            "train loss: 1.1761237382888794\n",
            "train loss: 1.1632051467895508\n",
            "train loss: 1.556503176689148\n",
            "train loss: 1.3050103187561035\n",
            "train loss: 1.0585393905639648\n",
            "train loss: 0.95325767993927\n",
            "train loss: 1.4109230041503906\n",
            "train loss: 0.9163982272148132\n",
            "train loss: 1.0864511728286743\n",
            "train loss: 0.9413560628890991\n",
            "train loss: 1.768531084060669\n",
            "train loss: 1.2561862468719482\n",
            "train loss: 1.9011693000793457\n",
            "train loss: 0.6117287278175354\n",
            "train loss: 0.530965268611908\n",
            "train loss: 1.375474452972412\n",
            "train loss: 1.6711982488632202\n",
            "train loss: 0.7586092352867126\n",
            "train loss: 0.9064924120903015\n",
            "train loss: 1.7068148851394653\n",
            "train loss: 1.9094902276992798\n",
            "train loss: 1.5918934345245361\n",
            "train loss: 1.8221428394317627\n",
            "train loss: 1.1314760446548462\n",
            "train loss: 1.0439411401748657\n",
            "train loss: 1.3944426774978638\n",
            "train loss: 1.4873279333114624\n",
            "train loss: 1.1600500345230103\n",
            "train loss: 0.3919014632701874\n",
            "train loss: 0.9926356077194214\n",
            "train loss: 1.2628929615020752\n",
            "train loss: 1.4791057109832764\n",
            "train loss: 0.974327564239502\n",
            "train loss: 1.8830163478851318\n",
            "train loss: 0.8548495769500732\n",
            "train loss: 1.2832330465316772\n",
            "train loss: 1.9930256605148315\n",
            "train loss: 0.5336737036705017\n",
            "train loss: 1.3011168241500854\n",
            "train loss: 0.3221217095851898\n",
            "train loss: 1.4993326663970947\n",
            "train loss: 0.9266757369041443\n",
            "train loss: 0.4047519862651825\n",
            "train loss: 1.2255139350891113\n",
            "train loss: 1.585068702697754\n",
            "train loss: 1.4984009265899658\n",
            "train loss: 1.2709527015686035\n",
            "train loss: 1.015854001045227\n",
            "train loss: 0.9505060911178589\n",
            "train loss: 1.1525074243545532\n",
            "train loss: 1.03070867061615\n",
            "train loss: 2.0194942951202393\n",
            "train loss: 1.4061739444732666\n",
            "train loss: 0.8752701878547668\n",
            "train loss: 0.7829256653785706\n",
            "train loss: 1.0339701175689697\n",
            "train loss: 0.8084815144538879\n",
            "train loss: 0.8563703894615173\n",
            "train loss: 1.3905729055404663\n",
            "train loss: 1.3723647594451904\n",
            "train loss: 1.11918044090271\n",
            "train loss: 2.026142120361328\n",
            "train loss: 1.1476595401763916\n",
            "train loss: 1.511727213859558\n",
            "train loss: 0.6584326028823853\n",
            "train loss: 1.3704956769943237\n",
            "train loss: 1.1679131984710693\n",
            "train loss: 1.0349622964859009\n",
            "train loss: 1.5701735019683838\n",
            "train loss: 1.4615799188613892\n",
            "train loss: 1.4417388439178467\n",
            "train loss: 1.2947988510131836\n",
            "train loss: 0.4584461748600006\n",
            "train loss: 1.7820541858673096\n",
            "train loss: 0.8058213591575623\n",
            "train loss: 2.0155441761016846\n",
            "train loss: 0.7629586458206177\n",
            "train loss: 0.9430654644966125\n",
            "train loss: 1.1921093463897705\n",
            "train loss: 0.8883016109466553\n",
            "train loss: 1.4878700971603394\n",
            "train loss: 1.231941819190979\n",
            "train loss: 0.9569991230964661\n",
            "train loss: 1.8468804359436035\n",
            "train loss: 1.3953503370285034\n",
            "train loss: 1.489363670349121\n",
            "train loss: 1.471384048461914\n",
            "train loss: 1.3708651065826416\n",
            "train loss: 1.3317523002624512\n",
            "train loss: 1.2874542474746704\n",
            "train loss: 1.1663063764572144\n",
            "train loss: 0.5951882004737854\n",
            "train loss: 1.2917261123657227\n",
            "train loss: 1.5801234245300293\n",
            "train loss: 0.6171835660934448\n",
            "train loss: 1.4144008159637451\n",
            "train loss: 0.39763203263282776\n",
            "train loss: 0.6004315614700317\n",
            "train loss: 1.4550511837005615\n",
            "train loss: 0.42283183336257935\n",
            "train loss: 2.08197021484375\n",
            "train loss: 0.832473874092102\n",
            "train loss: 1.1952710151672363\n",
            "train loss: 1.2371917963027954\n",
            "train loss: 0.5961576104164124\n",
            "train loss: 2.0336837768554688\n",
            "train loss: 1.005778193473816\n",
            "train loss: 1.7350636720657349\n",
            "train loss: 0.8262118697166443\n",
            "train loss: 1.555927038192749\n",
            "train loss: 1.1945747137069702\n",
            "train loss: 1.4203219413757324\n",
            "train loss: 1.1380434036254883\n",
            "train loss: 0.7219345569610596\n",
            "train loss: 0.527289628982544\n",
            "train loss: 1.1161497831344604\n",
            "train loss: 0.7263518571853638\n",
            "train loss: 1.2451881170272827\n",
            "train loss: 1.0312598943710327\n",
            "train loss: 0.7091662883758545\n",
            "train loss: 0.8560856580734253\n",
            "train loss: 1.1067880392074585\n",
            "train loss: 0.9673896431922913\n",
            "train loss: 0.7319859266281128\n",
            "train loss: 1.3156872987747192\n",
            "train loss: 0.6886319518089294\n",
            "train loss: 1.0476140975952148\n",
            "train loss: 0.7058128714561462\n",
            "train loss: 1.6765894889831543\n",
            "train loss: 1.8708728551864624\n",
            "train loss: 1.223608374595642\n",
            "train loss: 1.4289140701293945\n",
            "train loss: 1.3686431646347046\n",
            "train loss: 1.3369836807250977\n",
            "train loss: 1.4739855527877808\n",
            "train loss: 1.072772741317749\n",
            "train loss: 0.8691081404685974\n",
            "train loss: 1.116749882698059\n",
            "train loss: 1.2346440553665161\n",
            "train loss: 1.0724843740463257\n",
            "train loss: 1.1790361404418945\n",
            "train loss: 0.5848186612129211\n",
            "train loss: 0.38702136278152466\n",
            "train loss: 1.2769049406051636\n",
            "train loss: 0.432584673166275\n",
            "train loss: 1.7911626100540161\n",
            "train loss: 0.6811802387237549\n",
            "train loss: 1.78938889503479\n",
            "train loss: 0.7861177325248718\n",
            "train loss: 0.8348854184150696\n",
            "train loss: 0.8833824992179871\n",
            "train loss: 0.30115824937820435\n",
            "train loss: 0.4818265736103058\n",
            "train loss: 0.8048572540283203\n",
            "train loss: 1.8032796382904053\n",
            "train loss: 1.0247851610183716\n",
            "train loss: 1.2013992071151733\n",
            "train loss: 1.6786916255950928\n",
            "train loss: 0.44149690866470337\n",
            "train loss: 0.8079043030738831\n",
            "train loss: 0.7653038501739502\n",
            "train loss: 1.3855468034744263\n",
            "train loss: 0.8165092468261719\n",
            "train loss: 1.0929404497146606\n",
            "train loss: 0.932197630405426\n",
            "train loss: 2.1816890239715576\n",
            "train loss: 1.1546188592910767\n",
            "train loss: 1.3790034055709839\n",
            "train loss: 1.091493010520935\n",
            "train loss: 1.6081546545028687\n",
            "train loss: 0.6262816190719604\n",
            "train loss: 1.4357585906982422\n",
            "train loss: 0.5949644446372986\n",
            "train loss: 1.5323708057403564\n",
            "train loss: 0.7726961374282837\n",
            "train loss: 1.0948801040649414\n",
            "train loss: 0.39170041680336\n",
            "train loss: 0.5935652852058411\n",
            "train loss: 1.8828352689743042\n",
            "train loss: 0.9731096029281616\n",
            "train loss: 1.5500835180282593\n",
            "train loss: 1.0580253601074219\n",
            "train loss: 1.6484624147415161\n",
            "train loss: 0.862712025642395\n",
            "train loss: 1.397978663444519\n",
            "train loss: 1.079459547996521\n",
            "train loss: 1.273805022239685\n",
            "train loss: 0.8631671071052551\n",
            "train loss: 1.7844228744506836\n",
            "train loss: 0.7965755462646484\n",
            "train loss: 1.6688106060028076\n",
            "train loss: 0.9327267408370972\n",
            "train loss: 1.3967113494873047\n",
            "train loss: 0.7462082505226135\n",
            "train loss: 1.1984285116195679\n",
            "train loss: 1.9217581748962402\n",
            "train loss: 1.260211706161499\n",
            "train loss: 0.5009138584136963\n",
            "train loss: 0.6491385102272034\n",
            "train loss: 1.0175528526306152\n",
            "train loss: 0.47442224621772766\n",
            "train loss: 1.1388659477233887\n",
            "train loss: 0.7182665467262268\n",
            "train loss: 0.3560207784175873\n",
            "train loss: 0.3600338101387024\n",
            "train loss: 1.172756552696228\n",
            "train loss: 0.6265742778778076\n",
            "train loss: 1.0062587261199951\n",
            "train loss: 0.18981601297855377\n",
            "train loss: 0.6994065642356873\n",
            "train loss: 0.47040849924087524\n",
            "train loss: 0.7083877921104431\n",
            "train loss: 1.0110899209976196\n",
            "train loss: 0.9507008790969849\n",
            "train loss: 0.5617512464523315\n",
            "train loss: 0.49330204725265503\n",
            "train loss: 0.7393482327461243\n",
            "train loss: 1.306911587715149\n",
            "train loss: 0.47314104437828064\n",
            "train loss: 0.5892793536186218\n",
            "train loss: 0.856926679611206\n",
            "train loss: 0.3589421808719635\n",
            "train loss: 0.8412861227989197\n",
            "train loss: 0.6682941913604736\n",
            "train loss: 0.2882833182811737\n",
            "train loss: 1.0758675336837769\n",
            "train loss: 0.6803321838378906\n",
            "train loss: 1.272270917892456\n",
            "train loss: 0.5233733057975769\n",
            "train loss: 1.1998130083084106\n",
            "train loss: 0.7865353226661682\n",
            "train loss: 1.0178760290145874\n",
            "train loss: 1.1114022731781006\n",
            "train loss: 0.6772853136062622\n",
            "train loss: 0.8495369553565979\n",
            "train loss: 1.862077236175537\n",
            "train loss: 1.1167547702789307\n",
            "train loss: 1.1210381984710693\n",
            "train loss: 1.0434752702713013\n",
            "train loss: 1.3326172828674316\n",
            "train loss: 1.387025237083435\n",
            "train loss: 0.31412994861602783\n",
            "train loss: 0.5849992036819458\n",
            "train loss: 1.3139313459396362\n",
            "train loss: 0.9959633350372314\n",
            "train loss: 0.8959497809410095\n",
            "train loss: 1.0836832523345947\n",
            "train loss: 0.8306150436401367\n",
            "train loss: 0.3020327389240265\n",
            "train loss: 1.8125746250152588\n",
            "train loss: 1.7452588081359863\n",
            "train loss: 0.8361508250236511\n",
            "train loss: 1.2846400737762451\n",
            "train loss: 1.6205999851226807\n",
            "train loss: 1.7381703853607178\n",
            "train loss: 0.7729339599609375\n",
            "train loss: 1.0480610132217407\n",
            "train loss: 0.6975626945495605\n",
            "train loss: 0.7448186278343201\n",
            "train loss: 1.2040449380874634\n",
            "train loss: 1.4694746732711792\n",
            "train loss: 0.5540896058082581\n",
            "train loss: 0.7086035013198853\n",
            "train loss: 0.6341947913169861\n",
            "train loss: 0.6345505118370056\n",
            "train loss: 0.4138888716697693\n",
            "train loss: 0.860305905342102\n",
            "train loss: 0.5711320638656616\n",
            "train loss: 0.9671400785446167\n",
            "train loss: 0.645207941532135\n",
            "train loss: 0.5477524995803833\n",
            "train loss: 0.7914050221443176\n",
            "train loss: 1.158648133277893\n",
            "train loss: 0.9175248742103577\n",
            "train loss: 0.30196890234947205\n",
            "train loss: 1.2906603813171387\n",
            "train loss: 1.171221375465393\n",
            "train loss: 1.0922696590423584\n",
            "train loss: 0.25201690196990967\n",
            "train loss: 0.5013999342918396\n",
            "train loss: 0.38800275325775146\n",
            "train loss: 0.7312045693397522\n",
            "train loss: 1.7154051065444946\n",
            "train loss: 0.8356077671051025\n",
            "train loss: 0.7220966815948486\n",
            "train loss: 0.8732151389122009\n",
            "train loss: 0.44762372970581055\n",
            "train loss: 1.3859171867370605\n",
            "train loss: 0.8601235151290894\n",
            "train loss: 0.6531494855880737\n",
            "train loss: 0.5353580713272095\n",
            "train loss: 1.6460247039794922\n",
            "train loss: 0.662540078163147\n",
            "train loss: 1.378056526184082\n",
            "train loss: 0.8813084363937378\n",
            "train loss: 0.4358924329280853\n",
            "train loss: 0.9966062307357788\n",
            "train loss: 1.5261706113815308\n",
            "train loss: 1.6532446146011353\n",
            "train loss: 0.42443716526031494\n",
            "train loss: 1.0620356798171997\n",
            "train loss: 0.7864214777946472\n",
            "train loss: 0.7048673629760742\n",
            "train loss: 1.6461193561553955\n",
            "train loss: 1.4338717460632324\n",
            "train loss: 0.2281009405851364\n",
            "train loss: 0.28506654500961304\n",
            "train loss: 0.7128114104270935\n",
            "train loss: 0.9243577122688293\n",
            "train loss: 0.6427434086799622\n",
            "train loss: 0.7201700210571289\n",
            "train loss: 1.475743055343628\n",
            "train loss: 0.7578858137130737\n",
            "train loss: 0.4114738404750824\n",
            "train loss: 0.6324659585952759\n",
            "train loss: 0.7685163617134094\n",
            "train loss: 0.8407196998596191\n",
            "train loss: 1.022271752357483\n",
            "train loss: 0.5544080138206482\n",
            "train loss: 0.327464759349823\n",
            "train loss: 1.3667622804641724\n",
            "train loss: 0.949241578578949\n",
            "train loss: 0.996147632598877\n",
            "train loss: 0.6845034956932068\n",
            "train loss: 0.31145691871643066\n",
            "train loss: 0.8049769997596741\n",
            "train loss: 0.9484085440635681\n",
            "train loss: 1.1108391284942627\n",
            "train loss: 0.6378564834594727\n",
            "train loss: 0.781376838684082\n",
            "train loss: 1.025610089302063\n",
            "train loss: 1.6805392503738403\n",
            "train loss: 0.9270370602607727\n",
            "train loss: 1.4446666240692139\n",
            "train loss: 0.8561472296714783\n",
            "train loss: 1.6222929954528809\n",
            "train loss: 0.16556216776371002\n",
            "train loss: 0.7561026215553284\n",
            "train loss: 0.6833494901657104\n",
            "train loss: 1.28444242477417\n",
            "train loss: 1.3360265493392944\n",
            "train loss: 0.6251051425933838\n",
            "train loss: 1.0321887731552124\n",
            "train loss: 1.265911340713501\n",
            "train loss: 0.4877544641494751\n",
            "train loss: 1.044571876525879\n",
            "train loss: 0.5735793709754944\n",
            "train loss: 0.8117040395736694\n",
            "train loss: 1.1965515613555908\n",
            "train loss: 1.0063321590423584\n",
            "train loss: 0.6699318289756775\n",
            "train loss: 0.5647466778755188\n",
            "train loss: 0.6938651204109192\n",
            "train loss: 1.0501010417938232\n",
            "train loss: 0.4345240294933319\n",
            "train loss: 1.12602961063385\n",
            "train loss: 0.54219651222229\n",
            "train loss: 1.2033886909484863\n",
            "train loss: 0.3814706802368164\n",
            "train loss: 0.37747207283973694\n",
            "train loss: 0.831956684589386\n",
            "train loss: 0.5395850539207458\n",
            "train loss: 0.35606250166893005\n",
            "train loss: 0.9522546529769897\n",
            "train loss: 0.3624785542488098\n",
            "train loss: 0.4763045310974121\n",
            "train loss: 0.8641989827156067\n",
            "train loss: 1.506391167640686\n",
            "train loss: 0.7952779531478882\n",
            "train loss: 1.006455421447754\n",
            "train loss: 0.5400571823120117\n",
            "train loss: 1.1571484804153442\n",
            "train loss: 0.4899510145187378\n",
            "train loss: 1.9687718152999878\n",
            "train loss: 0.9294150471687317\n",
            "train loss: 0.7062273025512695\n",
            "train loss: 0.92780601978302\n",
            "train loss: 0.40936076641082764\n",
            "train loss: 1.2259262800216675\n",
            "train loss: 0.2284962236881256\n",
            "train loss: 0.3620210289955139\n",
            "train loss: 0.27872541546821594\n",
            "train loss: 0.7133228182792664\n",
            "train loss: 0.5636152029037476\n",
            "train loss: 0.9348864555358887\n",
            "train loss: 1.0398215055465698\n",
            "train loss: 0.5854583978652954\n",
            "train loss: 1.4954140186309814\n",
            "train loss: 0.7419548034667969\n",
            "train loss: 0.9656803011894226\n",
            "train loss: 0.5864488482475281\n",
            "train loss: 0.6793301105499268\n",
            "train loss: 0.7288053631782532\n",
            "train loss: 0.33400285243988037\n",
            "train loss: 1.1206433773040771\n",
            "train loss: 1.1945338249206543\n",
            "train loss: 0.35993748903274536\n",
            "train loss: 0.8586586713790894\n",
            "train loss: 0.5337740182876587\n",
            "train loss: 0.33084264397621155\n",
            "train loss: 0.6833603978157043\n",
            "train loss: 1.1023002862930298\n",
            "train loss: 0.9860051870346069\n",
            "train loss: 0.6536251306533813\n",
            "train loss: 0.3612215518951416\n",
            "train loss: 1.6199827194213867\n",
            "train loss: 1.3834084272384644\n",
            "train loss: 0.71299147605896\n",
            "train loss: 0.8109738230705261\n",
            "train loss: 0.6366555690765381\n",
            "train loss: 0.495510071516037\n",
            "train loss: 0.5140102505683899\n",
            "train loss: 0.5684396624565125\n",
            "train loss: 1.0868057012557983\n",
            "train loss: 0.7957568168640137\n",
            "train loss: 1.2324448823928833\n",
            "train loss: 0.892933189868927\n",
            "train loss: 0.8796099424362183\n",
            "train loss: 1.1362110376358032\n",
            "train loss: 1.2791926860809326\n",
            "train loss: 0.8432126641273499\n",
            "train loss: 0.38895198702812195\n",
            "train loss: 0.2977440059185028\n",
            "train loss: 0.731140673160553\n",
            "train loss: 1.0793864727020264\n",
            "train loss: 0.9493089318275452\n",
            "train loss: 0.2727530002593994\n",
            "train loss: 0.49074429273605347\n",
            "train loss: 0.3797879219055176\n",
            "train loss: 0.507391631603241\n",
            "train loss: 1.3886946439743042\n",
            "train loss: 0.49676579236984253\n",
            "train loss: 0.63957679271698\n",
            "train loss: 0.2973233461380005\n",
            "train loss: 1.6730412244796753\n",
            "train loss: 1.5862836837768555\n",
            "train loss: 0.44212639331817627\n",
            "train loss: 1.1440037488937378\n",
            "train loss: 1.4181278944015503\n",
            "train loss: 0.6161168217658997\n",
            "train loss: 1.2770864963531494\n",
            "train loss: 0.8284534215927124\n",
            "train loss: 0.32874029874801636\n",
            "train loss: 1.5462032556533813\n",
            "train loss: 1.9705321788787842\n",
            "train loss: 0.7851250171661377\n",
            "train loss: 0.4334476888179779\n",
            "train loss: 0.5053489208221436\n",
            "train loss: 0.38356587290763855\n",
            "train loss: 0.8309512138366699\n",
            "train loss: 0.28845447301864624\n",
            "train loss: 0.3988283574581146\n",
            "train loss: 0.9787628054618835\n",
            "train loss: 1.380647897720337\n",
            "train loss: 0.5040242075920105\n",
            "train loss: 0.5058526396751404\n",
            "train loss: 0.6385879516601562\n",
            "train loss: 0.5819043517112732\n",
            "train loss: 0.8356969356536865\n",
            "train loss: 1.089153528213501\n",
            "train loss: 0.9712044596672058\n",
            "train loss: 1.6789696216583252\n",
            "train loss: 0.352459579706192\n",
            "train loss: 0.3007812201976776\n",
            "train loss: 1.3179699182510376\n",
            "train loss: 0.5186986327171326\n",
            "train loss: 1.7083897590637207\n",
            "train loss: 0.8549612164497375\n",
            "train loss: 1.5877372026443481\n",
            "train loss: 1.291491150856018\n",
            "train loss: 0.8459060192108154\n",
            "train loss: 0.8195688128471375\n",
            "train loss: 0.4028124213218689\n",
            "train loss: 0.9536335468292236\n",
            "train loss: 0.275961697101593\n",
            "train loss: 0.4688694477081299\n",
            "train loss: 0.36372697353363037\n",
            "train loss: 1.1426604986190796\n",
            "train loss: 0.28147485852241516\n",
            "train loss: 0.29451173543930054\n",
            "train loss: 1.4236831665039062\n",
            "train loss: 0.5829516649246216\n",
            "train loss: 0.20092543959617615\n",
            "train loss: 0.6095102429389954\n",
            "train loss: 1.0390217304229736\n",
            "train loss: 0.8828449845314026\n",
            "train loss: 0.661505401134491\n",
            "train loss: 0.3140556812286377\n",
            "train loss: 0.5385906100273132\n",
            "train loss: 0.7495678067207336\n",
            "train loss: 0.23580972850322723\n",
            "train loss: 0.6610986590385437\n",
            "train loss: 0.7199006080627441\n",
            "train loss: 0.5768852233886719\n",
            "train loss: 0.9087468981742859\n",
            "train loss: 0.7532568573951721\n",
            "train loss: 0.22295041382312775\n",
            "train loss: 0.3639167845249176\n",
            "train loss: 0.1992465704679489\n",
            "train loss: 0.2480423003435135\n",
            "train loss: 0.38494205474853516\n",
            "train loss: 1.1193515062332153\n",
            "train loss: 0.37795236706733704\n",
            "train loss: 1.5604043006896973\n",
            "train loss: 0.26811668276786804\n",
            "train loss: 0.5772019028663635\n",
            "train loss: 1.2654958963394165\n",
            "train loss: 0.970890462398529\n",
            "train loss: 0.6451759338378906\n",
            "train loss: 0.2384190410375595\n",
            "train loss: 1.508982539176941\n",
            "train loss: 0.9316942095756531\n",
            "train loss: 1.2807719707489014\n",
            "train loss: 0.49442628026008606\n",
            "train loss: 0.18023861944675446\n",
            "train loss: 1.8837834596633911\n",
            "train loss: 0.6665815114974976\n",
            "train loss: 1.7657254934310913\n",
            "train loss: 0.5796206593513489\n",
            "train loss: 0.5348368883132935\n",
            "train loss: 0.649200439453125\n",
            "train loss: 0.7666407227516174\n",
            "train loss: 0.3033738136291504\n",
            "train loss: 1.0186607837677002\n",
            "train loss: 0.20590640604496002\n",
            "train loss: 0.14418771862983704\n",
            "train loss: 0.1427423357963562\n",
            "train loss: 0.6712296009063721\n",
            "train loss: 0.8939804434776306\n",
            "train loss: 0.6450245976448059\n",
            "train loss: 1.6281245946884155\n",
            "train loss: 0.7572367787361145\n",
            "train loss: 0.8126829862594604\n",
            "train loss: 0.3839779198169708\n",
            "train loss: 0.8836079835891724\n",
            "train loss: 0.6510812044143677\n",
            "train loss: 1.0887242555618286\n",
            "train loss: 0.5419903993606567\n",
            "train loss: 1.1005133390426636\n",
            "train loss: 0.5658790469169617\n",
            "train loss: 0.5628626346588135\n",
            "train loss: 0.4877340495586395\n",
            "train loss: 1.4561026096343994\n",
            "train loss: 1.1075774431228638\n",
            "train loss: 1.5129424333572388\n",
            "train loss: 0.8208988308906555\n",
            "train loss: 0.9771900177001953\n",
            "train loss: 1.4628334045410156\n",
            "train loss: 0.38431504368782043\n",
            "train loss: 0.30637165904045105\n",
            "train loss: 1.3557835817337036\n",
            "train loss: 0.5337744355201721\n",
            "train loss: 0.2548542320728302\n",
            "train loss: 1.1191043853759766\n",
            "train loss: 1.0901520252227783\n",
            "train loss: 1.4333566427230835\n",
            "train loss: 1.3241089582443237\n",
            "train loss: 0.9149560928344727\n",
            "train loss: 0.25921982526779175\n",
            "train loss: 0.46355631947517395\n",
            "train loss: 0.48040294647216797\n",
            "train loss: 0.8412840366363525\n",
            "train loss: 0.495332807302475\n",
            "train loss: 0.5526981949806213\n",
            "train loss: 1.2212543487548828\n",
            "train loss: 1.3428375720977783\n",
            "train loss: 0.5899067521095276\n",
            "train loss: 0.36406952142715454\n",
            "train loss: 0.35327431559562683\n",
            "train loss: 0.3759273588657379\n",
            "train loss: 1.5021334886550903\n",
            "train loss: 1.9916309118270874\n",
            "train loss: 0.8037290573120117\n",
            "train loss: 0.2284010946750641\n",
            "train loss: 0.45855414867401123\n",
            "train loss: 0.25043216347694397\n",
            "train loss: 0.8134331107139587\n",
            "train loss: 0.28318849205970764\n",
            "train loss: 0.44512248039245605\n",
            "train loss: 1.1302988529205322\n",
            "train loss: 0.7371878027915955\n",
            "train loss: 0.31172817945480347\n",
            "train loss: 2.178575277328491\n",
            "train loss: 0.8610382676124573\n",
            "train loss: 0.8209320306777954\n",
            "train loss: 1.4290555715560913\n",
            "train loss: 0.5703884959220886\n",
            "train loss: 0.16425821185112\n",
            "train loss: 0.5282526016235352\n",
            "train loss: 0.2999684810638428\n",
            "train loss: 0.8673090934753418\n",
            "train loss: 0.4563034772872925\n",
            "train loss: 1.5915272235870361\n",
            "train loss: 0.8028751015663147\n",
            "train loss: 0.6574994921684265\n",
            "train loss: 0.46162378787994385\n",
            "train loss: 0.6377332806587219\n",
            "train loss: 1.3977437019348145\n",
            "train loss: 0.4578271210193634\n",
            "train loss: 0.2950648069381714\n",
            "train loss: 0.6882234811782837\n",
            "train loss: 0.5084553360939026\n",
            "train loss: 0.5353729724884033\n",
            "train loss: 0.4744899272918701\n",
            "train loss: 0.41103777289390564\n",
            "train loss: 0.3405359089374542\n",
            "train loss: 0.38359978795051575\n",
            "train loss: 0.7934275269508362\n",
            "train loss: 1.6668272018432617\n",
            "train loss: 1.8591755628585815\n",
            "train loss: 0.9044272303581238\n",
            "train loss: 1.3524142503738403\n",
            "train loss: 0.7941617965698242\n",
            "train loss: 0.46844062209129333\n",
            "train loss: 0.8328208327293396\n",
            "train loss: 0.38592174649238586\n",
            "train loss: 0.6239651441574097\n",
            "train loss: 0.7073349952697754\n",
            "train loss: 0.38819625973701477\n",
            "train loss: 1.2717599868774414\n",
            "train loss: 0.9328885078430176\n",
            "train loss: 1.5866755247116089\n",
            "train loss: 0.8487623333930969\n",
            "train loss: 0.4847521483898163\n",
            "train loss: 1.367270827293396\n",
            "train loss: 0.2724435031414032\n",
            "train loss: 0.34442421793937683\n",
            "train loss: 0.547120213508606\n",
            "train loss: 1.3634803295135498\n",
            "train loss: 1.021506428718567\n",
            "train loss: 0.5943124890327454\n",
            "train loss: 0.542755663394928\n",
            "train loss: 0.19179727137088776\n",
            "train loss: 0.47222208976745605\n",
            "train loss: 1.8095142841339111\n",
            "train loss: 0.42461878061294556\n",
            "train loss: 0.3878558874130249\n",
            "train loss: 0.36758261919021606\n",
            "train loss: 0.4413945972919464\n",
            "train loss: 0.4062899053096771\n",
            "train loss: 1.4713271856307983\n",
            "train loss: 0.398104727268219\n",
            "train loss: 0.7946462631225586\n",
            "train loss: 0.3439822793006897\n",
            "train loss: 0.8148448467254639\n",
            "train loss: 0.314449280500412\n",
            "train loss: 0.8382984399795532\n",
            "train loss: 0.1966092884540558\n",
            "train loss: 0.5791316032409668\n",
            "train loss: 0.16238181293010712\n",
            "train loss: 0.8976300358772278\n",
            "train loss: 0.2393690049648285\n",
            "train loss: 0.9015745520591736\n",
            "train loss: 1.0382460355758667\n",
            "train loss: 0.8985700607299805\n",
            "train loss: 1.6566689014434814\n",
            "train loss: 0.9266992807388306\n",
            "train loss: 0.6253625154495239\n",
            "train loss: 1.7308682203292847\n",
            "train loss: 0.7597671747207642\n",
            "train loss: 0.8704010844230652\n",
            "train loss: 0.7400892972946167\n",
            "train loss: 1.2269309759140015\n",
            "train loss: 0.2515469789505005\n",
            "train loss: 0.44281357526779175\n",
            "train loss: 0.9032588601112366\n",
            "train loss: 0.5034580230712891\n",
            "train loss: 1.2427109479904175\n",
            "train loss: 1.7677091360092163\n",
            "train loss: 1.0040401220321655\n",
            "train loss: 0.4750517010688782\n",
            "train loss: 0.5696369409561157\n",
            "train loss: 0.363136887550354\n",
            "train loss: 1.0269312858581543\n",
            "train loss: 1.2157485485076904\n",
            "train loss: 0.7385869026184082\n",
            "train loss: 0.49877339601516724\n",
            "train loss: 0.5549538731575012\n",
            "train loss: 0.6700859665870667\n",
            "train loss: 0.8616392612457275\n",
            "train loss: 0.3874559998512268\n",
            "train loss: 0.28677353262901306\n",
            "train loss: 0.28048720955848694\n",
            "train loss: 0.7004538774490356\n",
            "train loss: 0.2702232301235199\n",
            "train loss: 0.8455072045326233\n",
            "train loss: 0.41527339816093445\n",
            "train loss: 0.21570368111133575\n",
            "train loss: 1.6185262203216553\n",
            "train loss: 0.559314489364624\n",
            "train loss: 0.7985306978225708\n",
            "train loss: 1.5029152631759644\n",
            "train loss: 0.561917781829834\n",
            "train loss: 0.5210303664207458\n",
            "train loss: 0.7901951670646667\n",
            "train loss: 0.35934287309646606\n",
            "train loss: 0.5386107563972473\n",
            "train loss: 0.7298346757888794\n",
            "train loss: 0.7663220167160034\n",
            "train loss: 0.3056480586528778\n",
            "train loss: 0.7219275236129761\n",
            "train loss: 0.6215274333953857\n",
            "train loss: 0.36973729729652405\n",
            "train loss: 0.7655583620071411\n",
            "train loss: 0.646380603313446\n",
            "train loss: 0.4715573787689209\n",
            "train loss: 0.5721458196640015\n",
            "train loss: 0.3568519353866577\n",
            "train loss: 0.4708768129348755\n",
            "train loss: 0.9543279409408569\n",
            "train loss: 0.6527089476585388\n",
            "train loss: 0.6016505360603333\n",
            "train loss: 0.8251899480819702\n",
            "train loss: 1.224283218383789\n",
            "train loss: 1.349070429801941\n",
            "train loss: 0.4478355944156647\n",
            "train loss: 0.6840604543685913\n",
            "train loss: 0.543457567691803\n",
            "train loss: 1.0262717008590698\n",
            "train loss: 0.5474140644073486\n",
            "train loss: 1.7703856229782104\n",
            "train loss: 0.21666239202022552\n",
            "train loss: 0.17513951659202576\n",
            "train loss: 0.6632795333862305\n",
            "train loss: 0.43565523624420166\n",
            "train loss: 0.4078733026981354\n",
            "train loss: 0.2809126675128937\n",
            "train loss: 0.879322350025177\n",
            "train loss: 0.33264079689979553\n",
            "train loss: 0.7105620503425598\n",
            "train loss: 0.7409710884094238\n",
            "train loss: 0.5939308404922485\n",
            "train loss: 1.1125950813293457\n",
            "train loss: 1.7743428945541382\n",
            "train loss: 0.8067277669906616\n",
            "train loss: 0.9913514256477356\n",
            "train loss: 1.0562279224395752\n",
            "train loss: 0.37028369307518005\n",
            "train loss: 1.0680913925170898\n",
            "train loss: 0.9907656311988831\n",
            "train loss: 1.0035444498062134\n",
            "train loss: 0.8312145471572876\n",
            "train loss: 0.5204410552978516\n",
            "train loss: 0.6525543332099915\n",
            "train loss: 0.43619734048843384\n",
            "train loss: 0.4641709625720978\n",
            "train loss: 0.46430131793022156\n",
            "train loss: 0.22799701988697052\n",
            "train loss: 0.9674129486083984\n",
            "train loss: 0.29722151160240173\n",
            "train loss: 0.299687922000885\n",
            "train loss: 0.8011864423751831\n",
            "train loss: 0.7611868977546692\n",
            "train loss: 1.392802119255066\n",
            "train loss: 0.33912214636802673\n",
            "train loss: 1.0626438856124878\n",
            "train loss: 0.36688873171806335\n",
            "train loss: 0.21129746735095978\n",
            "train loss: 0.4688488841056824\n",
            "train loss: 1.1013894081115723\n",
            "train loss: 0.416131854057312\n",
            "train loss: 0.2624918818473816\n",
            "train loss: 0.7218971848487854\n",
            "train loss: 0.9417540431022644\n",
            "train loss: 1.084492802619934\n",
            "train loss: 0.8767272233963013\n",
            "train loss: 0.2910817861557007\n",
            "train loss: 0.6229637265205383\n",
            "train loss: 0.8369665741920471\n",
            "train loss: 0.3703809380531311\n",
            "train loss: 0.09073183685541153\n",
            "train loss: 1.0057564973831177\n",
            "train loss: 0.7086896896362305\n",
            "train loss: 0.22006818652153015\n",
            "train loss: 1.473048448562622\n",
            "train loss: 0.4390448033809662\n",
            "train loss: 0.8810851573944092\n",
            "train loss: 0.30209967494010925\n",
            "train loss: 0.6041229963302612\n",
            "train loss: 0.710443913936615\n",
            "train loss: 0.33362558484077454\n",
            "train loss: 0.35484275221824646\n",
            "train loss: 0.8153418898582458\n",
            "train loss: 0.7395437359809875\n",
            "train loss: 1.287506103515625\n",
            "train loss: 0.38690662384033203\n",
            "train loss: 0.6197247505187988\n",
            "train loss: 0.903584361076355\n",
            "train loss: 0.3756243884563446\n",
            "train loss: 0.5396494269371033\n",
            "train loss: 0.6104268431663513\n",
            "train loss: 1.0713086128234863\n",
            "train loss: 0.30776938796043396\n",
            "train loss: 0.522200882434845\n",
            "train loss: 0.8126936554908752\n",
            "train loss: 0.8519524931907654\n",
            "train loss: 0.23567411303520203\n",
            "train loss: 0.5890746116638184\n",
            "train loss: 0.4956953227519989\n",
            "train loss: 0.5509341955184937\n",
            "train loss: 0.5036308169364929\n",
            "train loss: 0.7620677947998047\n",
            "train loss: 0.5314292311668396\n",
            "train loss: 1.0439395904541016\n",
            "train loss: 0.9577149152755737\n",
            "train loss: 0.5892794728279114\n",
            "train loss: 1.0966362953186035\n",
            "train loss: 0.32547682523727417\n",
            "train loss: 0.15886084735393524\n",
            "train loss: 0.2029431164264679\n",
            "train loss: 0.2536140978336334\n",
            "train loss: 0.30683979392051697\n",
            "train loss: 0.44313541054725647\n",
            "train loss: 0.4146150052547455\n",
            "train loss: 1.0487778186798096\n",
            "train loss: 0.4099368453025818\n",
            "train loss: 0.8776692152023315\n",
            "train loss: 0.5246597528457642\n",
            "train loss: 0.6219130754470825\n",
            "train loss: 0.31491467356681824\n",
            "train loss: 0.4233720004558563\n",
            "train loss: 0.4086951017379761\n",
            "train loss: 0.43192821741104126\n",
            "train loss: 0.8041074872016907\n",
            "train loss: 0.09362243115901947\n",
            "train loss: 1.495257019996643\n",
            "train loss: 0.13385988771915436\n",
            "train loss: 0.8690346479415894\n",
            "train loss: 0.21323658525943756\n",
            "train loss: 0.72328782081604\n",
            "train loss: 0.06702033430337906\n",
            "train loss: 0.2562152147293091\n",
            "train loss: 0.24023234844207764\n",
            "train loss: 0.3897961676120758\n",
            "train loss: 1.1885957717895508\n",
            "train loss: 0.3040587306022644\n",
            "train loss: 0.33589962124824524\n",
            "train loss: 0.8611817359924316\n",
            "train loss: 0.24770201742649078\n",
            "train loss: 0.41049379110336304\n",
            "train loss: 0.3987019658088684\n",
            "train loss: 0.3368748128414154\n",
            "train loss: 1.5321375131607056\n",
            "train loss: 0.45261242985725403\n",
            "train loss: 0.8497629165649414\n",
            "train loss: 0.24520918726921082\n",
            "train loss: 0.1368870735168457\n",
            "train loss: 0.35384097695350647\n",
            "train loss: 0.49683770537376404\n",
            "train loss: 0.18584248423576355\n",
            "train loss: 0.16990968585014343\n",
            "train loss: 0.18351851403713226\n",
            "train loss: 1.2722129821777344\n",
            "train loss: 0.8775643706321716\n",
            "train loss: 0.8138299584388733\n",
            "train loss: 1.544601321220398\n",
            "train loss: 1.2204318046569824\n",
            "train loss: 0.5812492966651917\n",
            "train loss: 0.6692894697189331\n",
            "train loss: 0.5123094320297241\n",
            "train loss: 0.2482023984193802\n",
            "train loss: 0.2783716022968292\n",
            "train loss: 0.7178474068641663\n",
            "train loss: 0.6012252569198608\n",
            "train loss: 0.35771337151527405\n",
            "train loss: 1.2497727870941162\n",
            "train loss: 0.794307291507721\n",
            "train loss: 0.2610439360141754\n",
            "train loss: 0.3337230682373047\n",
            "train loss: 1.2087689638137817\n",
            "train loss: 0.24008949100971222\n",
            "train loss: 0.25329291820526123\n",
            "train loss: 0.7921798229217529\n",
            "train loss: 0.23222605884075165\n",
            "train loss: 0.04844347760081291\n",
            "train loss: 0.47322869300842285\n",
            "train loss: 0.5701302886009216\n",
            "train loss: 0.6290709376335144\n",
            "train loss: 0.9546913504600525\n",
            "train loss: 1.0199936628341675\n",
            "train loss: 1.3050633668899536\n",
            "train loss: 0.6258441805839539\n",
            "train loss: 1.115927815437317\n",
            "train loss: 1.1599068641662598\n",
            "train loss: 0.7273770570755005\n",
            "train loss: 1.0648949146270752\n",
            "train loss: 0.2408178150653839\n",
            "train loss: 0.6724591851234436\n",
            "train loss: 0.14186760783195496\n",
            "train loss: 0.8476340174674988\n",
            "train loss: 0.30441394448280334\n",
            "train loss: 0.6446967124938965\n",
            "train loss: 0.6826150417327881\n",
            "train loss: 0.8236597180366516\n",
            "train loss: 0.5985763669013977\n",
            "train loss: 1.4455138444900513\n",
            "train loss: 0.29826271533966064\n",
            "train loss: 1.6050304174423218\n",
            "train loss: 1.2525177001953125\n",
            "train loss: 0.5466512441635132\n",
            "train loss: 0.23371665179729462\n",
            "train loss: 0.42870068550109863\n",
            "train loss: 0.40704599022865295\n",
            "train loss: 0.3424857556819916\n",
            "train loss: 0.4578339755535126\n",
            "train loss: 0.8245103359222412\n",
            "train loss: 0.8907506465911865\n",
            "train loss: 1.1460686922073364\n",
            "train loss: 1.3153433799743652\n",
            "train loss: 0.08417325466871262\n",
            "train loss: 0.424766480922699\n",
            "train loss: 0.8148692846298218\n",
            "train loss: 0.21977297961711884\n",
            "train loss: 0.7077466249465942\n",
            "train loss: 0.9457653164863586\n",
            "train loss: 0.5707356929779053\n",
            "train loss: 0.6454743146896362\n",
            "train loss: 0.18159620463848114\n",
            "train loss: 0.9095557928085327\n",
            "train loss: 0.1534672975540161\n",
            "train loss: 0.39973732829093933\n",
            "train loss: 0.3993237018585205\n",
            "train loss: 0.41990146040916443\n",
            "train loss: 0.7873371839523315\n",
            "train loss: 0.5745077133178711\n",
            "train loss: 0.0795043483376503\n",
            "train loss: 0.3203224241733551\n",
            "train loss: 1.0023936033248901\n",
            "train loss: 0.2805364429950714\n",
            "train loss: 0.7591161131858826\n",
            "train loss: 0.33456048369407654\n",
            "train loss: 0.23109667003154755\n",
            "train loss: 0.4075324237346649\n",
            "train loss: 0.35572439432144165\n",
            "train loss: 0.2801392376422882\n",
            "train loss: 0.5611465573310852\n",
            "train loss: 1.2708923816680908\n",
            "train loss: 0.2758542001247406\n",
            "train loss: 0.9352542757987976\n",
            "train loss: 1.1651784181594849\n",
            "train loss: 0.27989742159843445\n",
            "train loss: 1.2506321668624878\n",
            "train loss: 0.1624203324317932\n",
            "train loss: 0.6929761171340942\n",
            "train loss: 0.10238158702850342\n",
            "train loss: 0.404166042804718\n",
            "train loss: 1.0089162588119507\n",
            "train loss: 0.6385909914970398\n",
            "train loss: 1.066951870918274\n",
            "train loss: 0.12112799286842346\n",
            "train loss: 1.4156107902526855\n",
            "train loss: 0.25881513953208923\n",
            "train loss: 0.3925985097885132\n",
            "train loss: 0.1689695566892624\n",
            "train loss: 0.5249359011650085\n",
            "train loss: 0.6381426453590393\n",
            "train loss: 0.7243332266807556\n",
            "train loss: 0.47758278250694275\n",
            "train loss: 0.37153106927871704\n",
            "train loss: 0.44837915897369385\n",
            "train loss: 1.3083081245422363\n",
            "train loss: 1.0989357233047485\n",
            "train loss: 0.31214001774787903\n",
            "train loss: 0.06396882236003876\n",
            "train loss: 0.4201953709125519\n",
            "train loss: 0.597656786441803\n",
            "train loss: 0.8202152848243713\n",
            "train loss: 0.212140291929245\n",
            "train loss: 0.9729232788085938\n",
            "train loss: 0.7620052695274353\n",
            "train loss: 0.325685977935791\n",
            "train loss: 0.8947652578353882\n",
            "train loss: 0.6482058167457581\n",
            "train loss: 0.8000503778457642\n",
            "train loss: 0.5336142182350159\n",
            "train loss: 0.2677740156650543\n",
            "train loss: 1.0594990253448486\n",
            "train loss: 0.22883151471614838\n",
            "train loss: 0.4077349901199341\n",
            "train loss: 0.10612945258617401\n",
            "train loss: 0.9818088412284851\n",
            "train loss: 0.9382303953170776\n",
            "train loss: 0.08920252323150635\n",
            "train loss: 0.8858219385147095\n",
            "train loss: 0.641146183013916\n",
            "train loss: 0.24477756023406982\n",
            "train loss: 0.5663992762565613\n",
            "train loss: 0.6134527921676636\n",
            "train loss: 0.80783611536026\n",
            "train loss: 0.32611992955207825\n",
            "train loss: 0.38525763154029846\n",
            "train loss: 0.5523390173912048\n",
            "train loss: 1.563115119934082\n",
            "train loss: 0.9636635184288025\n",
            "train loss: 0.23992830514907837\n",
            "train loss: 1.023744821548462\n",
            "train loss: 0.3761201798915863\n",
            "train loss: 0.10011649876832962\n",
            "train loss: 0.5727942585945129\n",
            "train loss: 1.054226279258728\n",
            "train loss: 0.39523351192474365\n",
            "train loss: 0.25529590249061584\n",
            "train loss: 1.3127785921096802\n",
            "train loss: 1.1727714538574219\n",
            "train loss: 0.42800238728523254\n",
            "train loss: 1.1118501424789429\n",
            "train loss: 0.24659420549869537\n",
            "train loss: 0.529887855052948\n",
            "train loss: 0.6788472533226013\n",
            "train loss: 0.33410021662712097\n",
            "train loss: 1.4045182466506958\n",
            "train loss: 1.0060906410217285\n",
            "train loss: 0.8907134532928467\n",
            "train loss: 0.777774453163147\n",
            "train loss: 0.5021951198577881\n",
            "train loss: 0.20815236866474152\n",
            "train loss: 0.9695236086845398\n",
            "train loss: 0.15361231565475464\n",
            "train loss: 1.5194311141967773\n",
            "train loss: 1.666586995124817\n",
            "train loss: 1.3212496042251587\n",
            "train loss: 0.1716713309288025\n",
            "train loss: 0.2501256763935089\n",
            "train loss: 0.2721341848373413\n",
            "train loss: 0.8602153062820435\n",
            "train loss: 0.23602509498596191\n",
            "train loss: 0.14409419894218445\n",
            "train loss: 0.8899245858192444\n",
            "train loss: 0.5090645551681519\n",
            "train loss: 0.3808687925338745\n",
            "train loss: 0.8991910815238953\n",
            "train loss: 0.933345377445221\n",
            "train loss: 1.1951444149017334\n",
            "train loss: 1.0574462413787842\n",
            "train loss: 0.5363935828208923\n",
            "train loss: 0.37148192524909973\n",
            "train loss: 0.4020634889602661\n",
            "train loss: 0.8294262886047363\n",
            "train loss: 0.09105484187602997\n",
            "train loss: 0.3970942795276642\n",
            "train loss: 0.2554275095462799\n",
            "train loss: 0.3351770341396332\n",
            "train loss: 0.3708336055278778\n",
            "train loss: 0.7420058846473694\n",
            "train loss: 0.55128014087677\n",
            "train loss: 0.7638190984725952\n",
            "train loss: 1.1203802824020386\n",
            "train loss: 0.2989766597747803\n",
            "train loss: 0.8158904314041138\n",
            "train loss: 0.2742633819580078\n",
            "train loss: 0.06694868952035904\n",
            "train loss: 1.4247241020202637\n",
            "train loss: 0.12821254134178162\n",
            "train loss: 0.12899039685726166\n",
            "train loss: 0.32299986481666565\n",
            "train loss: 0.28721025586128235\n",
            "train loss: 0.2999463379383087\n",
            "train loss: 0.36990785598754883\n",
            "train loss: 0.48611587285995483\n",
            "train loss: 0.25353720784187317\n",
            "train loss: 0.32839077711105347\n",
            "train loss: 0.7685468792915344\n",
            "train loss: 0.3148321211338043\n",
            "train loss: 0.19774781167507172\n",
            "train loss: 0.9741261005401611\n",
            "train loss: 1.425812005996704\n",
            "train loss: 0.5982699394226074\n",
            "train loss: 0.13286933302879333\n",
            "train loss: 0.281267911195755\n",
            "train loss: 0.544915497303009\n",
            "train loss: 1.5224900245666504\n",
            "train loss: 0.13876742124557495\n",
            "train loss: 0.1562853753566742\n",
            "train loss: 0.9560028910636902\n",
            "train loss: 0.7726663947105408\n",
            "train loss: 0.6524200439453125\n",
            "train loss: 0.45435652136802673\n",
            "train loss: 0.9093201756477356\n",
            "train loss: 0.23614196479320526\n",
            "train loss: 0.5839020013809204\n",
            "train loss: 0.29340675473213196\n",
            "train loss: 0.7547017931938171\n",
            "train loss: 0.7720787525177002\n",
            "train loss: 0.6983250379562378\n",
            "train loss: 0.3705352544784546\n",
            "train loss: 0.07850765436887741\n",
            "train loss: 0.9282823801040649\n",
            "train loss: 0.23843540251255035\n",
            "train loss: 0.2735711634159088\n",
            "train loss: 0.7493056654930115\n",
            "train loss: 0.9266815185546875\n",
            "train loss: 0.4308384656906128\n",
            "train loss: 0.2134053111076355\n",
            "train loss: 0.2230304479598999\n",
            "train loss: 0.3078823685646057\n",
            "train loss: 0.2852874994277954\n",
            "train loss: 0.45697319507598877\n",
            "train loss: 0.388146311044693\n",
            "train loss: 0.3676251769065857\n",
            "train loss: 0.42184871435165405\n",
            "train loss: 0.18291370570659637\n",
            "train loss: 0.5216330289840698\n",
            "train loss: 0.7228025794029236\n",
            "train loss: 1.0359833240509033\n",
            "train loss: 0.7623975276947021\n",
            "train loss: 0.4543394148349762\n",
            "train loss: 0.4149458706378937\n",
            "train loss: 0.25028926134109497\n",
            "train loss: 0.37365010380744934\n",
            "train loss: 1.1116490364074707\n",
            "train loss: 0.2030569314956665\n",
            "train loss: 1.091828465461731\n",
            "train loss: 0.33181360363960266\n",
            "train loss: 0.8732041120529175\n",
            "train loss: 0.6303306818008423\n",
            "train loss: 0.06412825733423233\n",
            "train loss: 0.3847941756248474\n",
            "train loss: 0.7699713110923767\n",
            "train loss: 0.48943814635276794\n",
            "train loss: 0.4566532373428345\n",
            "train loss: 0.11935040354728699\n",
            "train loss: 0.5972229242324829\n",
            "train loss: 0.4162496328353882\n",
            "train loss: 0.4383294880390167\n",
            "train loss: 0.6022714376449585\n",
            "train loss: 0.060228146612644196\n",
            "train loss: 0.6310607194900513\n",
            "train loss: 0.20487645268440247\n",
            "train loss: 0.28601697087287903\n",
            "train loss: 0.6243656277656555\n",
            "train loss: 0.2445562183856964\n",
            "train loss: 0.33410879969596863\n",
            "train loss: 0.8621606230735779\n",
            "train loss: 0.2907494604587555\n",
            "train loss: 0.9440867304801941\n",
            "train loss: 0.27920493483543396\n",
            "train loss: 0.6661142110824585\n",
            "train loss: 0.6533514857292175\n",
            "train loss: 1.1734219789505005\n",
            "train loss: 1.12667977809906\n",
            "train loss: 0.1297701746225357\n",
            "train loss: 0.3881252408027649\n",
            "train loss: 0.2850162982940674\n",
            "train loss: 0.24504248797893524\n",
            "train loss: 1.3129479885101318\n",
            "train loss: 0.37665072083473206\n",
            "train loss: 0.2624902129173279\n",
            "train loss: 0.470876544713974\n",
            "train loss: 0.38304489850997925\n",
            "train loss: 1.657415747642517\n",
            "train loss: 0.3730127513408661\n",
            "train loss: 0.530763566493988\n",
            "train loss: 1.4589970111846924\n",
            "train loss: 0.533557116985321\n",
            "train loss: 1.0956958532333374\n",
            "train loss: 0.1836150884628296\n",
            "train loss: 1.081749439239502\n",
            "train loss: 0.39464566111564636\n",
            "train loss: 0.3791942000389099\n",
            "train loss: 0.4079169034957886\n",
            "train loss: 0.24902568757534027\n",
            "train loss: 0.561105489730835\n",
            "train loss: 0.521571159362793\n",
            "train loss: 0.5159031748771667\n",
            "train loss: 0.19355709850788116\n",
            "train loss: 0.6450888514518738\n",
            "train loss: 0.36534684896469116\n",
            "train loss: 0.9076105356216431\n",
            "train loss: 0.5634915828704834\n",
            "train loss: 0.8855066299438477\n",
            "train loss: 1.169599175453186\n",
            "train loss: 1.171616792678833\n",
            "train loss: 0.6438948512077332\n",
            "train loss: 0.23238180577754974\n",
            "train loss: 0.6863481402397156\n",
            "train loss: 0.23026058077812195\n",
            "train loss: 0.8861388564109802\n",
            "train loss: 0.19798731803894043\n",
            "train loss: 0.5397966504096985\n",
            "train loss: 0.4972268044948578\n",
            "train loss: 0.32306545972824097\n",
            "train loss: 0.9554811120033264\n",
            "train loss: 0.357839971780777\n",
            "train loss: 0.7054612636566162\n",
            "train loss: 0.3792996406555176\n",
            "train loss: 0.3570402264595032\n",
            "train loss: 0.3227056562900543\n",
            "train loss: 0.7005227208137512\n",
            "train loss: 0.08434029668569565\n",
            "train loss: 1.5058380365371704\n",
            "train loss: 0.4526369869709015\n",
            "train loss: 0.02372553199529648\n",
            "train loss: 0.5647380948066711\n",
            "train loss: 0.9449070692062378\n",
            "train loss: 0.9452955722808838\n",
            "train loss: 0.43345698714256287\n",
            "train loss: 0.13158637285232544\n",
            "train loss: 0.2210669070482254\n",
            "train loss: 0.4557753801345825\n",
            "train loss: 1.0187299251556396\n",
            "train loss: 0.3020142614841461\n",
            "train loss: 0.5214666724205017\n",
            "train loss: 0.316593736410141\n",
            "train loss: 0.4925879240036011\n",
            "train loss: 0.5185693502426147\n",
            "train loss: 0.5975365042686462\n",
            "train loss: 1.1434122323989868\n",
            "train loss: 0.21728041768074036\n",
            "train loss: 0.40044015645980835\n",
            "train loss: 0.3107231557369232\n",
            "train loss: 0.5664339661598206\n",
            "train loss: 1.6090654134750366\n",
            "train loss: 1.14616858959198\n",
            "train loss: 0.6386731863021851\n",
            "train loss: 0.2423432469367981\n",
            "train loss: 1.2815396785736084\n",
            "train loss: 0.356918066740036\n",
            "train loss: 0.26273855566978455\n",
            "train loss: 0.6964295506477356\n",
            "train loss: 1.089823603630066\n",
            "train loss: 1.232690691947937\n",
            "train loss: 0.3284839391708374\n",
            "train loss: 0.2499832808971405\n",
            "train loss: 0.5466873049736023\n",
            "train loss: 0.056287381798028946\n",
            "train loss: 1.2837010622024536\n",
            "train loss: 0.17905479669570923\n",
            "train loss: 1.1377308368682861\n",
            "train loss: 0.3736570477485657\n",
            "train loss: 0.6417768001556396\n",
            "train loss: 1.0667040348052979\n",
            "train loss: 0.9027050137519836\n",
            "train loss: 0.8142824769020081\n",
            "train loss: 0.07731149345636368\n",
            "train loss: 0.9563537240028381\n",
            "train loss: 0.5058751106262207\n",
            "train loss: 1.1441999673843384\n",
            "train loss: 0.6377132534980774\n",
            "train loss: 0.29889291524887085\n",
            "train loss: 0.20002299547195435\n",
            "train loss: 0.6920163631439209\n",
            "train loss: 0.4135562777519226\n",
            "train loss: 0.4126920998096466\n",
            "train loss: 0.05564507469534874\n",
            "train loss: 0.37921056151390076\n",
            "train loss: 0.48915988206863403\n",
            "train loss: 1.0958534479141235\n",
            "train loss: 0.11042981594800949\n",
            "train loss: 0.07298050075769424\n",
            "train loss: 1.399781346321106\n",
            "train loss: 1.1137700080871582\n",
            "train loss: 1.694669246673584\n",
            "train loss: 0.27901792526245117\n",
            "train loss: 0.18119719624519348\n",
            "train loss: 1.7066688537597656\n",
            "train loss: 1.0256280899047852\n",
            "train loss: 0.14111360907554626\n",
            "train loss: 1.1078306436538696\n",
            "train loss: 0.3699483573436737\n",
            "train loss: 0.15693895518779755\n",
            "train loss: 0.5510254502296448\n",
            "train loss: 0.43536388874053955\n",
            "train loss: 0.41833505034446716\n",
            "train loss: 0.2375931441783905\n",
            "train loss: 0.5619818568229675\n",
            "train loss: 0.3509695529937744\n",
            "train loss: 1.3022708892822266\n",
            "train loss: 0.7491787075996399\n",
            "train loss: 0.7656335830688477\n",
            "train loss: 0.4198170304298401\n",
            "train loss: 1.1969783306121826\n",
            "train loss: 0.5024388432502747\n",
            "train loss: 0.07218474894762039\n",
            "train loss: 0.5010865926742554\n",
            "train loss: 0.40320101380348206\n",
            "train loss: 0.7569745182991028\n",
            "train loss: 0.5559740662574768\n",
            "train loss: 0.19859442114830017\n",
            "train loss: 0.4299091100692749\n",
            "train loss: 0.6329010128974915\n",
            "train loss: 0.2816981077194214\n",
            "train loss: 0.3640184998512268\n",
            "train loss: 0.5214013457298279\n",
            "train loss: 0.35012388229370117\n",
            "train loss: 0.2942816913127899\n",
            "train loss: 0.5539299845695496\n",
            "train loss: 0.12134139984846115\n",
            "train loss: 0.4013315737247467\n",
            "train loss: 0.4475443661212921\n",
            "train loss: 0.4569014608860016\n",
            "train loss: 0.20509226620197296\n",
            "train loss: 0.10492344945669174\n",
            "train loss: 0.3522022068500519\n",
            "train loss: 1.2505356073379517\n",
            "train loss: 0.4051714539527893\n",
            "train loss: 0.5347732305526733\n",
            "train loss: 0.18873828649520874\n",
            "train loss: 0.3131546676158905\n",
            "train loss: 1.0918219089508057\n",
            "train loss: 0.13553465902805328\n",
            "train loss: 0.5808721780776978\n",
            "train loss: 0.34746259450912476\n",
            "train loss: 0.59658282995224\n",
            "train loss: 1.5979859828948975\n",
            "train loss: 0.8471494913101196\n",
            "train loss: 0.3287705183029175\n",
            "train loss: 0.573628306388855\n",
            "train loss: 0.1382410079240799\n",
            "train loss: 1.2098932266235352\n",
            "train loss: 0.16995806992053986\n",
            "train loss: 0.8387835621833801\n",
            "train loss: 0.7478434443473816\n",
            "train loss: 1.2441399097442627\n",
            "train loss: 0.29911643266677856\n",
            "train loss: 0.19706322252750397\n",
            "train loss: 0.8971683979034424\n",
            "train loss: 0.2518142759799957\n",
            "train loss: 0.8399701714515686\n",
            "train loss: 0.3077673017978668\n",
            "train loss: 0.10719969868659973\n",
            "train loss: 0.3916108012199402\n",
            "train loss: 0.7547819018363953\n",
            "train loss: 1.3671698570251465\n",
            "train loss: 0.2030528038740158\n",
            "train loss: 0.4094591438770294\n",
            "train loss: 0.22729316353797913\n",
            "train loss: 1.0437771081924438\n",
            "train loss: 1.4932149648666382\n",
            "train loss: 1.125178337097168\n",
            "train loss: 0.18248313665390015\n",
            "train loss: 1.0546530485153198\n",
            "train loss: 0.36850976943969727\n",
            "train loss: 1.0033197402954102\n",
            "train loss: 0.4913484752178192\n",
            "train loss: 0.41878437995910645\n",
            "train loss: 0.4078787863254547\n",
            "train loss: 0.3877645432949066\n",
            "train loss: 0.21019360423088074\n",
            "train loss: 0.2598916292190552\n",
            "train loss: 0.15127187967300415\n",
            "train loss: 0.7195712327957153\n",
            "train loss: 0.24541614949703217\n",
            "train loss: 0.45319443941116333\n",
            "train loss: 0.21752643585205078\n",
            "train loss: 0.29404905438423157\n",
            "train loss: 1.4613486528396606\n",
            "train loss: 0.5290728807449341\n",
            "train loss: 0.09682471305131912\n",
            "train loss: 0.24796491861343384\n",
            "train loss: 0.36266592144966125\n",
            "train loss: 1.0654345750808716\n",
            "train loss: 0.45383769273757935\n",
            "train loss: 0.16744495928287506\n",
            "train loss: 0.6537991762161255\n",
            "train loss: 0.6012831330299377\n",
            "train loss: 0.6405712962150574\n",
            "train loss: 0.1837705373764038\n",
            "train loss: 0.34813636541366577\n",
            "train loss: 0.44392886757850647\n",
            "train loss: 1.1980022192001343\n",
            "train loss: 1.0895415544509888\n",
            "train loss: 0.6016308069229126\n",
            "train loss: 0.9204240441322327\n",
            "train loss: 0.34083420038223267\n",
            "train loss: 0.31005024909973145\n",
            "train loss: 1.0545134544372559\n",
            "train loss: 0.4921015799045563\n",
            "train loss: 1.0610235929489136\n",
            "train loss: 0.49564024806022644\n",
            "train loss: 0.09273253381252289\n",
            "train loss: 0.34693410992622375\n",
            "train loss: 1.052355170249939\n",
            "train loss: 0.9996477961540222\n",
            "train loss: 0.5041170716285706\n",
            "train loss: 0.38197770714759827\n",
            "train loss: 0.28273147344589233\n",
            "train loss: 1.037858247756958\n",
            "train loss: 0.6077216267585754\n",
            "train loss: 0.37657037377357483\n",
            "train loss: 1.0593737363815308\n",
            "train loss: 1.6023362874984741\n",
            "train loss: 0.2339000403881073\n",
            "train loss: 0.17728151381015778\n",
            "train loss: 0.4119700491428375\n",
            "train loss: 0.7600260376930237\n",
            "train loss: 0.4816357493400574\n",
            "train loss: 0.4568340480327606\n",
            "train loss: 0.35408490896224976\n",
            "train loss: 0.686051070690155\n",
            "train loss: 1.1493710279464722\n",
            "train loss: 0.16012588143348694\n",
            "train loss: 0.4765698313713074\n",
            "train loss: 0.19958849251270294\n",
            "train loss: 0.9502009153366089\n",
            "train loss: 0.2543795704841614\n",
            "train loss: 0.754100501537323\n",
            "train loss: 0.15077495574951172\n",
            "train loss: 0.5057629346847534\n",
            "train loss: 0.051148466765880585\n",
            "train loss: 0.1882297247648239\n",
            "train loss: 0.31301629543304443\n",
            "train loss: 0.09997142851352692\n",
            "train loss: 0.4507085084915161\n",
            "train loss: 0.5329650640487671\n",
            "train loss: 0.20893815159797668\n",
            "train loss: 0.30514585971832275\n",
            "train loss: 0.31016817688941956\n",
            "train loss: 0.7030795216560364\n",
            "train loss: 0.6553690433502197\n",
            "train loss: 0.29059654474258423\n",
            "train loss: 1.4367667436599731\n",
            "train loss: 0.931438684463501\n",
            "train loss: 0.6596555709838867\n",
            "train loss: 0.8343856930732727\n",
            "train loss: 0.2884371280670166\n",
            "train loss: 0.21285435557365417\n",
            "train loss: 0.4992886483669281\n",
            "train loss: 0.38979846239089966\n",
            "train loss: 0.771765410900116\n",
            "train loss: 0.2854107618331909\n",
            "train loss: 0.6832553744316101\n",
            "train loss: 1.1360589265823364\n",
            "train loss: 0.5831103324890137\n",
            "train loss: 0.5192254781723022\n",
            "train loss: 1.1756501197814941\n",
            "train loss: 0.6266169548034668\n",
            "train loss: 0.4486466646194458\n",
            "train loss: 1.4094618558883667\n",
            "train loss: 0.4870428740978241\n",
            "train loss: 0.3406447470188141\n",
            "train loss: 0.46139290928840637\n",
            "train loss: 0.6444098353385925\n",
            "train loss: 0.4285691976547241\n",
            "train loss: 0.6133208870887756\n",
            "train loss: 0.18178394436836243\n",
            "train loss: 0.03368536755442619\n",
            "train loss: 0.6694859862327576\n",
            "train loss: 0.44635629653930664\n",
            "train loss: 0.5511599183082581\n",
            "train loss: 1.1327641010284424\n",
            "train loss: 0.2571302056312561\n",
            "train loss: 0.06637401878833771\n",
            "train loss: 1.045103669166565\n",
            "train loss: 0.6571305394172668\n",
            "train loss: 0.24030621349811554\n",
            "train loss: 0.38122591376304626\n",
            "train loss: 0.06308069080114365\n",
            "train loss: 1.5358563661575317\n",
            "train loss: 1.0079768896102905\n",
            "train loss: 0.6780017018318176\n",
            "train loss: 1.3413341045379639\n",
            "train loss: 0.6911689639091492\n",
            "train loss: 0.8028733134269714\n",
            "train loss: 1.2400336265563965\n",
            "train loss: 0.6066864728927612\n",
            "train loss: 1.3392341136932373\n",
            "train loss: 0.6118821501731873\n",
            "train loss: 0.8231850862503052\n",
            "train loss: 0.511647641658783\n",
            "train loss: 0.23004725575447083\n",
            "train loss: 0.10216225683689117\n",
            "train loss: 0.9704952239990234\n",
            "train loss: 0.704499363899231\n",
            "train loss: 0.5527262091636658\n",
            "train loss: 0.4726761281490326\n",
            "train loss: 0.07881703227758408\n",
            "train loss: 0.4448854625225067\n",
            "train loss: 0.1614341288805008\n",
            "train loss: 0.2307642549276352\n",
            "train loss: 0.2801479995250702\n",
            "train loss: 1.0683012008666992\n",
            "train loss: 0.02727780118584633\n",
            "train loss: 0.14390945434570312\n",
            "train loss: 0.5679444074630737\n",
            "train loss: 0.48290812969207764\n",
            "train loss: 0.8824970722198486\n",
            "train loss: 0.46371200680732727\n",
            "train loss: 0.08947861194610596\n",
            "train loss: 0.7017487287521362\n",
            "train loss: 0.7705536484718323\n",
            "train loss: 0.6723657250404358\n",
            "train loss: 0.6220827698707581\n",
            "train loss: 0.6753090023994446\n",
            "train loss: 0.11418648809194565\n",
            "train loss: 0.3030778467655182\n",
            "train loss: 0.1719919890165329\n",
            "train loss: 0.08617966622114182\n",
            "train loss: 0.6519466638565063\n",
            "train loss: 0.14714764058589935\n",
            "train loss: 0.0627448633313179\n",
            "train loss: 0.27952033281326294\n",
            "train loss: 1.24537992477417\n",
            "train loss: 1.1990125179290771\n",
            "train loss: 0.7786473035812378\n",
            "train loss: 0.984505832195282\n",
            "train loss: 0.4526594579219818\n",
            "train loss: 0.5349590182304382\n",
            "train loss: 0.42120361328125\n",
            "train loss: 0.4603257477283478\n",
            "train loss: 0.8065541982650757\n",
            "train loss: 0.49850791692733765\n",
            "train loss: 0.2490735948085785\n",
            "train loss: 0.8293651938438416\n",
            "train loss: 0.8954159021377563\n",
            "train loss: 0.29694923758506775\n",
            "train loss: 0.3735581040382385\n",
            "train loss: 0.1504031866788864\n",
            "train loss: 1.1426844596862793\n",
            "train loss: 0.930637776851654\n",
            "train loss: 0.5206677317619324\n",
            "train loss: 1.3046488761901855\n",
            "train loss: 0.05955248326063156\n",
            "train loss: 0.4341331422328949\n",
            "train loss: 0.17637450993061066\n",
            "train loss: 0.3297460377216339\n",
            "train loss: 1.1942040920257568\n",
            "train loss: 0.5635133981704712\n",
            "train loss: 0.16821184754371643\n",
            "train loss: 0.36374956369400024\n",
            "train loss: 0.21827800571918488\n",
            "train loss: 0.30880266427993774\n",
            "train loss: 0.350758820772171\n",
            "train loss: 0.36259427666664124\n",
            "train loss: 0.11995392292737961\n",
            "train loss: 0.14393651485443115\n",
            "train loss: 0.3832545578479767\n",
            "train loss: 0.4668465554714203\n",
            "train loss: 0.8701781630516052\n",
            "train loss: 0.482197105884552\n",
            "train loss: 0.27182140946388245\n",
            "train loss: 0.7604172229766846\n",
            "train loss: 0.36059311032295227\n",
            "train loss: 0.21758083999156952\n",
            "train loss: 0.31410494446754456\n",
            "train loss: 0.2666546404361725\n",
            "train loss: 0.6017860770225525\n",
            "train loss: 1.286105990409851\n",
            "train loss: 0.8090092539787292\n",
            "train loss: 0.314972460269928\n",
            "train loss: 0.740423321723938\n",
            "train loss: 0.2543060779571533\n",
            "train loss: 0.3141951560974121\n",
            "train loss: 0.2590198218822479\n",
            "train loss: 0.1964379847049713\n",
            "train loss: 0.10620692372322083\n",
            "train loss: 0.20996835827827454\n",
            "train loss: 0.26489630341529846\n",
            "train loss: 0.6795940399169922\n",
            "train loss: 0.2811892330646515\n",
            "train loss: 0.45665669441223145\n",
            "train loss: 0.27913761138916016\n",
            "train loss: 0.8055416941642761\n",
            "train loss: 0.5634526610374451\n",
            "train loss: 0.710737407207489\n",
            "train loss: 0.29419878125190735\n",
            "train loss: 1.3190557956695557\n",
            "train loss: 0.34165534377098083\n",
            "train loss: 0.3430558145046234\n",
            "train loss: 0.30022338032722473\n",
            "train loss: 0.29468128085136414\n",
            "train loss: 0.4298320710659027\n",
            "train loss: 0.5420225858688354\n",
            "train loss: 0.48541972041130066\n",
            "train loss: 0.4674074649810791\n",
            "train loss: 0.6938424110412598\n",
            "train loss: 0.2788706421852112\n",
            "train loss: 0.6060540676116943\n",
            "train loss: 0.44410279393196106\n",
            "train loss: 0.5276906490325928\n",
            "train loss: 0.6760967969894409\n",
            "train loss: 0.8404940962791443\n",
            "train loss: 0.2759135663509369\n",
            "train loss: 0.23116445541381836\n",
            "train loss: 0.6760834455490112\n",
            "train loss: 0.21845673024654388\n",
            "train loss: 0.227012038230896\n",
            "train loss: 0.1815832257270813\n",
            "train loss: 0.3261500895023346\n",
            "train loss: 0.9093658328056335\n",
            "train loss: 0.3300516605377197\n",
            "train loss: 0.22255860269069672\n",
            "train loss: 0.24228650331497192\n",
            "train loss: 0.21246230602264404\n",
            "train loss: 0.15730096399784088\n",
            "train loss: 0.21411268413066864\n",
            "train loss: 1.2246214151382446\n",
            "train loss: 0.5173631906509399\n",
            "train loss: 0.7301904559135437\n",
            "train loss: 0.1375463753938675\n",
            "train loss: 0.768004298210144\n",
            "train loss: 1.4807413816452026\n",
            "train loss: 0.40875881910324097\n",
            "train loss: 0.4412075877189636\n",
            "train loss: 0.08219200372695923\n",
            "train loss: 0.7628222107887268\n",
            "train loss: 0.6438889503479004\n",
            "train loss: 0.8860387802124023\n",
            "train loss: 1.1158478260040283\n",
            "train loss: 0.3951873183250427\n",
            "train loss: 0.2374585121870041\n",
            "train loss: 0.6331303715705872\n",
            "train loss: 0.6956039667129517\n",
            "train loss: 0.38203802704811096\n",
            "train loss: 1.4793473482131958\n",
            "train loss: 0.21481408178806305\n",
            "train loss: 0.16972380876541138\n",
            "train loss: 0.2806607484817505\n",
            "train loss: 1.307362675666809\n",
            "train loss: 0.6539897918701172\n",
            "train loss: 0.12299557030200958\n",
            "train loss: 0.2736278474330902\n",
            "train loss: 0.2646420896053314\n",
            "train loss: 0.5969247221946716\n",
            "train loss: 0.18642675876617432\n",
            "train loss: 0.36531487107276917\n",
            "train loss: 1.8105101585388184\n",
            "train loss: 0.6523923873901367\n",
            "train loss: 0.34203872084617615\n",
            "train loss: 1.2719236612319946\n",
            "train loss: 0.1907075047492981\n",
            "train loss: 0.4675922393798828\n",
            "train loss: 0.29984167218208313\n",
            "train loss: 0.5259444713592529\n",
            "train loss: 1.2973006963729858\n",
            "train loss: 0.8591014742851257\n",
            "train loss: 0.4177025854587555\n",
            "train loss: 0.5593215227127075\n",
            "train loss: 0.25373974442481995\n",
            "train loss: 0.5400133728981018\n",
            "train loss: 0.20039987564086914\n",
            "train loss: 0.7616199851036072\n",
            "train loss: 0.29288363456726074\n",
            "train loss: 0.30074140429496765\n",
            "train loss: 1.0113205909729004\n",
            "train loss: 0.5439936518669128\n",
            "train loss: 0.6712254285812378\n",
            "train loss: 0.6457797884941101\n",
            "train loss: 0.3876279592514038\n",
            "train loss: 0.1654265820980072\n",
            "train loss: 0.5105656981468201\n",
            "train loss: 0.3806810677051544\n",
            "train loss: 0.40571296215057373\n",
            "train loss: 1.4748741388320923\n",
            "train loss: 0.11220274865627289\n",
            "train loss: 0.2990110516548157\n",
            "train loss: 0.29139018058776855\n",
            "train loss: 0.81184321641922\n",
            "train loss: 1.4355882406234741\n",
            "train loss: 0.19418111443519592\n",
            "train loss: 0.5700926184654236\n",
            "train loss: 0.15591323375701904\n",
            "train loss: 1.075368881225586\n",
            "train loss: 0.40497779846191406\n",
            "train loss: 0.7704067826271057\n",
            "train loss: 1.4358054399490356\n",
            "train loss: 0.5874806642532349\n",
            "train loss: 0.1948259174823761\n",
            "train loss: 0.591148316860199\n",
            "train loss: 0.08323292434215546\n",
            "train loss: 1.5158019065856934\n",
            "train loss: 1.588663101196289\n",
            "train loss: 0.5961441993713379\n",
            "train loss: 0.3634333312511444\n",
            "train loss: 0.4125478267669678\n",
            "train loss: 0.49665501713752747\n",
            "train loss: 0.24767684936523438\n",
            "train loss: 0.1882891058921814\n",
            "train loss: 0.9137672185897827\n",
            "train loss: 0.2838432192802429\n",
            "train loss: 0.3203980326652527\n",
            "train loss: 0.07641470432281494\n",
            "train loss: 0.38341763615608215\n",
            "train loss: 0.5610322952270508\n",
            "train loss: 0.27422431111335754\n",
            "train loss: 0.36865606904029846\n",
            "train loss: 0.08656668663024902\n",
            "train loss: 0.16451764106750488\n",
            "train loss: 1.2766385078430176\n",
            "train loss: 0.3697015643119812\n",
            "train loss: 0.8406367301940918\n",
            "train loss: 0.6480481028556824\n",
            "train loss: 0.871717095375061\n",
            "train loss: 1.10603666305542\n",
            "train loss: 0.19848419725894928\n",
            "train loss: 0.3780112564563751\n",
            "train loss: 0.19348002970218658\n",
            "train loss: 0.13021382689476013\n",
            "train loss: 0.995908260345459\n",
            "train loss: 0.48631638288497925\n",
            "train loss: 0.1452418863773346\n",
            "train loss: 0.2652878165245056\n",
            "train loss: 0.14766405522823334\n",
            "train loss: 0.17552435398101807\n",
            "train loss: 0.473636656999588\n",
            "train loss: 0.16106157004833221\n",
            "train loss: 0.9689935445785522\n",
            "train loss: 0.08288419991731644\n",
            "train loss: 0.44408005475997925\n",
            "train loss: 0.6970232725143433\n",
            "train loss: 0.3376862406730652\n",
            "train loss: 0.16552923619747162\n",
            "train loss: 0.1565054953098297\n",
            "train loss: 0.1578253209590912\n",
            "train loss: 0.5444262027740479\n",
            "train loss: 0.19536413252353668\n",
            "train loss: 0.32170888781547546\n",
            "train loss: 0.8537086844444275\n",
            "train loss: 0.5073405504226685\n",
            "train loss: 0.34163111448287964\n",
            "train loss: 0.8872806429862976\n",
            "train loss: 1.2000428438186646\n",
            "train loss: 0.25446003675460815\n",
            "train loss: 0.3527296185493469\n",
            "train loss: 0.15750065445899963\n",
            "train loss: 0.6587137579917908\n",
            "train loss: 0.16751886904239655\n",
            "train loss: 1.3187414407730103\n",
            "train loss: 0.36940741539001465\n",
            "train loss: 0.9835814237594604\n",
            "train loss: 0.37528160214424133\n",
            "train loss: 1.0239888429641724\n",
            "train loss: 0.18578913807868958\n",
            "train loss: 0.28235960006713867\n",
            "train loss: 0.24438302218914032\n",
            "train loss: 0.13610970973968506\n",
            "train loss: 0.7191827297210693\n",
            "train loss: 0.8666224479675293\n",
            "train loss: 0.7052226662635803\n",
            "train loss: 0.12238837778568268\n",
            "train loss: 0.08959844708442688\n",
            "train loss: 0.13120947778224945\n",
            "train loss: 0.18797753751277924\n",
            "train loss: 0.3561001718044281\n",
            "train loss: 0.8158989548683167\n",
            "train loss: 0.8350047469139099\n",
            "train loss: 0.6424362659454346\n",
            "train loss: 0.5516980886459351\n",
            "train loss: 0.27152156829833984\n",
            "train loss: 0.4293772876262665\n",
            "train loss: 0.7240245938301086\n",
            "train loss: 0.14405383169651031\n",
            "train loss: 0.5149025917053223\n",
            "train loss: 0.33694738149642944\n",
            "train loss: 0.33945760130882263\n",
            "train loss: 1.1035548448562622\n",
            "train loss: 0.47470325231552124\n",
            "train loss: 0.9416685104370117\n",
            "train loss: 0.7860621809959412\n",
            "train loss: 0.12527985870838165\n",
            "train loss: 1.0896085500717163\n",
            "train loss: 0.2678183615207672\n",
            "train loss: 0.3918847143650055\n",
            "train loss: 0.29611480236053467\n",
            "train loss: 0.21549077332019806\n",
            "train loss: 0.9250441789627075\n",
            "train loss: 1.2704190015792847\n",
            "train loss: 0.12995457649230957\n",
            "train loss: 0.7256037592887878\n",
            "train loss: 1.1479246616363525\n",
            "train loss: 1.1460810899734497\n",
            "train loss: 0.11143019050359726\n",
            "train loss: 0.22782781720161438\n",
            "train loss: 0.10291653871536255\n",
            "train loss: 0.8962417840957642\n",
            "train loss: 0.41647735238075256\n",
            "train loss: 0.8478390574455261\n",
            "train loss: 0.22150744497776031\n",
            "train loss: 0.26161348819732666\n",
            "train loss: 0.5348994135856628\n",
            "train loss: 0.8009704351425171\n",
            "train loss: 0.1322452574968338\n",
            "train loss: 0.41795286536216736\n",
            "train loss: 0.09034112095832825\n",
            "train loss: 0.7174647450447083\n",
            "train loss: 0.04638669639825821\n",
            "train loss: 0.32829350233078003\n",
            "train loss: 0.9068164825439453\n",
            "train loss: 0.6954905986785889\n",
            "train loss: 0.3465425670146942\n",
            "train loss: 0.07252569496631622\n",
            "train loss: 0.13930396735668182\n",
            "train loss: 0.5742833614349365\n",
            "train loss: 0.9370574951171875\n",
            "train loss: 0.8276901245117188\n",
            "train loss: 0.34896430373191833\n",
            "train loss: 0.35830697417259216\n",
            "train loss: 0.507165789604187\n",
            "train loss: 0.5012794137001038\n",
            "train loss: 0.21537058055400848\n",
            "train loss: 0.29566898941993713\n",
            "train loss: 0.5402700304985046\n",
            "train loss: 0.27462342381477356\n",
            "train loss: 0.40276551246643066\n",
            "train loss: 0.5461797118186951\n",
            "train loss: 0.7237719297409058\n",
            "train loss: 0.3137083351612091\n",
            "train loss: 0.2549138367176056\n",
            "train loss: 0.5055531859397888\n",
            "train loss: 0.5052376985549927\n",
            "train loss: 0.1994803547859192\n",
            "train loss: 0.9470603466033936\n",
            "train loss: 0.4061858355998993\n",
            "train loss: 0.7950927019119263\n",
            "train loss: 0.16433674097061157\n",
            "train loss: 0.150301992893219\n",
            "train loss: 0.41506463289260864\n",
            "train loss: 0.10363852977752686\n",
            "train loss: 0.5826712250709534\n",
            "train loss: 1.3596152067184448\n",
            "train loss: 0.4199376702308655\n",
            "train loss: 0.05847857892513275\n",
            "train loss: 0.34854742884635925\n",
            "train loss: 0.2835743725299835\n",
            "train loss: 0.282959908246994\n",
            "train loss: 0.5382329225540161\n",
            "train loss: 0.24077776074409485\n",
            "train loss: 0.7063536643981934\n",
            "train loss: 0.4448944926261902\n",
            "train loss: 0.31488236784935\n",
            "train loss: 0.4578273296356201\n",
            "train loss: 0.9265806674957275\n",
            "train loss: 0.09784067422151566\n",
            "train loss: 0.6753383278846741\n",
            "train loss: 0.23062221705913544\n",
            "train loss: 0.6659826636314392\n",
            "train loss: 0.7096269130706787\n",
            "train loss: 0.21812951564788818\n",
            "train loss: 0.1292344331741333\n",
            "train loss: 0.8963271975517273\n",
            "train loss: 0.30625075101852417\n",
            "train loss: 0.026215076446533203\n",
            "train loss: 0.33500415086746216\n",
            "train loss: 0.2607421576976776\n",
            "train loss: 0.22299091517925262\n",
            "train loss: 0.42265287041664124\n",
            "train loss: 0.5484596490859985\n",
            "train loss: 0.23923800885677338\n",
            "train loss: 0.23962874710559845\n",
            "train loss: 0.5640424489974976\n",
            "train loss: 1.4053449630737305\n",
            "train loss: 1.023134708404541\n",
            "train loss: 0.20126892626285553\n",
            "train loss: 0.12005224823951721\n",
            "train loss: 0.17646396160125732\n",
            "train loss: 0.03974544256925583\n",
            "train loss: 0.23651023209095\n",
            "train loss: 0.04918202385306358\n",
            "train loss: 0.6211115717887878\n",
            "train loss: 0.6930932998657227\n",
            "train loss: 0.8882730007171631\n",
            "train loss: 0.22913001477718353\n",
            "train loss: 0.3291199803352356\n",
            "train loss: 0.3657142221927643\n",
            "train loss: 0.33532822132110596\n",
            "train loss: 0.27541565895080566\n",
            "train loss: 0.1723359078168869\n",
            "train loss: 0.4066909849643707\n",
            "train loss: 0.0532078742980957\n",
            "train loss: 0.16394293308258057\n",
            "train loss: 0.2545930743217468\n",
            "train loss: 0.365628182888031\n",
            "train loss: 0.2169419825077057\n",
            "train loss: 0.5159900784492493\n",
            "train loss: 0.812997043132782\n",
            "train loss: 0.3333158791065216\n",
            "train loss: 0.03571421280503273\n",
            "train loss: 0.9167670607566833\n",
            "train loss: 0.24039724469184875\n",
            "train loss: 0.22405138611793518\n",
            "train loss: 0.1544937640428543\n",
            "train loss: 0.31619057059288025\n",
            "train loss: 0.21470223367214203\n",
            "train loss: 1.3140143156051636\n",
            "train loss: 0.7114380598068237\n",
            "train loss: 1.265099048614502\n",
            "train loss: 0.2352622151374817\n",
            "train loss: 1.2288416624069214\n",
            "train loss: 0.11191820353269577\n",
            "train loss: 0.6563865542411804\n",
            "train loss: 0.48544973134994507\n",
            "train loss: 0.35515308380126953\n",
            "train loss: 1.0930366516113281\n",
            "train loss: 0.5230430960655212\n",
            "train loss: 1.22451651096344\n",
            "train loss: 1.009188175201416\n",
            "train loss: 1.2173751592636108\n",
            "train loss: 0.33182796835899353\n",
            "train loss: 0.2828797698020935\n",
            "train loss: 0.8213780522346497\n",
            "train loss: 0.4551164209842682\n",
            "train loss: 0.26275834441185\n",
            "train loss: 0.7775348424911499\n",
            "train loss: 0.3770197629928589\n",
            "train loss: 0.36767029762268066\n",
            "train loss: 0.6125975251197815\n",
            "train loss: 0.48057156801223755\n",
            "train loss: 0.9903541803359985\n",
            "train loss: 0.24038437008857727\n",
            "train loss: 0.34485018253326416\n",
            "train loss: 0.10843254625797272\n",
            "train loss: 0.5197008848190308\n",
            "train loss: 0.17608998715877533\n",
            "train loss: 0.3725244402885437\n",
            "train loss: 0.07313214242458344\n",
            "train loss: 0.18675541877746582\n",
            "train loss: 0.505909264087677\n",
            "train loss: 0.9419224858283997\n",
            "train loss: 0.37202969193458557\n",
            "train loss: 0.8314206600189209\n",
            "train loss: 0.05797841399908066\n",
            "train loss: 0.21466563642024994\n",
            "train loss: 0.22556376457214355\n",
            "train loss: 0.6029931306838989\n",
            "train loss: 0.1407439112663269\n",
            "train loss: 0.2966971695423126\n",
            "train loss: 0.4779849648475647\n",
            "train loss: 0.9879164099693298\n",
            "train loss: 0.3865662217140198\n",
            "train loss: 0.45895707607269287\n",
            "train loss: 0.40137189626693726\n",
            "train loss: 0.1336747258901596\n",
            "train loss: 0.6311004757881165\n",
            "train loss: 0.3848077356815338\n",
            "train loss: 0.5325974225997925\n",
            "train loss: 0.20134985446929932\n",
            "train loss: 0.6829973459243774\n",
            "train loss: 0.4670141935348511\n",
            "train loss: 0.5297336578369141\n",
            "train loss: 0.1494240164756775\n",
            "train loss: 0.7510332465171814\n",
            "train loss: 1.3037109375\n",
            "train loss: 1.3471654653549194\n",
            "train loss: 0.27926433086395264\n",
            "train loss: 0.789326012134552\n",
            "train loss: 1.1987465620040894\n",
            "train loss: 0.7380260229110718\n",
            "train loss: 0.3444043695926666\n",
            "train loss: 1.6830979585647583\n",
            "train loss: 0.4417768120765686\n",
            "train loss: 0.09793751686811447\n",
            "train loss: 0.1495354175567627\n",
            "train loss: 0.09492741525173187\n",
            "train loss: 0.4065823554992676\n",
            "train loss: 0.8029994964599609\n",
            "train loss: 0.4466792345046997\n",
            "train loss: 0.6151922345161438\n",
            "train loss: 0.3524910807609558\n",
            "train loss: 1.3644853830337524\n",
            "train loss: 0.38626301288604736\n",
            "train loss: 0.717444121837616\n",
            "train loss: 0.2403399497270584\n",
            "train loss: 0.27053573727607727\n",
            "train loss: 0.09001746028661728\n",
            "train loss: 0.6625974774360657\n",
            "train loss: 0.09290511161088943\n",
            "train loss: 0.6891924738883972\n",
            "train loss: 0.2616218030452728\n",
            "train loss: 0.9327144622802734\n",
            "train loss: 1.5483006238937378\n",
            "train loss: 0.6551836729049683\n",
            "train loss: 0.14616313576698303\n",
            "train loss: 0.07740343362092972\n",
            "train loss: 0.22982370853424072\n",
            "train loss: 0.21525780856609344\n",
            "train loss: 0.06236175075173378\n",
            "train loss: 0.5636480450630188\n",
            "train loss: 0.5485792756080627\n",
            "train loss: 0.10587195307016373\n",
            "train loss: 0.10092537850141525\n",
            "train loss: 1.243801236152649\n",
            "train loss: 0.3735843896865845\n",
            "train loss: 1.172611951828003\n",
            "train loss: 1.5015233755111694\n",
            "train loss: 0.20921771228313446\n",
            "train loss: 0.45367422699928284\n",
            "train loss: 0.4182719588279724\n",
            "train loss: 0.050735898315906525\n",
            "train loss: 0.3247796595096588\n",
            "train loss: 0.1625595986843109\n",
            "train loss: 0.294017493724823\n",
            "train loss: 0.5278035402297974\n",
            "train loss: 0.06788542866706848\n",
            "train loss: 0.18574638664722443\n",
            "train loss: 0.21220973134040833\n",
            "train loss: 0.5293759107589722\n",
            "train loss: 0.22872421145439148\n",
            "train loss: 0.23820355534553528\n",
            "train loss: 0.375113308429718\n",
            "train loss: 0.19468821585178375\n",
            "train loss: 0.2834074795246124\n",
            "train loss: 0.6768330335617065\n",
            "train loss: 0.20091785490512848\n",
            "train loss: 0.24921098351478577\n",
            "train loss: 0.17737478017807007\n",
            "train loss: 0.24832452833652496\n",
            "train loss: 0.9281291961669922\n",
            "train loss: 0.17440257966518402\n",
            "train loss: 0.516638994216919\n",
            "train loss: 0.6792988181114197\n",
            "train loss: 0.23827460408210754\n",
            "train loss: 0.09285780787467957\n",
            "train loss: 0.19611579179763794\n",
            "train loss: 0.34469398856163025\n",
            "train loss: 0.0749037116765976\n",
            "train loss: 0.2905508875846863\n",
            "train loss: 0.21371765434741974\n",
            "train loss: 0.37647995352745056\n",
            "train loss: 0.3792153000831604\n",
            "train loss: 0.22242072224617004\n",
            "train loss: 0.5099257230758667\n",
            "train loss: 1.0715047121047974\n",
            "train loss: 0.2960461378097534\n",
            "train loss: 0.5062835812568665\n",
            "train loss: 0.37729594111442566\n",
            "train loss: 0.3551776111125946\n",
            "train loss: 0.7740198373794556\n",
            "train loss: 1.193477988243103\n",
            "train loss: 1.312483549118042\n",
            "train loss: 0.5650556087493896\n",
            "train loss: 0.7507997751235962\n",
            "train loss: 1.7439643144607544\n",
            "train loss: 0.12814868986606598\n",
            "train loss: 0.18686027824878693\n",
            "train loss: 0.5616437196731567\n",
            "train loss: 0.35228070616722107\n",
            "train loss: 0.44976258277893066\n",
            "train loss: 0.340422123670578\n",
            "train loss: 0.5757520794868469\n",
            "train loss: 0.7420741319656372\n",
            "train loss: 1.034129023551941\n",
            "train loss: 0.8862321972846985\n",
            "train loss: 1.0316134691238403\n",
            "train loss: 0.9075483679771423\n",
            "train loss: 0.7145696878433228\n",
            "train loss: 1.1051549911499023\n",
            "train loss: 0.8606471419334412\n",
            "train loss: 0.6625069379806519\n",
            "train loss: 0.30712926387786865\n",
            "train loss: 0.5383505821228027\n",
            "train loss: 0.14205634593963623\n",
            "train loss: 0.4747049808502197\n",
            "train loss: 0.2600169777870178\n",
            "train loss: 0.5937222242355347\n",
            "train loss: 0.5777301788330078\n",
            "train loss: 0.12696658074855804\n",
            "train loss: 0.3231654465198517\n",
            "train loss: 0.7449129223823547\n",
            "train loss: 0.17677421867847443\n",
            "train loss: 0.3938222825527191\n",
            "train loss: 0.07826318591833115\n",
            "train loss: 0.3682785928249359\n",
            "train loss: 0.33491337299346924\n",
            "train loss: 0.6541021466255188\n",
            "train loss: 0.6066247224807739\n",
            "train loss: 0.6084418892860413\n",
            "train loss: 0.5582574605941772\n",
            "train loss: 0.16704323887825012\n",
            "train loss: 0.5482010841369629\n",
            "train loss: 0.5213704109191895\n",
            "train loss: 0.07218948006629944\n",
            "train loss: 0.24516674876213074\n",
            "train loss: 1.2434779405593872\n",
            "train loss: 0.6632822751998901\n",
            "train loss: 0.4491921663284302\n",
            "train loss: 0.1118968054652214\n",
            "train loss: 0.5200562477111816\n",
            "train loss: 0.32555681467056274\n",
            "train loss: 0.5445046424865723\n",
            "train loss: 0.5386409163475037\n",
            "train loss: 0.8065069317817688\n",
            "train loss: 0.7559686899185181\n",
            "train loss: 0.1436454802751541\n",
            "train loss: 0.37382593750953674\n",
            "train loss: 0.29187315702438354\n",
            "train loss: 0.3663136065006256\n",
            "train loss: 0.2289130985736847\n",
            "train loss: 0.8852708339691162\n",
            "train loss: 0.3468623757362366\n",
            "train loss: 0.4969363808631897\n",
            "train loss: 0.37791872024536133\n",
            "train loss: 0.25005903840065\n",
            "train loss: 0.08911462128162384\n",
            "train loss: 0.5076115727424622\n",
            "train loss: 0.11133692413568497\n",
            "train loss: 0.16768455505371094\n",
            "train loss: 0.26239001750946045\n",
            "train loss: 0.39644691348075867\n",
            "train loss: 0.7954326868057251\n",
            "train loss: 0.8901941180229187\n",
            "train loss: 0.9144797325134277\n",
            "train loss: 0.27876681089401245\n",
            "train loss: 0.7877379059791565\n",
            "train loss: 0.1559496968984604\n",
            "train loss: 0.34204304218292236\n",
            "train loss: 0.6392214298248291\n",
            "train loss: 0.8496273756027222\n",
            "train loss: 0.12477809190750122\n",
            "train loss: 0.3210877776145935\n",
            "train loss: 0.5039952397346497\n",
            "train loss: 0.5315604209899902\n",
            "train loss: 0.15411004424095154\n",
            "train loss: 0.8690746426582336\n",
            "train loss: 0.7591003179550171\n",
            "train loss: 0.735662579536438\n",
            "train loss: 0.9838154911994934\n",
            "train loss: 0.4605845808982849\n",
            "train loss: 1.1935476064682007\n",
            "train loss: 0.10790007561445236\n",
            "train loss: 0.4590381979942322\n",
            "train loss: 0.10712684690952301\n",
            "train loss: 0.14393600821495056\n",
            "train loss: 0.4239964187145233\n",
            "train loss: 0.19837768375873566\n",
            "train loss: 0.374332457780838\n",
            "train loss: 0.5053831934928894\n",
            "train loss: 0.07616355270147324\n",
            "train loss: 0.17627079784870148\n",
            "train loss: 0.14830175042152405\n",
            "train loss: 0.35808077454566956\n",
            "train loss: 0.2814488112926483\n",
            "train loss: 0.21127335727214813\n",
            "train loss: 0.1714569330215454\n",
            "train loss: 0.9534196257591248\n",
            "train loss: 0.12584839761257172\n",
            "train loss: 0.09266617149114609\n",
            "train loss: 0.20138391852378845\n",
            "train loss: 0.6131640672683716\n",
            "train loss: 0.6718987822532654\n",
            "train loss: 0.1595599204301834\n",
            "train loss: 0.4111267328262329\n",
            "train loss: 0.38097986578941345\n",
            "train loss: 0.09163939952850342\n",
            "train loss: 0.18451941013336182\n",
            "train loss: 0.131254643201828\n",
            "train loss: 0.1649373471736908\n",
            "train loss: 0.4437919855117798\n",
            "train loss: 0.26148897409439087\n",
            "train loss: 0.3532956838607788\n",
            "train loss: 0.576899528503418\n",
            "train loss: 0.5047580599784851\n",
            "train loss: 0.07787727564573288\n",
            "train loss: 0.4787721037864685\n",
            "train loss: 0.3553420603275299\n",
            "train loss: 0.3444617986679077\n",
            "train loss: 0.20549552142620087\n",
            "train loss: 1.2837976217269897\n",
            "train loss: 0.09613510221242905\n",
            "train loss: 0.12842713296413422\n",
            "train loss: 0.3769191801548004\n",
            "train loss: 0.48134127259254456\n",
            "train loss: 0.29619285464286804\n",
            "train loss: 0.33091482520103455\n",
            "train loss: 0.30918729305267334\n",
            "train loss: 0.5759560465812683\n",
            "train loss: 0.4502188563346863\n",
            "train loss: 0.6369760632514954\n",
            "train loss: 0.4319937527179718\n",
            "train loss: 0.30682358145713806\n",
            "train loss: 0.38439878821372986\n",
            "train loss: 0.7765459418296814\n",
            "train loss: 0.46118417382240295\n",
            "train loss: 0.3455277681350708\n",
            "train loss: 0.7463329434394836\n",
            "train loss: 0.21711333096027374\n",
            "train loss: 0.18768249452114105\n",
            "train loss: 0.22477813065052032\n",
            "train loss: 0.3417185842990875\n",
            "train loss: 0.14448940753936768\n",
            "train loss: 0.1333000659942627\n",
            "train loss: 1.4455562829971313\n",
            "train loss: 1.0283507108688354\n",
            "train loss: 0.6519972681999207\n",
            "train loss: 0.6984881162643433\n",
            "train loss: 1.1196134090423584\n",
            "train loss: 0.5965259075164795\n",
            "train loss: 0.6373562216758728\n",
            "train loss: 0.2758826017379761\n",
            "train loss: 0.08613299578428268\n",
            "train loss: 0.5343599915504456\n",
            "train loss: 0.23534296452999115\n",
            "train loss: 0.546519935131073\n",
            "train loss: 1.6235853433609009\n",
            "train loss: 0.6633239388465881\n",
            "train loss: 0.2215663641691208\n",
            "train loss: 1.0503826141357422\n",
            "train loss: 0.7060792446136475\n",
            "train loss: 0.053995899856090546\n",
            "train loss: 0.6481397747993469\n",
            "train loss: 0.16555306315422058\n",
            "train loss: 0.4584130644798279\n",
            "train loss: 0.501869261264801\n",
            "train loss: 0.14903193712234497\n",
            "train loss: 0.15008233487606049\n",
            "train loss: 0.12799741327762604\n",
            "train loss: 0.29479309916496277\n",
            "train loss: 0.7617420554161072\n",
            "train loss: 0.3708413541316986\n",
            "train loss: 0.12011103332042694\n",
            "train loss: 0.583875834941864\n",
            "train loss: 0.17573484778404236\n",
            "train loss: 0.20525765419006348\n",
            "train loss: 0.08921296149492264\n",
            "train loss: 0.6366527676582336\n",
            "train loss: 0.8300247192382812\n",
            "train loss: 0.4295565187931061\n",
            "train loss: 0.14217695593833923\n",
            "train loss: 1.0187931060791016\n",
            "train loss: 0.17386654019355774\n",
            "train loss: 0.2213548868894577\n",
            "train loss: 1.0198760032653809\n",
            "train loss: 0.2617967426776886\n",
            "train loss: 0.2110305279493332\n",
            "train loss: 0.18346518278121948\n",
            "train loss: 1.2444560527801514\n",
            "train loss: 0.40878549218177795\n",
            "train loss: 0.3158072531223297\n",
            "train loss: 0.2869030237197876\n",
            "train loss: 0.3627925217151642\n",
            "train loss: 0.8198872208595276\n",
            "train loss: 0.5078644752502441\n",
            "train loss: 0.29802602529525757\n",
            "train loss: 1.2061145305633545\n",
            "train loss: 0.4039723575115204\n",
            "train loss: 0.4564906656742096\n",
            "train loss: 0.2920699715614319\n",
            "train loss: 0.4689323604106903\n",
            "train loss: 0.2037978172302246\n",
            "train loss: 0.730431318283081\n",
            "train loss: 0.5507542490959167\n",
            "train loss: 0.09193234145641327\n",
            "train loss: 0.33692046999931335\n",
            "train loss: 0.08203897625207901\n",
            "train loss: 0.3262837827205658\n",
            "train loss: 0.12642693519592285\n",
            "train loss: 1.0790953636169434\n",
            "train loss: 0.3794974386692047\n",
            "train loss: 1.4879785776138306\n",
            "train loss: 1.1686627864837646\n",
            "train loss: 0.464174747467041\n",
            "train loss: 0.9351977705955505\n",
            "train loss: 0.3321734368801117\n",
            "train loss: 0.09152915328741074\n",
            "train loss: 0.06472983211278915\n",
            "train loss: 0.1725250780582428\n",
            "train loss: 0.13329875469207764\n",
            "train loss: 0.5509223341941833\n",
            "train loss: 0.10542216151952744\n",
            "train loss: 0.747016429901123\n",
            "train loss: 0.32839056849479675\n",
            "train loss: 0.3458133339881897\n",
            "train loss: 0.07624705880880356\n",
            "train loss: 0.1324434131383896\n",
            "train loss: 0.37430906295776367\n",
            "train loss: 0.17428195476531982\n",
            "train loss: 0.7351893782615662\n",
            "train loss: 0.0907668024301529\n",
            "train loss: 0.6546894311904907\n",
            "train loss: 0.3013753294944763\n",
            "train loss: 0.16858206689357758\n",
            "train loss: 0.5449450016021729\n",
            "train loss: 0.2566154897212982\n",
            "train loss: 0.2882591485977173\n",
            "train loss: 0.534427285194397\n",
            "train loss: 0.09727366268634796\n",
            "train loss: 0.19493110477924347\n",
            "train loss: 0.5691921710968018\n",
            "train loss: 0.040618907660245895\n",
            "train loss: 0.7178054451942444\n",
            "train loss: 0.2629461884498596\n",
            "train loss: 0.3831249177455902\n",
            "train loss: 0.7371692657470703\n",
            "train loss: 0.43710872530937195\n",
            "train loss: 0.4083375632762909\n",
            "train loss: 0.18787413835525513\n",
            "train loss: 0.18609319627285004\n",
            "train loss: 0.3569285571575165\n",
            "train loss: 0.16412171721458435\n",
            "train loss: 0.19009479880332947\n",
            "train loss: 0.16851337254047394\n",
            "train loss: 0.26575571298599243\n",
            "train loss: 0.4594556987285614\n",
            "train loss: 0.2644902765750885\n",
            "train loss: 0.3744736313819885\n",
            "train loss: 0.6533306837081909\n",
            "train loss: 0.07076820731163025\n",
            "train loss: 0.5339756608009338\n",
            "train loss: 0.20525342226028442\n",
            "train loss: 0.4225718080997467\n",
            "train loss: 0.9913205504417419\n",
            "train loss: 0.11889941245317459\n",
            "train loss: 0.9866507053375244\n",
            "train loss: 0.4099000096321106\n",
            "train loss: 0.299708753824234\n",
            "train loss: 0.20672482252120972\n",
            "train loss: 0.9696125388145447\n",
            "train loss: 0.130017951130867\n",
            "train loss: 0.23845934867858887\n",
            "train loss: 0.9406206011772156\n",
            "train loss: 0.2500872015953064\n",
            "train loss: 0.827372133731842\n",
            "train loss: 0.5379458069801331\n",
            "train loss: 0.17137275636196136\n",
            "train loss: 0.4304007291793823\n",
            "train loss: 0.18502908945083618\n",
            "train loss: 0.49712955951690674\n",
            "train loss: 0.509273886680603\n",
            "train loss: 0.04943958297371864\n",
            "train loss: 0.9591104984283447\n",
            "train loss: 0.2062627673149109\n",
            "train loss: 0.7469117641448975\n",
            "train loss: 0.2264488935470581\n",
            "train loss: 0.28368231654167175\n",
            "train loss: 0.09301025420427322\n",
            "train loss: 0.16918468475341797\n",
            "train loss: 0.30862686038017273\n",
            "train loss: 0.1099788174033165\n",
            "train loss: 0.3074724078178406\n",
            "train loss: 0.6013326644897461\n",
            "train loss: 0.14280712604522705\n",
            "train loss: 0.4817048907279968\n",
            "train loss: 0.5124499797821045\n",
            "train loss: 0.5332416892051697\n",
            "train loss: 0.8755287528038025\n",
            "train loss: 0.2301619052886963\n",
            "train loss: 0.2993323802947998\n",
            "train loss: 0.28556153178215027\n",
            "train loss: 0.6468762755393982\n",
            "train loss: 0.24967420101165771\n",
            "train loss: 1.269579529762268\n",
            "train loss: 0.4198192059993744\n",
            "train loss: 0.24083203077316284\n",
            "train loss: 0.237042635679245\n",
            "train loss: 0.15914437174797058\n",
            "train loss: 1.4012764692306519\n",
            "train loss: 0.41410037875175476\n",
            "train loss: 0.76212078332901\n",
            "train loss: 0.5931117534637451\n",
            "train loss: 0.2438829243183136\n",
            "train loss: 0.08243703842163086\n",
            "train loss: 1.2897546291351318\n",
            "train loss: 0.6092889904975891\n",
            "train loss: 0.9945123791694641\n",
            "train loss: 1.2589575052261353\n",
            "train loss: 0.8153449296951294\n",
            "train loss: 1.277082085609436\n",
            "train loss: 0.693267285823822\n",
            "train loss: 0.21614523231983185\n",
            "train loss: 0.7693042755126953\n",
            "train loss: 0.4343448579311371\n",
            "train loss: 0.09635979682207108\n",
            "train loss: 0.4225902557373047\n",
            "train loss: 1.0182104110717773\n",
            "train loss: 0.680077314376831\n",
            "train loss: 1.1630029678344727\n",
            "train loss: 0.5173885226249695\n",
            "train loss: 0.12694403529167175\n",
            "train loss: 0.3088292181491852\n",
            "train loss: 1.125089406967163\n",
            "train loss: 0.8458823561668396\n",
            "train loss: 0.35820522904396057\n",
            "train loss: 0.5831374526023865\n",
            "train loss: 0.05647948011755943\n",
            "train loss: 0.24635352194309235\n",
            "train loss: 0.29157155752182007\n",
            "train loss: 1.1416125297546387\n",
            "train loss: 0.31213682889938354\n",
            "train loss: 0.20456457138061523\n",
            "train loss: 0.9534119963645935\n",
            "train loss: 0.15431581437587738\n",
            "train loss: 0.16948185861110687\n",
            "train loss: 0.15434764325618744\n",
            "train loss: 0.18203584849834442\n",
            "train loss: 0.6060768961906433\n",
            "train loss: 0.06957579404115677\n",
            "train loss: 0.08986334502696991\n",
            "train loss: 1.2363234758377075\n",
            "train loss: 0.7721931338310242\n",
            "train loss: 0.18959610164165497\n",
            "train loss: 0.14232094585895538\n",
            "train loss: 0.05923742800951004\n",
            "train loss: 0.8947710394859314\n",
            "train loss: 0.7925211191177368\n",
            "train loss: 0.08045995235443115\n",
            "train loss: 0.24098612368106842\n",
            "train loss: 0.4623488187789917\n",
            "train loss: 0.3807337284088135\n",
            "train loss: 1.2499562501907349\n",
            "train loss: 1.3035173416137695\n",
            "train loss: 0.0957668200135231\n",
            "train loss: 0.9338405728340149\n",
            "train loss: 0.1842251569032669\n",
            "train loss: 0.041202060878276825\n",
            "train loss: 0.5400673747062683\n",
            "train loss: 0.12558819353580475\n",
            "train loss: 0.1068490743637085\n",
            "train loss: 0.1904231160879135\n",
            "train loss: 1.0468889474868774\n",
            "train loss: 0.33723917603492737\n",
            "train loss: 0.7765069603919983\n",
            "train loss: 0.19964244961738586\n",
            "train loss: 0.15819285809993744\n",
            "train loss: 0.20372611284255981\n",
            "train loss: 0.5789800882339478\n",
            "train loss: 0.9206064939498901\n",
            "train loss: 0.3320671021938324\n",
            "train loss: 0.09324352443218231\n",
            "train loss: 0.5691828727722168\n",
            "train loss: 0.3583177924156189\n",
            "train loss: 0.3286989629268646\n",
            "train loss: 0.46283063292503357\n",
            "train loss: 0.24084073305130005\n",
            "train loss: 0.6833765506744385\n",
            "train loss: 0.07804694771766663\n",
            "train loss: 0.281774640083313\n",
            "train loss: 0.9770995378494263\n",
            "train loss: 0.39034947752952576\n",
            "train loss: 0.5645004510879517\n",
            "train loss: 1.2327969074249268\n",
            "train loss: 1.037038803100586\n",
            "train loss: 0.34049245715141296\n",
            "train loss: 0.13752156496047974\n",
            "train loss: 0.11003462225198746\n",
            "train loss: 0.34953173995018005\n",
            "train loss: 0.6166250109672546\n",
            "train loss: 0.03269852325320244\n",
            "train loss: 0.17151503264904022\n",
            "train loss: 0.16005514562129974\n",
            "train loss: 0.09632015228271484\n",
            "train loss: 0.4044604003429413\n",
            "train loss: 0.027022656053304672\n",
            "train loss: 0.2221302092075348\n",
            "train loss: 0.12177521735429764\n",
            "train loss: 0.2716212868690491\n",
            "train loss: 0.7466127872467041\n",
            "train loss: 0.45640015602111816\n",
            "train loss: 0.49444523453712463\n",
            "train loss: 0.08015038818120956\n",
            "train loss: 0.8094220757484436\n",
            "train loss: 0.11030220240354538\n",
            "train loss: 0.20900440216064453\n",
            "train loss: 0.2095324546098709\n",
            "train loss: 0.8936787247657776\n",
            "train loss: 1.085953950881958\n",
            "train loss: 0.10232686996459961\n",
            "train loss: 0.12348439544439316\n",
            "train loss: 0.17534220218658447\n",
            "train loss: 0.4737875163555145\n",
            "train loss: 0.028533881530165672\n",
            "train loss: 0.49457278847694397\n",
            "train loss: 1.6625182628631592\n",
            "train loss: 1.1647675037384033\n",
            "train loss: 1.1038808822631836\n",
            "train loss: 0.7899401187896729\n",
            "train loss: 0.20723968744277954\n",
            "train loss: 0.2597557306289673\n",
            "train loss: 0.27187731862068176\n",
            "train loss: 0.5995408296585083\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
        "\n",
        "#wandb_logger = WandbLogger(project=\"Donut\", name=\"demo-run-cord\")\n",
        "\n",
        "# class PushToHubCallback(Callback):\n",
        "#     def on_train_epoch_end(self, trainer, pl_module):\n",
        "#         print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
        "#         pl_module.model.push_to_hub(\"nielsr/donut-demo\",\n",
        "#                                     commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
        "\n",
        "#     def on_train_end(self, trainer, pl_module):\n",
        "#         print(f\"Pushing model to the hub after training\")\n",
        "#         pl_module.processor.push_to_hub(\"nielsr/donut-demo\",\n",
        "#                                     commit_message=f\"Training done\")\n",
        "#         pl_module.model.push_to_hub(\"nielsr/donut-demo\",\n",
        "#                                     commit_message=f\"Training done\")\n",
        "\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=100, verbose=False, mode=\"min\")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "        accelerator=\"gpu\",\n",
        "        #devices='auto',\n",
        "        max_epochs=config.get(\"max_epochs\"),\n",
        "        val_check_interval=config.get(\"val_check_interval\"),\n",
        "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
        "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
        "        precision='16-mixed', # we'll use mixed precision\n",
        "        num_sanity_val_steps=0,\n",
        "        #logger=,\n",
        "        callbacks=[early_stop_callback],\n",
        ")\n",
        "\n",
        "trainer.fit(model_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w_Uz6yD8dkz"
      },
      "outputs": [],
      "source": [
        "processor.tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxmfdEc3-TRX"
      },
      "outputs": [],
      "source": [
        "pixel_values, labels, answers = test_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbVP0-Tx-6pN"
      },
      "outputs": [],
      "source": [
        "pixel_values.unsqueeze(0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHjs5IIM_NBG"
      },
      "outputs": [],
      "source": [
        "pixel_values, labels, answers = train_dataset[3]\n",
        "\n",
        "pixel_values = pixel_values.unsqueeze(0)\n",
        "labels = labels.unsqueeze(0)\n",
        "\n",
        "batch_size = pixel_values.shape[0]\n",
        "decoder_input_ids = torch.full((batch_size, 1), model.config.decoder_start_token_id, device='cuda')\n",
        "\n",
        "\n",
        "\n",
        "outputs = trainer.model.model.generate(pixel_values.to('cuda'),\n",
        "                                   decoder_input_ids=decoder_input_ids,\n",
        "                                   max_length=max_length,\n",
        "                                   early_stopping=True,\n",
        "                                   pad_token_id=processor.tokenizer.pad_token_id,\n",
        "                                   eos_token_id=processor.tokenizer.eos_token_id,\n",
        "                                   use_cache=True,\n",
        "                                   num_beams=1,\n",
        "                                   bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
        "                                   return_dict_in_generate=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHx1vjZT_cVv"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "for seq in processor.tokenizer.batch_decode(outputs.sequences):\n",
        "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
        "    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
        "    predictions.append(seq)\n",
        "\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkYz7KtC_lsD"
      },
      "outputs": [],
      "source": [
        "answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HdlpvKrBXNU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "82d1272a2cd5481ca5704290fcb099ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67cc97da815b46ac9fc09d12f1f812e0",
              "IPY_MODEL_640828bbc02c49d5b97497a1856fe7f0",
              "IPY_MODEL_c9df95ccb5b5438ea35fb0893a9975ee"
            ],
            "layout": "IPY_MODEL_13eb1132dc6341efbb6e3981889f8f7f"
          }
        },
        "67cc97da815b46ac9fc09d12f1f812e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bcd2c1ad3bd4485809fda79477ebedc",
            "placeholder": "​",
            "style": "IPY_MODEL_810b73528d484e3baaab713f7c3c3715",
            "value": "Epoch 0:  51%"
          }
        },
        "640828bbc02c49d5b97497a1856fe7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73316b9f8d48462b8bee82addd8721bd",
            "max": 8167,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c03e7e21a1640d68a70e37ee358c2c7",
            "value": 4180
          }
        },
        "c9df95ccb5b5438ea35fb0893a9975ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67901ddb81914ca6ab19969277ecdabf",
            "placeholder": "​",
            "style": "IPY_MODEL_3fbf5f101476406a8b7c621627f8ce8f",
            "value": " 4180/8167 [43:41&lt;41:40,  1.59it/s, v_num=2]"
          }
        },
        "13eb1132dc6341efbb6e3981889f8f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "7bcd2c1ad3bd4485809fda79477ebedc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "810b73528d484e3baaab713f7c3c3715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73316b9f8d48462b8bee82addd8721bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c03e7e21a1640d68a70e37ee358c2c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67901ddb81914ca6ab19969277ecdabf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fbf5f101476406a8b7c621627f8ce8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}